{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Data Preparation {#setup-and-data-preparation}\n",
        "\n",
        "Install Delta-rs and supporting libraries:\n",
        "\n",
        "```bash\n",
        "pip install deltalake pandas duckdb polars\n",
        "```\n",
        "\n",
        "We'll use actual NYC Yellow Taxi data to demonstrate real-world scenarios. The NYC Taxi & Limousine Commission provides monthly trip records in Parquet format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from deltalake import DeltaTable, write_deltalake\n",
        "import duckdb\n",
        "import polars as pl\n",
        "\n",
        "# Download NYC Yellow Taxi data (June 2024 as example)\n",
        "# Full dataset available at: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
        "taxi_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-06.parquet\"\n",
        "\n",
        "# Read a sample of the data for demonstration\n",
        "sample_data = pd.read_parquet(taxi_url).head(10000)\n",
        "\n",
        "print(f\"Loaded {len(sample_data)} taxi trips from NYC TLC\")\n",
        "print(f\"Data shape: {sample_data.shape}\")\n",
        "print(f\"Date range: {sample_data['tpep_pickup_datetime'].min()} to {sample_data['tpep_pickup_datetime'].max()}\")\n",
        "\n",
        "sample_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Loaded 10000 taxi trips from NYC TLC\n",
        "Data shape: (10000, 19)\n",
        "Date range: 2024-05-31 15:33:34 to 2024-06-01 02:59:54\n",
        "   VendorID tpep_pickup_datetime  ... congestion_surcharge  Airport_fee\n",
        "0         1  2024-06-01 00:03:46  ...                  0.0         1.75\n",
        "1         2  2024-06-01 00:55:22  ...                  0.0         1.75\n",
        "2         1  2024-06-01 00:23:53  ...                  0.0         0.00\n",
        "3         1  2024-06-01 00:32:24  ...                  2.5         0.00\n",
        "4         1  2024-06-01 00:51:38  ...                  2.5         0.00\n",
        "\n",
        "[5 rows x 19 columns]\n",
        "```\n",
        "\n",
        "## Creating Your First Delta Table {#creating-your-first-delta-table}\n",
        "\n",
        "Create your first Delta table in the `data` directory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "write_deltalake(\"data/taxi_delta_table\", sample_data, mode=\"overwrite\")\n",
        "print(\"Created Delta table\")\n",
        "\n",
        "# Read back from Delta table\n",
        "dt = DeltaTable(\"data/taxi_delta_table\")\n",
        "df_from_delta = dt.to_pandas()\n",
        "\n",
        "print(f\"Delta table contains {len(df_from_delta)} records\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Created Delta table\n",
        "Delta table contains 10000 records\n",
        "```\n",
        "\n",
        "View the Delta table structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inspect Delta table metadata\n",
        "print(\"Delta table schema:\")\n",
        "print(dt.schema().to_arrow())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Delta table schema:\n",
        "arro3.core.Schema\n",
        "------------\n",
        "VendorID: Int32\n",
        "tpep_pickup_datetime: Timestamp(Microsecond, None)\n",
        "tpep_dropoff_datetime: Timestamp(Microsecond, None)\n",
        "passenger_count: Float64\n",
        "trip_distance: Float64\n",
        "...\n",
        "total_amount: Float64\n",
        "congestion_surcharge: Float64\n",
        "Airport_fee: Float64\n",
        "```\n",
        "\n",
        "View the current version of the Delta table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Current version: {dt.version()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Current version: 0\n",
        "```\n",
        "## Incremental Updates and CRUD Operations {#incremental-updates-and-crud-operations}\n",
        "\n",
        "Instead of rewriting entire datasets when adding new records, incremental updates append only what changed. Delta-rs handles these efficient operations natively.\n",
        "\n",
        "To demonstrate this, we'll simulate late-arriving data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulate late-arriving data\n",
        "late_data = pd.read_parquet(taxi_url).iloc[10000:10050]\n",
        "print(f\"New data to add: {len(late_data)} records\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "New data to add: 50 records\n",
        "```\n",
        "\n",
        "### Traditional Approach: Process Everything {#traditional-approach-process-everything}\n",
        "\n",
        "The pandas workflow requires loading both existing and new data, combining them, and rewriting the entire output file:\n",
        "\n",
        "```python\n",
        "# Pandas approach - reload existing data and merge\n",
        "existing_df = pd.read_parquet(taxi_url).head(10000)\n",
        "complete_df = pd.concat([existing_df, late_data])\n",
        "complete_df.to_parquet(\"data/taxi_complete.parquet\")\n",
        "print(f\"Processed {len(complete_df)} total records\")\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```text\n",
        "Processed 10050 total records\n",
        "```\n",
        "\n",
        "Pandas processed all 10,050 records to add just 50 new ones, demonstrating the inefficiency of full-dataset operations.\n",
        "\n",
        "### Delta-rs Approach: Process Only New Data {#delta-rs-approach-process-only-new-data}\n",
        "\n",
        "Delta-rs appends only the new records without touching existing data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Delta-rs - append only what's new\n",
        "write_deltalake(\"data/taxi_delta_table\", late_data, mode=\"append\")\n",
        "\n",
        "dt = DeltaTable(\"data/taxi_delta_table\")\n",
        "print(f\"Added {len(late_data)} new records\")\n",
        "print(f\"Table version: {dt.version()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Added 50 new records\n",
        "Table version: 1\n",
        "```\n",
        "\n",
        "Delta-rs processed only the 50 new records while automatically incrementing to version 1, enabling efficient operations and data lineage.\n",
        "\n",
        "\n",
        "## Time Travel and Data Versioning {#time-travel-and-data-versioning}\n",
        "\n",
        "Time travel and data versioning let you access any previous state of your data. This is essential for auditing changes, recovering from errors, and understanding how data evolved over time without maintaining separate backup files.\n",
        "\n",
        "### Traditional Approach: Manual Backup Strategy {#traditional-approach-manual-backup-strategy}\n",
        "\n",
        "Traditional file-based workflows rely on timestamped copies and manual versioning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Traditional pproach - manual timestamped backups\n",
        "import datetime\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "df.to_parquet(f\"data/taxi_backup_{timestamp}.parquet\")  # Create manual backup\n",
        "df_modified.to_parquet(\"data/taxi_data.parquet\")  # Overwrite original\n",
        "# To recover: manually identify and reload backup file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delta-rs Approach: Built-in Time Travel {#delta-rs-approach-built-in-time-travel}\n",
        "\n",
        "Delta-rs automatically tracks every change with instant access to any version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Access any historical version instantly\n",
        "dt_v0 = DeltaTable(\"data/taxi_delta_table\", version=0)\n",
        "current_dt = DeltaTable(\"data/taxi_delta_table\")\n",
        "\n",
        "print(f\"Version 0: {len(dt_v0.to_pandas())} records\")\n",
        "print(f\"Current version: {len(current_dt.to_pandas())} records\")\n",
        "print(f\"Available versions: {current_dt.version() + 1}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Version 0: 10000 records\n",
        "Current version: 10050 records\n",
        "Available versions: 2\n",
        "```\n",
        "\n",
        "Delta-rs maintains 2 complete versions while traditional backups would require separate 57MB files for each timestamp.\n",
        "\n",
        "> üìö For comprehensive production data workflows and version control best practices, check out [Production-Ready Data Science](https://codecut.ai/production-ready-data-science/).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Schema Evolution in Action {#schema-evolution-in-action}\n",
        "\n",
        "As requirements evolve, you often need to add new columns or change data types. Schema evolution handles these changes automatically, letting you update your data structure without breaking existing queries or reprocessing historical records.\n",
        "\n",
        "\n",
        "To demonstrate this, imagine NYC's taxi authority introduces weather tracking and surge pricing features, requiring your pipeline to handle new `weather_condition` and `surge_multiplier` columns alongside existing fare data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copy the existing data\n",
        "enhanced_data = pd.read_parquet(taxi_url).iloc[20000:20100].copy()\n",
        "\n",
        "# Simulate new data with additional business columns\n",
        "weather_options = ['clear', 'rain', 'snow', 'cloudy']\n",
        "surge_options = [1.0, 1.2, 1.5, 2.0]\n",
        "enhanced_data['weather_condition'] = [weather_options[i % 4] for i in range(len(enhanced_data))]\n",
        "enhanced_data['surge_multiplier'] = [surge_options[i % 4] for i in range(len(enhanced_data))]\n",
        "\n",
        "print(f\"Enhanced data: {len(enhanced_data)} records with {len(enhanced_data.columns)} columns\")\n",
        "print(f\"New columns: {[col for col in enhanced_data.columns if col not in sample_data.columns]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Enhanced data: 100 records with 21 columns\n",
        "New columns: ['weather_condition', 'surge_multiplier']\n",
        "```\n",
        "\n",
        "### Traditional Approach: No Schema History {#traditional-approach-no-schema-history}\n",
        "\n",
        "Traditional formats provide no tracking of schema changes or evolution history:\n",
        "\n",
        "```python\n",
        "# Traditional approach - no schema versioning or history\n",
        "df_v1 = pd.read_parquet(\"taxi_v1.parquet\")  # Original schema\n",
        "df_v2 = pd.read_parquet(\"taxi_v2.parquet\")  # Enhanced schema\n",
        "```\n",
        "\n",
        "### Delta-rs Approach: Schema Versioning and History {#delta-rs-approach-schema-versioning-and-history}\n",
        "\n",
        "Delta-rs automatically merges schemas while tracking every change:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Schema evolution with automatic versioning\n",
        "write_deltalake(\n",
        "    \"data/taxi_delta_table\", \n",
        "    enhanced_data, \n",
        "    mode=\"append\",\n",
        "    schema_mode=\"merge\"\n",
        ")\n",
        "\n",
        "dt = DeltaTable(\"data/taxi_delta_table\")\n",
        "print(f\"Schema evolved: {len(dt.to_pandas().columns)} columns | Version: {dt.version()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Schema evolved: 21 columns | Version: 2\n",
        "```\n",
        "\n",
        "Explore the complete schema evolution history and access any previous version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View schema change history\n",
        "history = dt.history()\n",
        "for entry in history[:2]:\n",
        "    print(f\"Version {entry['version']}: {entry['operation']} at {entry['timestamp']}\")\n",
        "\n",
        "# Access different schema versions\n",
        "original_schema = DeltaTable(\"data/taxi_delta_table\", version=0)\n",
        "print(f\"\\nOriginal schema (v0): {len(original_schema.to_pandas().columns)} columns\")\n",
        "print(f\"Current schema (v{dt.version()}): {len(dt.to_pandas().columns)} columns\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Version 2: WRITE at 1755180763083\n",
        "Version 1: WRITE at 1755180762968\n",
        "\n",
        "Original schema (v0): 19 columns\n",
        "Current schema (v2): 21 columns\n",
        "```\n",
        "\n",
        "Delta-rs expanded from 19 to 21 columns across 10,150 records without schema migration scripts or pipeline failures.\n",
        "\n",
        "## Selective Updates with Merge Operations {#selective-updates-with-merge-operations}\n",
        "\n",
        "Merge operations combine updates and inserts in a single transaction based on matching conditions. This eliminates the need to process entire datasets when you only need to modify specific records, dramatically improving efficiency at scale.\n",
        "\n",
        "To demonstrate this, let's create a simple taxi trips table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create initial Delta table with 5 trips\n",
        "trips = pd.DataFrame({\n",
        "    'trip_id': [1, 2, 3, 4, 5],\n",
        "    'fare_amount': [15.5, 20.0, 18.3, 12.5, 25.0],\n",
        "    'payment_type': [1, 1, 2, 1, 2]\n",
        "})\n",
        "write_deltalake(\"data/trips_merge_demo\", trips, mode=\"overwrite\")\n",
        "print(\"Initial trips:\")\n",
        "print(trips)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Initial trips:\n",
        "   trip_id  fare_amount  payment_type\n",
        "0        1         15.5             1\n",
        "1        2         20.0             1\n",
        "2        3         18.3             2\n",
        "3        4         12.5             1\n",
        "4        5         25.0             2\n",
        "```\n",
        "\n",
        "Here are the updates we want to make:\n",
        "\n",
        "- **Update** trip 2: change fare from $20.00 to $22.00\n",
        "- **Update** trip 4: change fare from $12.50 to $13.80\n",
        "- **Insert** trip 6: new trip with fare $30.00\n",
        "- **Insert** trip 7: new trip with fare $16.50\n",
        "\n",
        "### Traditional Approach: Full Dataset Processing {#traditional-approach-full-dataset-processing}\n",
        "\n",
        "Traditional workflows require loading complete datasets, identifying matches, and rewriting all records. This process becomes increasingly expensive as data grows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Traditional approach - load, modify, and rewrite everything\n",
        "existing_df = trips.copy()\n",
        "\n",
        "# Updates: manually locate and modify rows\n",
        "existing_df.loc[existing_df['trip_id'] == 2, 'fare_amount'] = 22.0\n",
        "existing_df.loc[existing_df['trip_id'] == 4, 'fare_amount'] = 13.8\n",
        "\n",
        "# Inserts: create new rows and append\n",
        "new_trips = pd.DataFrame({\n",
        "    'trip_id': [6, 7],\n",
        "    'fare_amount': [30.0, 16.5],\n",
        "    'payment_type': [1, 1]\n",
        "})\n",
        "updated_df = pd.concat([existing_df, new_trips], ignore_index=True)\n",
        "\n",
        "# Rewrite entire dataset\n",
        "updated_df.to_parquet(\"data/trips_traditional.parquet\")\n",
        "print(updated_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "   trip_id  fare_amount  payment_type\n",
        "0        1         15.5             1\n",
        "1        2         22.0             1  # Updated\n",
        "2        3         18.3             2\n",
        "3        4         13.8             1  # Updated\n",
        "4        5         25.0             2\n",
        "5        6         30.0             1  # Inserted\n",
        "6        7         16.5             1  # Inserted\n",
        "```\n",
        "\n",
        "### Delta-rs Approach: Upsert with Merge Operations {#delta-rs-approach-upsert-with-merge-operations}\n",
        "\n",
        "Delta-rs merge operations handle both updates and inserts in a single atomic operation, processing only affected records:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare changes: 2 updates + 2 inserts\n",
        "changes = pd.DataFrame({\n",
        "    'trip_id': [2, 4, 6, 7],\n",
        "    'fare_amount': [22.0, 13.8, 30.0, 16.5],\n",
        "    'payment_type': [2, 2, 1, 1]\n",
        "})\n",
        "\n",
        "# Load Delta table\n",
        "dt = DeltaTable(\"data/trips_merge_demo\")\n",
        "\n",
        "# Upsert operation: update existing, insert new\n",
        "(\n",
        "    dt.merge(\n",
        "        source=changes,\n",
        "        predicate=\"target.trip_id = source.trip_id\",\n",
        "        source_alias=\"source\",\n",
        "        target_alias=\"target\",\n",
        "    )\n",
        "    .when_matched_update(\n",
        "        updates={\n",
        "            \"fare_amount\": \"source.fare_amount\",\n",
        "            \"payment_type\": \"source.payment_type\",\n",
        "        }\n",
        "    )\n",
        "    .when_not_matched_insert(\n",
        "        updates={\n",
        "            \"trip_id\": \"source.trip_id\",\n",
        "            \"fare_amount\": \"source.fare_amount\",\n",
        "            \"payment_type\": \"source.payment_type\",\n",
        "        }\n",
        "    )\n",
        "    .execute()\n",
        ")\n",
        "\n",
        "# Verify results\n",
        "result = dt.to_pandas().sort_values('trip_id').reset_index(drop=True)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "   trip_id  fare_amount  payment_type\n",
        "0        1         15.5             1\n",
        "1        2         22.0             2  # Updated\n",
        "2        3         18.3             2\n",
        "3        4         13.8             2  # Updated\n",
        "4        5         25.0             2\n",
        "5        6         30.0             1  # Inserted\n",
        "6        7         16.5             1  # Inserted\n",
        "```\n",
        "\n",
        "Delta-rs processed exactly 4 records (2 updates + 2 inserts) while pandas processed all 7 records. This efficiency compounds dramatically with larger datasets.\n",
        "\n",
        "## Multi-Engine Integration {#multi-engine-integration}\n",
        "\n",
        "Different teams often use different tools: pandas for exploration, DuckDB for SQL queries, Polars for performance. Multi-engine support lets all these tools access the same data directly without creating duplicates or writing conversion scripts.\n",
        "\n",
        "### Traditional Approach: Engine-Specific Optimization Requirements {#traditional-approach-engine-specific-optimization-requirements}\n",
        "\n",
        "Each engine needs different file optimizations that don't transfer between tools:\n",
        "\n",
        "Start with the original dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Traditional approach - Each engine needs different optimizations\n",
        "data = {\"payment_type\": [1, 1, 2, 1, 2], \"fare_amount\": [15.5, 20.0, 18.3, 12.5, 25.0]}\n",
        "df = pd.DataFrame(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Pandas team optimizes for indexed lookups:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pandas team needs indexed Parquet for fast lookups\n",
        "df.to_parquet(\"data/pandas_optimized.parquet\", index=True)\n",
        "pandas_result = pd.read_parquet(\"data/pandas_optimized.parquet\")\n",
        "print(f\"Pandas: {len(pandas_result)} trips, avg ${pandas_result['fare_amount'].mean():.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Pandas: 5 trips, avg $17.66\n",
        "```\n",
        "\n",
        "The Polars team needs sorted data for predicate pushdown optimization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Polars team needs sorted columns for predicate pushdown\n",
        "df.sort_values('payment_type').to_parquet(\"data/polars_optimized.parquet\")\n",
        "polars_result = pl.read_parquet(\"data/polars_optimized.parquet\").select([\n",
        "    pl.len().alias(\"trips\"), pl.col(\"fare_amount\").mean().alias(\"avg_fare\")\n",
        "])\n",
        "print(f\"Polars: {polars_result}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "Polars: shape: (1, 2)\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ trips ‚îÜ avg_fare ‚îÇ\n",
        "‚îÇ ---   ‚îÜ ---      ‚îÇ\n",
        "‚îÇ u32   ‚îÜ f64      ‚îÇ\n",
        "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
        "‚îÇ 5     ‚îÜ 18.26    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "The DuckDB team requires specific compression for query performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DuckDB needs specific compression/statistics for query planning\n",
        "df.to_parquet(\"data/duckdb_optimized.parquet\", compression='zstd')\n",
        "duckdb_result = duckdb.execute(\"\"\"\n",
        "    SELECT COUNT(*) as trips, ROUND(AVG(fare_amount), 2) as avg_fare\n",
        "    FROM 'data/duckdb_optimized.parquet'\n",
        "\"\"\").fetchone()\n",
        "print(f\"DuckDB: {duckdb_result[0]} trips, ${duckdb_result[1]} avg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "DuckDB: 5 trips, $18.26 avg\n",
        "```\n",
        "\n",
        "### Delta-rs Approach: Universal Optimizations {#delta-rs-approach-universal-optimizations}\n",
        "\n",
        "Delta-rs provides built-in optimizations that benefit all engines simultaneously:\n",
        "\n",
        "Create one optimized Delta table that serves all engines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Delta-rs approach - Universal optimizations for all engines\n",
        "from deltalake import write_deltalake, DeltaTable\n",
        "import polars as pl\n",
        "import duckdb\n",
        "\n",
        "# Create Delta table with built-in optimizations:\n",
        "data = {\"payment_type\": [1, 1, 2, 1, 2], \"fare_amount\": [15.5, 20.0, 18.3, 12.5, 25.0]}\n",
        "write_deltalake(\"data/universal_demo\", pd.DataFrame(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pandas benefits from Delta's statistics for efficient filtering:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Pandas gets automatic optimization benefits\n",
        "dt = DeltaTable(\"data/universal_demo\")\n",
        "pandas_result = dt.to_pandas()\n",
        "print(f\"Pandas: {len(pandas_result)} trips, avg ${pandas_result['fare_amount'].mean():.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Pandas: 5 trips, avg $17.66\n",
        "```\n",
        "\n",
        "Polars leverages Delta's column statistics for predicate pushdown:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Polars gets predicate pushdown optimization automatically\n",
        "polars_result = pl.read_delta(\"data/universal_demo\").select([\n",
        "    pl.len().alias(\"trips\"), \n",
        "    pl.col(\"fare_amount\").mean().alias(\"avg_fare\")\n",
        "])\n",
        "print(f\"Polars: {polars_result}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Polars: shape: (1, 2)\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ trips ‚îÜ avg_fare ‚îÇ\n",
        "‚îÇ ---   ‚îÜ ---      ‚îÇ\n",
        "‚îÇ u32   ‚îÜ f64      ‚îÇ\n",
        "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
        "‚îÇ 5     ‚îÜ 18.26    ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "DuckDB uses Delta's statistics for query planning optimization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# DuckDB gets optimized query plans from Delta statistics\n",
        "duckdb_result = duckdb.execute(\"\"\"\n",
        "    SELECT COUNT(*) as trips, ROUND(AVG(fare_amount), 2) as avg_fare\n",
        "    FROM delta_scan('data/universal_demo')\n",
        "\"\"\").fetchone()\n",
        "print(f\"DuckDB: {duckdb_result[0]} trips, ${duckdb_result[1]} avg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "DuckDB: 5 trips, $17.66\n",
        "```\n",
        "\n",
        "One Delta table with universal optimizations benefiting all engines.\n",
        "\n",
        "\n",
        "## Automatic File Cleanup {#automatic-file-cleanup}\n",
        "\n",
        "Every data update creates new files while keeping old versions for time travel. Vacuum identifies files older than your retention period and safely deletes them, freeing storage space without affecting active data or recent history.\n",
        "\n",
        "### Traditional Approach: Manual Cleanup Scripts {#traditional-approach-manual-cleanup-scripts}\n",
        "\n",
        "Traditional workflows require custom scripts to manage file cleanup:\n",
        "\n",
        "```python\n",
        "# Traditional approach - manual file management\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Find old backup files manually\n",
        "old_files = []\n",
        "cutoff_date = datetime.now() - timedelta(days=7)\n",
        "for file in glob.glob(\"data/taxi_backup_*.parquet\"):\n",
        "    file_time = datetime.fromtimestamp(os.path.getmtime(file))\n",
        "    if file_time < cutoff_date:\n",
        "        old_files.append(file)\n",
        "        os.remove(file)  # Manual cleanup with risk\n",
        "```\n",
        "\n",
        "### Delta-rs Approach: Built-in Vacuum Operation {#delta-rs-approach-built-in-vacuum-operation}\n",
        "\n",
        "Delta-rs provides safe, automated cleanup through its `vacuum()` operation, which removes unused transaction files while preserving data integrity. Files become unused when:\n",
        "\n",
        "‚Ä¢ **UPDATE operations** create new versions, leaving old data files unreferenced\n",
        "‚Ä¢ **DELETE operations** remove data, making those files obsolete  \n",
        "‚Ä¢ **Failed transactions** leave temporary files that were never committed\n",
        "‚Ä¢ **Table optimization** consolidates small files, making originals unnecessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Delta-rs vacuum removes unused files safely with ACID protection\n",
        "from deltalake import DeltaTable\n",
        "import os\n",
        "\n",
        "def get_size(path):\n",
        "    \"\"\"Calculate total directory size in MB\"\"\"\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for filename in filenames:\n",
        "            total_size += os.path.getsize(os.path.join(dirpath, filename))\n",
        "    return total_size / (1024 * 1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With our size calculation helper in place, let's measure storage before and after vacuum:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dt = DeltaTable(\"data/taxi_delta_table\")\n",
        "\n",
        "# Measure storage before cleanup\n",
        "before_size = get_size(\"data/taxi_delta_table\")\n",
        "\n",
        "# Safe cleanup - files only deleted if no active readers/writers\n",
        "dt.vacuum(retention_hours=168)  # Built-in safety: won't delete files in use\n",
        "\n",
        "# Measure storage after cleanup\n",
        "after_size = get_size(\"data/taxi_delta_table\")\n",
        "\n",
        "print(f\"Delta vacuum completed safely\")\n",
        "print(f\"Storage before: {before_size:.1f} MB\")\n",
        "print(f\"Storage after: {after_size:.1f} MB\")\n",
        "print(f\"Space reclaimed: {before_size - after_size:.1f} MB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Delta vacuum completed safely\n",
        "Storage before: 8.2 MB\n",
        "Storage after: 5.7 MB\n",
        "Space reclaimed: 2.5 MB\n",
        "```\n",
        "\n",
        "Delta vacuum removed 2.5 MB of obsolete file versions, reducing storage footprint by 30% while maintaining ACID transaction guarantees and time travel capabilities.\n",
        "\n",
        "## Related Tutorials\n",
        "\n",
        "- **Alternative Scaling**: [Scaling Pandas Workflows with PySpark's Pandas API](https://codecut.ai/scaling-pandas-workflows-with-pysparks-pandas-api/) for Spark-based approaches\n",
        "- **Data Versioning**: [Version Control for Data and Models Using DVC](https://codecut.ai/introduction-to-dvc-data-version-control-tool-for-machine-learning-projects-2/) for broader versioning strategies\n",
        "- **DataFrame Performance**: [Polars vs. Pandas: A Fast, Multi-Core Alternative](https://codecut.ai/polars-vs-pandas-a-fast-multi-core-alternative-for-dataframes/) for DataFrame optimization techniques"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}