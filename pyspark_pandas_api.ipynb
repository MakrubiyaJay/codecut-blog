{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advantages of Using Pandas API on Spark Over Pandas {#advantages-of-using-pandas-api-on-spark-over-pandas}\n",
        "\n",
        "> ðŸ“– Read the full article: [Scaling Pandas Workflows with PySpark's Pandas API](https://codecut.ai/scaling-pandas-workflows-with-pysparks-pandas-api/)\n",
        "\n",
        "\n",
        "- **Faster query execution**: Pandas on Spark uses all available CPU cores to parallelize computations, significantly speeding up queries compared to pandas, which is limited to a single core.\n",
        "- **Scalable to larger-than-memory datasets**: Unlike pandas, which requires the entire dataset to fit in memory and often fails with memory errors, Spark can work with datasets that are bigger than your computer's memory by processing small parts at a time.\n",
        "- **Provides access to Spark's battle-tested query optimizer**: Pandas on Spark uses Spark's Catalyst optimizer, which automatically improves queries by selecting only the needed columns and filtering rows early.\n",
        "\n",
        "![Pandas on Spark Architecture](https://codecut.ai/wp-content/uploads/2025/04/Pandas-on-spark.png)\n",
        "\n",
        "## Setup {#setup}\n",
        "\n",
        "First, install PySpark if you haven't:\n",
        "\n",
        "```bash\n",
        "pip install pyspark\n",
        "```\n",
        "\n",
        "Then, start a local Spark session:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.pandas as ps\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> ðŸ’» **Get the Code**: The complete source code and Jupyter notebook for this tutorial are available on [GitHub](https://github.com/CodeCutTech/Data-science/blob/master/data_science_tools/pandas_api_on_spark.ipynb). Clone it to follow along!\n",
        "\n",
        "## Object Creation {#object-creation}\n",
        "\n",
        "You can create a pandas-on-Spark Series or DataFrames using the same syntax as pandas:\n",
        "\n",
        "Create a pandas-on Spark Series:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ps_s = ps.Series([1, 3, 5, 6, 8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a pandas-on Spark DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "ps_df = ps.DataFrame(\n",
        "    {\"id\": np.arange(1, 1_000_001), \"value\": np.random.randn(1_000_000)}\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can even convert an existing pandas object to a pandas-on-Spark easily:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ps_df = ps.from_pandas(pandas_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Operations {#basic-operations}\n",
        "\n",
        "You can perform operations like with pandas, but now it's distributed. Here are some examples of basic operations using the pandas API on Spark:\n",
        "\n",
        "Compute basic statistics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ps_df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```python\n",
        "                   id           value\n",
        "count  1000000.000000  1000000.000000\n",
        "mean    500000.500000       -0.000697\n",
        "std     288675.278932        0.999534\n",
        "min          1.000000       -5.051222\n",
        "25%     250000.750000       -0.674671\n",
        "50%     500000.500000       -0.000586\n",
        "75%     750000.250000        0.672834\n",
        "max    1000000.000000        4.553696\n",
        "```\n",
        "\n",
        "Get the first few rows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ps_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```python\n",
        "   id     value\n",
        "0   1 -3.334066\n",
        "1   2  0.966236\n",
        "2   3 -1.148075\n",
        "3   4  1.108155\n",
        "4   5 -0.049615\n",
        "```\n",
        "\n",
        "Filter rows and drop any NaN values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "filtered_df = ps_df.where(ps_df.value > 0).dropna()\n",
        "filtered_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```python\n",
        "      id     value\n",
        "1    2.0  0.966236\n",
        "3    4.0  1.108155\n",
        "9   10.0  0.562544\n",
        "12  13.0  0.809431\n",
        "13  14.0  1.478501\n",
        "```\n",
        "\n",
        "## GroupBy {#groupby}\n",
        "\n",
        "Grouping work similarly but happen in parallel across partitions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a sample DataFrame\n",
        "ps_df_2 = ps.DataFrame(\n",
        "    {\"category\": [\"A\", \"B\", \"A\", \"C\", \"B\"], \"value\": [10, 20, 15, 30, 25]}\n",
        ")\n",
        "\n",
        "# Compute mean value by category\n",
        "ps_df_2.groupby(\"category\").value.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```python\n",
        "category\n",
        "A    12.5\n",
        "B    22.5\n",
        "C    30.0\n",
        "Name: value, dtype: float64\n",
        "```\n",
        "\n",
        "## Plotting {#plotting}\n",
        "\n",
        "Basic plotting is supported. Below are some examples:\n",
        "\n",
        "Plot a histogram:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ps_df[\"value\"].plot.hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Histogram Plot](https://codecut.ai/wp-content/uploads/2025/04/result-1-1.png)\n",
        "\n",
        "Plot a bar graph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ps_df_2.plot.bar(x=\"category\", y=\"value\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Bar Graph](https://codecut.ai/wp-content/uploads/2025/04/newplot.png)\n",
        "\n",
        "## Reading and Writing Data {#reading-and-writing-data}\n",
        "\n",
        "You can easily load and save datasets in common formats. For examples, you can:\n",
        "\n",
        "Read and write to CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write to CSV\n",
        "ps_df.to_csv(\"output_data.csv\")\n",
        "\n",
        "# Read back\n",
        "new_df = ps.read_csv(\"output_data.csv\")\n",
        "print(new_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read and write to Parquet:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write to Parquet\n",
        "ps_df.to_parquet(\"output_data.parquet\")\n",
        "\n",
        "# Read back\n",
        "new_parquet_df = ps.read_parquet(\"output_data.parquet\")\n",
        "print(new_parquet_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Pandas API on Spark with Regular Pandas {#using-pandas-api-on-spark-with-regular-pandas}\n",
        "\n",
        "Combining Pandas API on Spark with pandas to get the best of both worlds is often useful. For example, you can clean and aggregate a large dataset with Pandas API on Spark to benefit from fast, parallel processing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pyspark.pandas as ps\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create a large Pandas API on Spark DataFrame\n",
        "psdf = ps.DataFrame({\n",
        "    \"feature1\": range(1_000_000),\n",
        "    \"feature2\": range(1_000_000, 2_000_000),\n",
        "    \"target\": range(500_000, 1_500_000)\n",
        "})\n",
        "print(f\"Length of the original DataFrame: {len(psdf):,}\")\n",
        "\n",
        "# Aggregate the data to a smaller size\n",
        "aggregated = psdf.groupby(psdf.feature1 // 10000).mean()\n",
        "print(f\"Length of the aggregated DataFrame: {len(aggregated):,}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```python\n",
        "Length of the original DataFrame: 1,000,000\n",
        "Length of the aggregated DataFrame: 100\n",
        "```\n",
        "\n",
        "Once the dataset is small enough, you can convert it to a pandas DataFrame using `.to_pandas()` and then apply a scikit-learn machine learning model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert to pandas DataFrame\n",
        "small_pdf = aggregated.to_pandas()\n",
        "\n",
        "# Train a scikit-learn model\n",
        "model = LinearRegression()\n",
        "X = small_pdf[[\"feature1\", \"feature2\"]]\n",
        "y = small_pdf[\"target\"]\n",
        "model.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This approach works well if the cleaned and aggregated data fits comfortably into memory.\n",
        "\n",
        "## Pandas API on Spark vs. Pandas: Query Execution Model {#pandas-api-on-spark-vs-pandas-query-execution-model}\n",
        "\n",
        "Pandas API on Spark executes queries differently than pandas:\n",
        "\n",
        "- **Pandas API on Spark** uses lazy evaluation. It builds a logical query plan, optimizes it, and only executes when results are requested\n",
        "- **Pandas** uses eager evaluation. It loads data into memory immediately and performs each operation as it is called, without optimizations\n",
        "\n",
        "**Example in pandas (eager execution):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pandas_df[\"value\"] = pandas_df[\"value\"] + 1  # Operation executes immediately\n",
        "print(pandas_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "```python\n",
        "   value\n",
        "0      2\n",
        "1      3\n",
        "2      4\n",
        "3      5\n",
        "4      6\n",
        "```\n",
        "\n",
        "**Example in Pandas API on Spark (lazy execution):**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Using Pandas API on Spark\n",
        "updated_psdf = ps_df.assign(a=ps_df[\"value\"] + 1)  # Lazy operation\n",
        "print(updated_psdf.head())  # Triggers actual computation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "```python\n",
        "   id     value         a\n",
        "0   1 -0.002641  0.997359\n",
        "1   2 -1.818039 -0.818039\n",
        "2   3  2.371413  3.371413\n",
        "3   4  0.909148  1.909148\n",
        "4   5  2.365013  3.365013\n",
        "```\n",
        "\n",
        "## Pandas API on Spark vs. PySpark Differences {#pandas-api-on-spark-vs-pyspark-differences}\n",
        "\n",
        "Both Pandas API on Spark and PySpark generate logical query plans and optimize execution with Spark. As a result, their performance is often similar.\n",
        "\n",
        "The main difference is **syntax**: Pandas API on Spark follows a pandas-like syntax, while PySpark follows Spark SQL/DataFrame syntax.\n",
        "\n",
        "Pandas API on Spark syntax:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pandas_spark_df = ps.DataFrame({\"col1\": [1, 2, 3], \"col2\": [4, 5, 6]})\n",
        "(pandas_spark_df[\"col1\"] + pandas_spark_df[\"col2\"]).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PySpark syntax:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "pyspark_df = spark.createDataFrame([(1, 4), (2, 5), (3, 6)], [\"col1\", \"col2\"])\n",
        "pyspark_df.select((col(\"col1\") + col(\"col2\")).alias(\"sum\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can easily convert a Pandas-on-Spark DataFrame to a Spark DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert Pandas API on Spark DataFrame to PySpark DataFrame\n",
        "spark_native_df = pandas_spark_df.to_spark()\n",
        "\n",
        "# Now you can use full PySpark functionality\n",
        "spark_native_df.select((col(\"col1\") + col(\"col2\")).alias(\"sum\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}