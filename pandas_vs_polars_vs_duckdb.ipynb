{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tool Strengths at a Glance\n",
        "\n",
        "> ğŸ“– Read the full article: [pandas vs Polars vs DuckDB: A Data Scientist''s Guide to Choosing the Right](https://codecut.ai/pandas-vs-polars-vs-duckdb-comparison/)\n",
        "\n",
        "\n",
        "### pandas\n",
        "\n",
        "[pandas](https://github.com/pandas-dev/pandas) is the original DataFrame library for Python that excels at interactive data exploration and integrates seamlessly with the ML ecosystem. Key capabilities include:\n",
        "\n",
        "- Direct compatibility with scikit-learn, statsmodels, and visualization libraries\n",
        "- Rich ecosystem of extensions (pandas-profiling, pandasql, etc.)\n",
        "- Mature time series functionality\n",
        "- Familiar syntax that most data scientists already know\n",
        "\n",
        "### Polars\n",
        "\n",
        "[Polars](https://github.com/pola-rs/polars) is a Rust-powered DataFrame library designed for speed that brings multi-threaded execution and query optimization to Python. Key capabilities include:\n",
        "\n",
        "- Speeds up operations by using all available CPU cores by default\n",
        "- Builds a query plan first, then executes only what's needed\n",
        "- Streaming mode for processing datasets larger than RAM\n",
        "- Expressive method chaining with a pandas-like API\n",
        "\n",
        "### DuckDB\n",
        "\n",
        "[DuckDB](https://github.com/duckdb/duckdb) is an embedded SQL database optimized for analytics that brings database-level query optimization to local files. Key capabilities include:\n",
        "\n",
        "- Native SQL syntax with full analytical query support\n",
        "- Queries CSV, Parquet, and JSON files directly without loading\n",
        "- Uses disk storage automatically when data exceeds available memory\n",
        "- Zero-configuration embedded database requiring no server setup\n",
        "\n",
        "## Setup\n",
        "\n",
        "Install all three libraries:\n",
        "\n",
        "```bash\n",
        "pip install pandas polars duckdb\n",
        "```\n",
        "\n",
        "Generate sample data for benchmarking:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "n_rows = 5_000_000\n",
        "\n",
        "data = {\n",
        "    \"category\": np.random.choice([\"Electronics\", \"Clothing\", \"Food\", \"Books\"], size=n_rows),\n",
        "    \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], size=n_rows),\n",
        "    \"amount\": np.random.rand(n_rows) * 1000,\n",
        "    \"quantity\": np.random.randint(1, 100, size=n_rows),\n",
        "}\n",
        "\n",
        "df_pandas = pd.DataFrame(data)\n",
        "df_pandas.to_csv(\"sales_data.csv\", index=False)\n",
        "print(f\"Created sales_data.csv with {n_rows:,} rows\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "Created sales_data.csv with 5,000,000 rows\n",
        "```\n",
        "\n",
        "## Syntax Comparison\n",
        "\n",
        "All three tools can perform the same operations with different syntax. Here's a side-by-side comparison of common tasks.\n",
        "\n",
        "### Filtering Rows\n",
        "\n",
        "**pandas:**\n",
        "Uses bracket notation with boolean conditions, which is concise but can become hard to read with complex conditions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_pd = pd.read_csv(\"sales_data.csv\")\n",
        "result_pd = df_pd[(df_pd[\"amount\"] > 500) & (df_pd[\"category\"] == \"Electronics\")]\n",
        "result_pd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|    | category    | region | amount     | quantity |\n",
        "|----|-------------|--------|------------|----------|\n",
        "| 7  | Electronics | West   | 662.803066 | 80       |\n",
        "| 15 | Electronics | North  | 826.004963 | 25       |\n",
        "| 30 | Electronics | North  | 766.081832 | 7        |\n",
        "| 31 | Electronics | West   | 772.084261 | 36       |\n",
        "| 37 | Electronics | East   | 527.967145 | 35       |\n",
        "\n",
        "**Polars:**\n",
        "Uses method chaining with `pl.col()` expressions, avoiding the repeated `df[\"column\"]` references required by pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "\n",
        "df_pl = pl.read_csv(\"sales_data.csv\")\n",
        "result_pl = df_pl.filter(\n",
        "    (pl.col(\"amount\") > 500) & (pl.col(\"category\") == \"Electronics\")\n",
        ")\n",
        "result_pl.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| category      | region  | amount     | quantity |\n",
        "|---------------|---------|------------|----------|\n",
        "| str           | str     | f64        | i64      |\n",
        "| \"Electronics\" | \"West\"  | 662.803066 | 80       |\n",
        "| \"Electronics\" | \"North\" | 826.004963 | 25       |\n",
        "| \"Electronics\" | \"North\" | 766.081832 | 7        |\n",
        "| \"Electronics\" | \"West\"  | 772.084261 | 36       |\n",
        "| \"Electronics\" | \"East\"  | 527.967145 | 35       |\n",
        "\n",
        "\n",
        "**DuckDB:**\n",
        "Uses standard SQL with a `WHERE` clause, which is more readable by those who know SQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import duckdb\n",
        "\n",
        "result_duckdb = duckdb.sql(\"\"\"\n",
        "    SELECT * FROM 'sales_data.csv'\n",
        "    WHERE amount > 500 AND category = 'Electronics'\n",
        "\"\"\").df()\n",
        "result_duckdb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|   | category    | region | amount     | quantity |\n",
        "|---|-------------|--------|------------|----------|\n",
        "| 0 | Electronics | West   | 662.803066 | 80       |\n",
        "| 1 | Electronics | North  | 826.004963 | 25       |\n",
        "| 2 | Electronics | North  | 766.081832 | 7        |\n",
        "| 3 | Electronics | West   | 772.084261 | 36       |\n",
        "| 4 | Electronics | East   | 527.967145 | 35       |\n",
        "\n",
        "\n",
        "### Selecting Columns\n",
        "\n",
        "**pandas:**\n",
        "Double brackets return a DataFrame with selected columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_pd = df_pd[[\"category\", \"amount\"]]\n",
        "result_pd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|   | category    | amount     |\n",
        "|---|-------------|------------|\n",
        "| 0 | Food        | 516.653322 |\n",
        "| 1 | Books       | 937.337226 |\n",
        "| 2 | Electronics | 450.941022 |\n",
        "| 3 | Food        | 674.488081 |\n",
        "| 4 | Food        | 188.847906 |\n",
        "\n",
        "\n",
        "**Polars:**\n",
        "The `select()` method clearly communicates column selection intent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_pl = df_pl.select([\"category\", \"amount\"])\n",
        "result_pl.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| category      | amount     |\n",
        "|---------------|------------|\n",
        "| str           | f64        |\n",
        "| \"Food\"        | 516.653322 |\n",
        "| \"Books\"       | 937.337226 |\n",
        "| \"Electronics\" | 450.941022 |\n",
        "| \"Food\"        | 674.488081 |\n",
        "| \"Food\"        | 188.847906 |\n",
        "\n",
        "\n",
        "**DuckDB:**\n",
        "SQL's `SELECT` clause makes column selection intuitive for SQL users."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_duckdb = duckdb.sql(\"\"\"\n",
        "    SELECT category, amount FROM 'sales_data.csv'\n",
        "\"\"\").df()\n",
        "result_duckdb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|   | category    | amount     |\n",
        "|---|-------------|------------|\n",
        "| 0 | Food        | 516.653322 |\n",
        "| 1 | Books       | 937.337226 |\n",
        "| 2 | Electronics | 450.941022 |\n",
        "| 3 | Food        | 674.488081 |\n",
        "| 4 | Food        | 188.847906 |\n",
        "\n",
        "\n",
        "### GroupBy Aggregation\n",
        "\n",
        "**pandas:**\n",
        "Uses a dictionary to specify aggregations, but returns multi-level column headers that often require flattening before further use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_pd = df_pd.groupby(\"category\").agg({\n",
        "    \"amount\": [\"sum\", \"mean\"],\n",
        "    \"quantity\": \"sum\"\n",
        "})\n",
        "result_pd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|             | amount       |            | quantity |\n",
        "|             | sum          | mean       | sum      |\n",
        "|-------------|--------------|------------|----------|\n",
        "| Books       | 6.247506e+08 | 499.998897 | 62463285 |\n",
        "| Clothing    | 6.253924e+08 | 500.139837 | 62505224 |\n",
        "| Electronics | 6.244453e+08 | 499.938189 | 62484265 |\n",
        "| Food        | 6.254034e+08 | 499.916417 | 62577943 |\n",
        "\n",
        "**Polars:**\n",
        "Uses explicit `alias()` calls for each aggregation, producing flat column names directly without post-processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_pl = df_pl.group_by(\"category\").agg([\n",
        "    pl.col(\"amount\").sum().alias(\"amount_sum\"),\n",
        "    pl.col(\"amount\").mean().alias(\"amount_mean\"),\n",
        "    pl.col(\"quantity\").sum().alias(\"quantity_sum\"),\n",
        "])\n",
        "result_pl.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| category      | amount_sum | amount_mean | quantity_sum |\n",
        "|---------------|------------|-------------|--------------|\n",
        "| str           | f64        | f64         | i64          |\n",
        "| \"Clothing\"    | 6.2539e8   | 500.139837  | 62505224     |\n",
        "| \"Books\"       | 6.2475e8   | 499.998897  | 62463285     |\n",
        "| \"Electronics\" | 6.2445e8   | 499.938189  | 62484265     |\n",
        "| \"Food\"        | 6.2540e8   | 499.916417  | 62577943     |\n",
        "\n",
        "\n",
        "**DuckDB:**\n",
        "Standard SQL aggregation with column aliases produces clean, flat output ready for downstream use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_duckdb = duckdb.sql(\"\"\"\n",
        "    SELECT\n",
        "        category,\n",
        "        SUM(amount) as amount_sum,\n",
        "        AVG(amount) as amount_mean,\n",
        "        SUM(quantity) as quantity_sum\n",
        "    FROM 'sales_data.csv'\n",
        "    GROUP BY category\n",
        "\"\"\").df()\n",
        "result_duckdb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|   | category    | amount_sum   | amount_mean | quantity_sum |\n",
        "|---|-------------|--------------|-------------|--------------|\n",
        "| 0 | Food        | 6.254034e+08 | 499.916417  | 62577943.0   |\n",
        "| 1 | Electronics | 6.244453e+08 | 499.938189  | 62484265.0   |\n",
        "| 2 | Clothing    | 6.253924e+08 | 500.139837  | 62505224.0   |\n",
        "| 3 | Books       | 6.247506e+08 | 499.998897  | 62463285.0   |\n",
        "\n",
        "### Adding Columns\n",
        "\n",
        "**pandas:**\n",
        "The `assign()` method creates new columns with repeated DataFrame references like `df_pd[\"amount\"]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_pd = df_pd.assign(\n",
        "    amount_with_tax=df_pd[\"amount\"] * 1.1,\n",
        "    high_value=df_pd[\"amount\"] > 500\n",
        ")\n",
        "result_pd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|   | category    | region | amount     | quantity | amount_with_tax | high_value |\n",
        "|---|-------------|--------|------------|----------|-----------------|------------|\n",
        "| 0 | Food        | South  | 516.653322 | 40       | 568.318654      | True       |\n",
        "| 1 | Books       | East   | 937.337226 | 45       | 1031.070948     | True       |\n",
        "| 2 | Electronics | North  | 450.941022 | 93       | 496.035124      | False      |\n",
        "| 3 | Food        | East   | 674.488081 | 46       | 741.936889      | True       |\n",
        "| 4 | Food        | East   | 188.847906 | 98       | 207.732697      | False      |\n",
        "\n",
        "**Polars:**\n",
        "The `with_columns()` method uses composable expressions that chain naturally without repeating the DataFrame name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_pl = df_pl.with_columns([\n",
        "    (pl.col(\"amount\") * 1.1).alias(\"amount_with_tax\"),\n",
        "    (pl.col(\"amount\") > 500).alias(\"high_value\")\n",
        "])\n",
        "result_pl.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| category      | region  | amount     | quantity | amount_with_tax | high_value |\n",
        "|---------------|---------|------------|----------|-----------------|------------|\n",
        "| str           | str     | f64        | i64      | f64             | bool       |\n",
        "| \"Food\"        | \"South\" | 516.653322 | 40       | 568.318654      | true       |\n",
        "| \"Books\"       | \"East\"  | 937.337226 | 45       | 1031.070948     | true       |\n",
        "| \"Electronics\" | \"North\" | 450.941022 | 93       | 496.035124      | false      |\n",
        "| \"Food\"        | \"East\"  | 674.488081 | 46       | 741.936889      | true       |\n",
        "| \"Food\"        | \"East\"  | 188.847906 | 98       | 207.732697      | false      |\n",
        "\n",
        "**DuckDB:**\n",
        "SQL's `SELECT` clause defines new columns directly in the query, keeping transformations readable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_duckdb = duckdb.sql(\"\"\"\n",
        "    SELECT *,\n",
        "        amount * 1.1 as amount_with_tax,\n",
        "        amount > 500 as high_value\n",
        "    FROM df_pd\n",
        "\"\"\").df()\n",
        "result_duckdb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|   | category    | region | amount     | quantity | amount_with_tax | high_value |\n",
        "|---|-------------|--------|------------|----------|-----------------|------------|\n",
        "| 0 | Food        | South  | 516.653322 | 40       | 568.318654      | True       |\n",
        "| 1 | Books       | East   | 937.337226 | 45       | 1031.070948     | True       |\n",
        "| 2 | Electronics | North  | 450.941022 | 93       | 496.035124      | False      |\n",
        "| 3 | Food        | East   | 674.488081 | 46       | 741.936889      | True       |\n",
        "| 4 | Food        | East   | 188.847906 | 98       | 207.732697      | False      |\n",
        "\n",
        "### Conditional Logic\n",
        "\n",
        "**pandas:**\n",
        "Requires `np.where()` for simple conditions or slow `apply()` for complex logic, which breaks method chaining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "result_pd = df_pd.assign(\n",
        "    value_tier=np.where(\n",
        "        df_pd[\"amount\"] > 700, \"high\",\n",
        "        np.where(df_pd[\"amount\"] > 300, \"medium\", \"low\")\n",
        "    )\n",
        ")\n",
        "result_pd[[\"category\", \"amount\", \"value_tier\"]].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|   | category    | amount     | value_tier |\n",
        "|---|-------------|------------|------------|\n",
        "| 0 | Food        | 516.653322 | medium     |\n",
        "| 1 | Books       | 937.337226 | high       |\n",
        "| 2 | Electronics | 450.941022 | medium     |\n",
        "| 3 | Food        | 674.488081 | medium     |\n",
        "| 4 | Food        | 188.847906 | low        |\n",
        "\n",
        "**Polars:**\n",
        "The `when().then().otherwise()` chain is readable and integrates naturally with method chaining."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_pl = df_pl.with_columns(\n",
        "    pl.when(pl.col(\"amount\") > 700).then(pl.lit(\"high\"))\n",
        "      .when(pl.col(\"amount\") > 300).then(pl.lit(\"medium\"))\n",
        "      .otherwise(pl.lit(\"low\"))\n",
        "      .alias(\"value_tier\")\n",
        ")\n",
        "result_pl.select([\"category\", \"amount\", \"value_tier\"]).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| category      | amount     | value_tier |\n",
        "|---------------|------------|------------|\n",
        "| str           | f64        | str        |\n",
        "| \"Food\"        | 516.653322 | \"medium\"   |\n",
        "| \"Books\"       | 937.337226 | \"high\"     |\n",
        "| \"Electronics\" | 450.941022 | \"medium\"   |\n",
        "| \"Food\"        | 674.488081 | \"medium\"   |\n",
        "| \"Food\"        | 188.847906 | \"low\"      |\n",
        "\n",
        "**DuckDB:**\n",
        "Standard SQL `CASE WHEN` syntax is more readable by those who know SQL."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_duckdb = duckdb.sql(\"\"\"\n",
        "    SELECT category, amount,\n",
        "        CASE\n",
        "            WHEN amount > 700 THEN 'high'\n",
        "            WHEN amount > 300 THEN 'medium'\n",
        "            ELSE 'low'\n",
        "        END as value_tier\n",
        "    FROM df_pd\n",
        "\"\"\").df()\n",
        "result_duckdb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|   | category    | amount     | value_tier |\n",
        "|---|-------------|------------|------------|\n",
        "| 0 | Food        | 516.653322 | medium     |\n",
        "| 1 | Books       | 937.337226 | high       |\n",
        "| 2 | Electronics | 450.941022 | medium     |\n",
        "| 3 | Food        | 674.488081 | medium     |\n",
        "| 4 | Food        | 188.847906 | low        |\n",
        "\n",
        "### Window Functions\n",
        "\n",
        "**pandas:**\n",
        "Uses `groupby().transform()` which requires repeating the groupby clause for each calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_pd = df_pd.assign(\n",
        "    category_avg=df_pd.groupby(\"category\")[\"amount\"].transform(\"mean\"),\n",
        "    category_rank=df_pd.groupby(\"category\")[\"amount\"].rank(ascending=False)\n",
        ")\n",
        "result_pd[[\"category\", \"amount\", \"category_avg\", \"category_rank\"]].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|   | category    | amount     | category_avg | category_rank |\n",
        "|---|-------------|------------|--------------|---------------|\n",
        "| 0 | Food        | 516.653322 | 499.916417   | 604342.0      |\n",
        "| 1 | Books       | 937.337226 | 499.998897   | 78423.0       |\n",
        "| 2 | Electronics | 450.941022 | 499.938189   | 685881.0      |\n",
        "| 3 | Food        | 674.488081 | 499.916417   | 407088.0      |\n",
        "| 4 | Food        | 188.847906 | 499.916417   | 1015211.0     |\n",
        "\n",
        "**Polars:**\n",
        "The `over()` expression appends the partition to any calculation, avoiding repeated group definitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_pl = df_pl.with_columns([\n",
        "    pl.col(\"amount\").mean().over(\"category\").alias(\"category_avg\"),\n",
        "    pl.col(\"amount\").rank(descending=True).over(\"category\").alias(\"category_rank\")\n",
        "])\n",
        "result_pl.select([\"category\", \"amount\", \"category_avg\", \"category_rank\"]).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| category      | amount     | category_avg | category_rank |\n",
        "|---------------|------------|--------------|---------------|\n",
        "| str           | f64        | f64          | f64           |\n",
        "| \"Food\"        | 516.653322 | 499.916417   | 604342.0      |\n",
        "| \"Books\"       | 937.337226 | 499.998897   | 78423.0       |\n",
        "| \"Electronics\" | 450.941022 | 499.938189   | 685881.0      |\n",
        "| \"Food\"        | 674.488081 | 499.916417   | 407088.0      |\n",
        "| \"Food\"        | 188.847906 | 499.916417   | 1015211.0     |\n",
        "\n",
        "**DuckDB:**\n",
        "SQL window functions with `OVER (PARTITION BY ...)` are the industry standard for this type of calculation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_duckdb = duckdb.sql(\"\"\"\n",
        "    SELECT category, amount,\n",
        "        AVG(amount) OVER (PARTITION BY category) as category_avg,\n",
        "        RANK() OVER (PARTITION BY category ORDER BY amount DESC) as category_rank\n",
        "    FROM df_pd\n",
        "\"\"\").df()\n",
        "result_duckdb.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|   | category | amount     | category_avg | category_rank |\n",
        "|---|----------|------------|--------------|---------------|\n",
        "| 0 | Clothing | 513.807166 | 500.139837   | 608257        |\n",
        "| 1 | Clothing | 513.806596 | 500.139837   | 608258        |\n",
        "| 2 | Clothing | 513.806515 | 500.139837   | 608259        |\n",
        "| 3 | Clothing | 513.806063 | 500.139837   | 608260        |\n",
        "| 4 | Clothing | 513.806056 | 500.139837   | 608261        |\n",
        "\n",
        "## Data Loading Performance\n",
        "\n",
        "pandas reads CSV files on a single CPU core. Polars and DuckDB use multi-threaded execution, distributing the work across all available cores to read different parts of the file simultaneously.\n",
        "\n",
        "### pandas\n",
        "\n",
        "Single-threaded CSV parsing loads data sequentially.\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ CPU Core 1                                  â”‚\n",
        "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
        "â”‚ â”‚ Chunk 1 â†’ Chunk 2 â†’ Chunk 3 â†’ ... â†’ End â”‚ â”‚\n",
        "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
        "â”‚ CPU Core 2  [idle]                          â”‚\n",
        "â”‚ CPU Core 3  [idle]                          â”‚\n",
        "â”‚ CPU Core 4  [idle]                          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pandas_time = %timeit -o pd.read_csv(\"sales_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "1.05 s Â± 26.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
        "```\n",
        "\n",
        "### Polars\n",
        "\n",
        "Multi-threaded parsing reads multiple chunks simultaneously.\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ CPU Core 1  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
        "â”‚             â”‚ Chunk 1        â”‚              â”‚\n",
        "â”‚ CPU Core 2  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
        "â”‚             â”‚ Chunk 2        â”‚              â”‚\n",
        "â”‚ CPU Core 3  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
        "â”‚             â”‚ Chunk 3        â”‚              â”‚\n",
        "â”‚ CPU Core 4  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
        "â”‚             â”‚ Chunk 4        â”‚              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "polars_time = %timeit -o pl.read_csv(\"sales_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "137 ms Â± 34 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
        "```\n",
        "\n",
        "### DuckDB\n",
        "\n",
        "Vectorized execution processes data in batches across cores.\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ CPU Core 1                                  â”‚\n",
        "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
        "â”‚ â”‚ Batch 1 â”‚ â”‚ Batch 2 â”‚ â”‚ Batch 3 â”‚  ...    â”‚\n",
        "â”‚ â”‚  1024   â”‚ â”‚  1024   â”‚ â”‚  1024   â”‚         â”‚\n",
        "â”‚ â”‚  rows   â”‚ â”‚  rows   â”‚ â”‚  rows   â”‚         â”‚\n",
        "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "duckdb_time = %timeit -o duckdb.sql(\"SELECT * FROM 'sales_data.csv'\").df()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "762 ms Â± 77.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Polars is {pandas_time.average / polars_time.average:.1f}Ã— faster than pandas\")\n",
        "print(f\"DuckDB is {pandas_time.average / duckdb_time.average:.1f}Ã— faster than pandas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "Polars is 7.7Ã— faster than pandas\n",
        "DuckDB is 1.4Ã— faster than pandas\n",
        "```\n",
        "\n",
        "While Polars leads with a 7.7Ã— speedup in CSV reading, DuckDB's 1.4Ã— improvement shows parsing isn't its focus. DuckDB shines when querying files directly or running complex analytical queries.\n",
        "\n",
        "## Query Optimization\n",
        "\n",
        "### pandas: No Optimization\n",
        "\n",
        "pandas executes operations eagerly, creating intermediate DataFrames at each step. This wastes memory and prevents optimization.\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Step 1: Load ALL rows          â†’ 10M rows in memory         â”‚\n",
        "â”‚ Step 2: Filter (amount > 100)  â†’ 5M rows in memory          â”‚\n",
        "â”‚ Step 3: GroupBy                â†’ New DataFrame              â”‚\n",
        "â”‚ Step 4: Mean                   â†’ Final result               â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "Memory: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (high - stores all intermediates)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def pandas_query():\n",
        "    return (\n",
        "        pd.read_csv(\"sales_data.csv\")\n",
        "        .query('amount > 100')\n",
        "        .groupby('category')['amount']\n",
        "        .mean()\n",
        "    )\n",
        "\n",
        "pandas_opt_time = %timeit -o pandas_query()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "1.46 s Â± 88.9 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
        "```\n",
        "\n",
        "This approach has three problems:\n",
        "\n",
        "- **Full CSV load**: All rows are read before filtering\n",
        "- **No predicate pushdown**: Rows are filtered after loading the entire file into memory\n",
        "- **No projection pushdown**: All columns are loaded, even unused ones\n",
        "\n",
        "### Polars: Lazy Evaluation\n",
        "\n",
        "Polars supports lazy evaluation, which builds a query plan and optimizes it before execution:\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Query Plan Built:                                           â”‚\n",
        "â”‚   scan_csv â†’ filter â†’ group_by â†’ agg                        â”‚\n",
        "â”‚                                                             â”‚\n",
        "â”‚ Optimizations Applied:                                      â”‚\n",
        "â”‚   â€¢ Predicate pushdown (filter during scan)                 â”‚\n",
        "â”‚   â€¢ Projection pushdown (read only needed columns)          â”‚\n",
        "â”‚   â€¢ Multi-threaded execution (parallel across CPU cores)    â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "Memory: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (low - no intermediate DataFrames)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "query_pl = (\n",
        "    pl.scan_csv(\"sales_data.csv\")\n",
        "    .filter(pl.col(\"amount\") > 100)\n",
        "    .group_by(\"category\")\n",
        "    .agg(pl.col(\"amount\").mean().alias(\"avg_amount\"))\n",
        ")\n",
        "\n",
        "# View the optimized query plan\n",
        "print(query_pl.explain())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "AGGREGATE[maintain_order: false]\n",
        "  [col(\"amount\").mean().alias(\"avg_amount\")] BY [col(\"category\")]\n",
        "  FROM\n",
        "  Csv SCAN [sales_data.csv] [id: 4687118704]\n",
        "  PROJECT 2/4 COLUMNS\n",
        "  SELECTION: [(col(\"amount\")) > (100.0)]\n",
        "```\n",
        "\n",
        "The query plan shows these optimizations:\n",
        "\n",
        "- **Predicate pushdown**: `SELECTION` filters during scan, not after loading\n",
        "- **Projection pushdown**: `PROJECT 2/4 COLUMNS` reads only what's needed\n",
        "- **Operation reordering**: Aggregate runs on filtered data, not the full dataset\n",
        "\n",
        "Execute the optimized query:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def polars_query():\n",
        "    return (\n",
        "        pl.scan_csv(\"sales_data.csv\")\n",
        "        .filter(pl.col(\"amount\") > 100)\n",
        "        .group_by(\"category\")\n",
        "        .agg(pl.col(\"amount\").mean().alias(\"avg_amount\"))\n",
        "        .collect()\n",
        "    )\n",
        "\n",
        "polars_opt_time = %timeit -o polars_query()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "148 ms Â± 32.3 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
        "```\n",
        "\n",
        "### DuckDB: SQL Optimizer\n",
        "\n",
        "DuckDB's SQL optimizer applies similar optimizations automatically:\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Query Plan Built:                                           â”‚\n",
        "â”‚   SQL â†’ Parser â†’ Optimizer â†’ Execution Plan                 â”‚\n",
        "â”‚                                                             â”‚\n",
        "â”‚ Optimizations Applied:                                      â”‚\n",
        "â”‚   â€¢ Predicate pushdown (WHERE during scan)                  â”‚\n",
        "â”‚   â€¢ Projection pushdown (SELECT only needed columns)        â”‚\n",
        "â”‚   â€¢ Vectorized execution (process 1024 rows per batch)      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "Memory: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (low - streaming execution)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def duckdb_query():\n",
        "    return duckdb.sql(\"\"\"\n",
        "        SELECT category, AVG(amount) as avg_amount\n",
        "        FROM 'sales_data.csv'\n",
        "        WHERE amount > 100\n",
        "        GROUP BY category\n",
        "    \"\"\").df()\n",
        "\n",
        "duckdb_opt_time = %timeit -o duckdb_query()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "245 ms Â± 12.1 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
        "```\n",
        "\n",
        "Let's compare the performance of the optimized queries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Polars is {pandas_opt_time.average / polars_opt_time.average:.1f}Ã— faster than pandas\")\n",
        "print(f\"DuckDB is {pandas_opt_time.average / duckdb_opt_time.average:.1f}Ã— faster than pandas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "Polars is 9.9Ã— faster than pandas\n",
        "DuckDB is 6.0Ã— faster than pandas\n",
        "```\n",
        "\n",
        "Polars outperforms DuckDB (9.9Ã— vs 6.0Ã—) in this benchmark because its Rust-based engine handles the filter-then-aggregate pattern efficiently. DuckDB's strength lies in complex SQL queries with joins and subqueries.\n",
        "\n",
        "## GroupBy Performance\n",
        "\n",
        "Computing aggregates requires scanning every row, a workload that scales linearly with CPU cores. This makes groupby operations the clearest test of parallel execution.\n",
        "\n",
        "Let's load the data for the groupby benchmarks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load data for fair comparison\n",
        "df_pd = pd.read_csv(\"sales_data.csv\")\n",
        "df_pl = pl.read_csv(\"sales_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### pandas: Single-Threaded\n",
        "\n",
        "pandas processes groupby operations on a single CPU core, which becomes a bottleneck on large datasets.\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ CPU Core 1                                                  â”‚\n",
        "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
        "â”‚ â”‚ Group A â†’ Group B â†’ Group C â†’ Group D â†’ ... â†’ Aggregate â”‚ â”‚\n",
        "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
        "â”‚ CPU Core 2  [idle]                                          â”‚\n",
        "â”‚ CPU Core 3  [idle]                                          â”‚\n",
        "â”‚ CPU Core 4  [idle]                                          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def pandas_groupby():\n",
        "    return df_pd.groupby(\"category\")[\"amount\"].mean()\n",
        "\n",
        "pandas_groupby_time = %timeit -o pandas_groupby()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "271 ms Â± 135 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n",
        "```\n",
        "\n",
        "### Polars: Multi-Threaded\n",
        "\n",
        "Polars partitions data across CPU cores, computes partial aggregates in parallel, then merges the results.\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ CPU Core 1  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
        "â”‚             â”‚ Group A, B   â”‚ â†’ Partial Aggregate            â”‚\n",
        "â”‚ CPU Core 2  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
        "â”‚             â”‚ Group C, D   â”‚ â†’ Partial Aggregate            â”‚\n",
        "â”‚ CPU Core 3  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
        "â”‚             â”‚ Group E, F   â”‚ â†’ Partial Aggregate            â”‚\n",
        "â”‚ CPU Core 4  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                â”‚\n",
        "â”‚             â”‚ Group G, H   â”‚ â†’ Partial Aggregate            â”‚\n",
        "â”‚                      â†“                                      â”‚\n",
        "â”‚             Final Merge â†’ Result                            â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def polars_groupby():\n",
        "    return df_pl.group_by(\"category\").agg(pl.col(\"amount\").mean())\n",
        "\n",
        "polars_groupby_time = %timeit -o polars_groupby()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "31.1 ms Â± 3.65 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
        "```\n",
        "\n",
        "### DuckDB: Columnar Processing\n",
        "\n",
        "DuckDB processes batches of 1024 rows sequentially on a single core, using vectorized execution to maximize throughput per CPU instruction.\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  Vectorized Aggregation (single core, 1024 rows per batch)  â”‚\n",
        "â”‚                                                             â”‚\n",
        "â”‚  CPU Core 1:                                                â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚\n",
        "â”‚  â”‚ Batch 1 â”‚â†’â”‚ Batch 2 â”‚â†’â”‚ Batch 3 â”‚â†’â”‚ Batch 4 â”‚â†’ ...       â”‚\n",
        "â”‚  â”‚  1024   â”‚ â”‚  1024   â”‚ â”‚  1024   â”‚ â”‚  1024   â”‚            â”‚\n",
        "â”‚  â”‚  rows   â”‚ â”‚  rows   â”‚ â”‚  rows   â”‚ â”‚  rows   â”‚            â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚\n",
        "â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚\n",
        "â”‚                        â†“                                    â”‚\n",
        "â”‚              Hash Table Aggregation                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def duckdb_groupby():\n",
        "    return duckdb.sql(\"\"\"\n",
        "        SELECT category, AVG(amount)\n",
        "        FROM df_pd\n",
        "        GROUP BY category\n",
        "    \"\"\").df()\n",
        "\n",
        "duckdb_groupby_time = %timeit -o duckdb_groupby()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "29 ms Â± 3.33 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Polars is {pandas_groupby_time.average / polars_groupby_time.average:.1f}Ã— faster than pandas\")\n",
        "print(f\"DuckDB is {pandas_groupby_time.average / duckdb_groupby_time.average:.1f}Ã— faster than pandas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "Polars is 8.7Ã— faster than pandas\n",
        "DuckDB is 9.4Ã— faster than pandas\n",
        "```\n",
        "\n",
        "DuckDB and Polars perform similarly (9.4Ã— vs 8.7Ã—), both leveraging parallel execution. DuckDB's slight edge comes from columnar storage, which groups all category values together for faster scanning.\n",
        "\n",
        "## Memory Efficiency\n",
        "\n",
        "### pandas: Full Memory Load\n",
        "\n",
        "pandas loads the entire dataset into RAM:\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  RAM                                                        â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
        "â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ â”‚\n",
        "â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ALL 10M ROWS â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ â”‚\n",
        "â”‚  â”‚â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ”‚ â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
        "â”‚  Usage: 707,495 KB (entire dataset in memory)               â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_pd_mem = pd.read_csv(\"sales_data.csv\")\n",
        "pandas_mem = df_pd_mem.memory_usage(deep=True).sum() / 1e3\n",
        "print(f\"pandas memory usage: {pandas_mem:,.0f} KB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "pandas memory usage: 707,495 KB\n",
        "```\n",
        "\n",
        "For larger-than-RAM datasets, pandas throws an out-of-memory error.\n",
        "\n",
        "### Polars: Streaming Mode\n",
        "\n",
        "Polars can process data in streaming mode, handling chunks without loading everything:\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  RAM                                                        â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
        "â”‚  â”‚â–ˆ                                                       â”‚ â”‚\n",
        "â”‚  â”‚                    (result only)                       â”‚ â”‚\n",
        "â”‚  â”‚                                                        â”‚ â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
        "â”‚  Usage: 0.06 KB (streams chunks, keeps only result)         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result_pl_stream = (\n",
        "    pl.scan_csv(\"sales_data.csv\")\n",
        "    .group_by(\"category\")\n",
        "    .agg(pl.col(\"amount\").mean())\n",
        "    .collect(streaming=True)\n",
        ")\n",
        "\n",
        "polars_mem = result_pl_stream.estimated_size() / 1e3\n",
        "print(f\"Polars result memory: {polars_mem:.2f} KB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "Polars result memory: 0.06 KB\n",
        "```\n",
        "\n",
        "For larger-than-RAM files, use `sink_parquet` instead of `collect()`. It writes results directly to disk as chunks are processed, never holding the full dataset in memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "(\n",
        "    pl.scan_csv(\"sales_data.csv\")\n",
        "    .filter(pl.col(\"amount\") > 500)\n",
        "    .sink_parquet(\"filtered_sales.parquet\")\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DuckDB: Automatic Spill-to-Disk\n",
        "\n",
        "DuckDB automatically writes intermediate results to temporary files when data exceeds available RAM:\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  RAM                              Disk (if needed)          â”‚\n",
        "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "â”‚  â”‚â–ˆ                         â”‚     â”‚â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â”‚  â”‚\n",
        "â”‚  â”‚     (up to 500MB)        â”‚ â”€â”€â†’ â”‚    (overflow here)   â”‚  â”‚\n",
        "â”‚  â”‚                          â”‚     â”‚                      â”‚  â”‚\n",
        "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
        "â”‚  Usage: 0.42 KB (spills to disk when RAM full)              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configure memory limit and temp directory\n",
        "duckdb.sql(\"SET memory_limit = '500MB'\")\n",
        "duckdb.sql(\"SET temp_directory = '/tmp/duckdb_temp'\")\n",
        "\n",
        "# DuckDB handles larger-than-RAM automatically\n",
        "result_duckdb_mem = duckdb.sql(\"\"\"\n",
        "    SELECT category, AVG(amount) as avg_amount\n",
        "    FROM 'sales_data.csv'\n",
        "    GROUP BY category\n",
        "\"\"\").df()\n",
        "\n",
        "duckdb_mem = result_duckdb_mem.memory_usage(deep=True).sum() / 1e3\n",
        "print(f\"DuckDB result memory: {duckdb_mem:.2f} KB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "DuckDB result memory: 0.42 KB\n",
        "```\n",
        "\n",
        "DuckDB's out-of-core processing makes it ideal for embedded analytics where memory is limited."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"pandas: {pandas_mem:,.0f} KB (full dataset)\")\n",
        "print(f\"Polars: {polars_mem:.2f} KB (result only)\")\n",
        "print(f\"DuckDB: {duckdb_mem:.2f} KB (result only)\")\n",
        "print(f\"\\nPolars uses {pandas_mem / polars_mem:,.0f}Ã— less memory than pandas\")\n",
        "print(f\"DuckDB uses {pandas_mem / duckdb_mem:,.0f}Ã— less memory than pandas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "pandas: 707,495 KB (full dataset)\n",
        "Polars: 0.06 KB (result only)\n",
        "DuckDB: 0.42 KB (result only)\n",
        "\n",
        "Polars uses 11,791,583Ã— less memory than pandas\n",
        "DuckDB uses 1,684,512Ã— less memory than pandas\n",
        "```\n",
        "\n",
        "The million-fold reduction comes from streaming: Polars and DuckDB process data in chunks and only keep the 4-row result in memory, while pandas must hold all 10 million rows to compute the same aggregation.\n",
        "\n",
        "## Join Operations\n",
        "\n",
        "Joining tables is one of the most common operations in data analysis. Let's compare how each tool handles a left join between 1 million orders and 100K customers.\n",
        "\n",
        "Let's create two tables for join benchmarking:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create orders table (1M rows)\n",
        "orders_pd = pd.DataFrame({\n",
        "    \"order_id\": range(1_000_000),\n",
        "    \"customer_id\": np.random.randint(1, 100_000, size=1_000_000),\n",
        "    \"amount\": np.random.rand(1_000_000) * 500\n",
        "})\n",
        "\n",
        "# Create customers table (100K rows)\n",
        "customers_pd = pd.DataFrame({\n",
        "    \"customer_id\": range(100_000),\n",
        "    \"region\": np.random.choice([\"North\", \"South\", \"East\", \"West\"], size=100_000)\n",
        "})\n",
        "\n",
        "# Convert to Polars\n",
        "orders_pl = pl.from_pandas(orders_pd)\n",
        "customers_pl = pl.from_pandas(customers_pd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### pandas: Single-Threaded\n",
        "\n",
        "pandas processes the join on a single CPU core.\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ CPU Core 1                                  â”‚\n",
        "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
        "â”‚ â”‚ Row 1 â†’ Row 2 â†’ Row 3 â†’ ... â†’ Row 1M    â”‚ â”‚\n",
        "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
        "â”‚ CPU Core 2  [idle]                          â”‚\n",
        "â”‚ CPU Core 3  [idle]                          â”‚\n",
        "â”‚ CPU Core 4  [idle]                          â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def pandas_join():\n",
        "    return orders_pd.merge(customers_pd, on=\"customer_id\", how=\"left\")\n",
        "\n",
        "pandas_join_time = %timeit -o pandas_join()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "60.4 ms Â± 6.98 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
        "```\n",
        "\n",
        "### Polars: Multi-Threaded\n",
        "\n",
        "Polars distributes the join across all available CPU cores.\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ CPU Core 1  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
        "â”‚             â”‚ Rows 1-250K    â”‚              â”‚\n",
        "â”‚ CPU Core 2  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
        "â”‚             â”‚ Rows 250K-500K â”‚              â”‚\n",
        "â”‚ CPU Core 3  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
        "â”‚             â”‚ Rows 500K-750K â”‚              â”‚\n",
        "â”‚ CPU Core 4  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚\n",
        "â”‚             â”‚ Rows 750K-1M   â”‚              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def polars_join():\n",
        "    return orders_pl.join(customers_pl, on=\"customer_id\", how=\"left\")\n",
        "\n",
        "polars_join_time = %timeit -o polars_join()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "11.8 ms Â± 6.42 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
        "```\n",
        "\n",
        "### DuckDB: Vectorized Execution\n",
        "\n",
        "DuckDB processes rows in batches rather than one at a time.\n",
        "\n",
        "```text\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ CPU Core 1                                  â”‚\n",
        "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚\n",
        "â”‚ â”‚ Batch 1 â”‚ â”‚ Batch 2 â”‚ â”‚ Batch 3 â”‚  ...    â”‚\n",
        "â”‚ â”‚  1024   â”‚ â”‚  1024   â”‚ â”‚  1024   â”‚         â”‚\n",
        "â”‚ â”‚  rows   â”‚ â”‚  rows   â”‚ â”‚  rows   â”‚         â”‚\n",
        "â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def duckdb_join():\n",
        "    return duckdb.sql(\"\"\"\n",
        "        SELECT o.*, c.region\n",
        "        FROM orders_pd o\n",
        "        LEFT JOIN customers_pd c ON o.customer_id = c.customer_id\n",
        "    \"\"\").df()\n",
        "\n",
        "duckdb_join_time = %timeit -o duckdb_join()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "55.7 ms Â± 1.14 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n",
        "```\n",
        "\n",
        "Let's compare the performance of the joins:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"Polars is {pandas_join_time.average / polars_join_time.average:.1f}Ã— faster than pandas\")\n",
        "print(f\"DuckDB is {pandas_join_time.average / duckdb_join_time.average:.1f}Ã— faster than pandas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "Polars is 5.1Ã— faster than pandas\n",
        "DuckDB is 1.1Ã— faster than pandas\n",
        "```\n",
        "\n",
        "Polars' multi-threaded join delivers a 5.1Ã— speedup, significantly outperforming DuckDB's 1.1Ã— improvement. Parallelization matters more than vectorization for this join size.\n",
        "\n",
        "## Interoperability\n",
        "\n",
        "All three tools work together seamlessly. Use each tool for what it does best in a single pipeline.\n",
        "\n",
        "### pandas DataFrame to DuckDB\n",
        "\n",
        "Query pandas DataFrames directly with SQL:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.DataFrame({\n",
        "    \"product\": [\"A\", \"B\", \"C\"],\n",
        "    \"sales\": [100, 200, 150]\n",
        "})\n",
        "\n",
        "# DuckDB queries pandas DataFrames by variable name\n",
        "result = duckdb.sql(\"SELECT * FROM df WHERE sales > 120\").df()\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "  product  sales\n",
        "0       B    200\n",
        "1       C    150\n",
        "```\n",
        "\n",
        "### Polars to pandas\n",
        "\n",
        "Convert Polars DataFrames when ML libraries require pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_polars = pl.DataFrame({\n",
        "    \"feature1\": [1, 2, 3],\n",
        "    \"feature2\": [4, 5, 6],\n",
        "    \"target\": [0, 1, 0]\n",
        "})\n",
        "\n",
        "# Convert to pandas for scikit-learn\n",
        "df_pandas = df_polars.to_pandas()\n",
        "print(type(df_pandas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "<class 'pandas.core.frame.DataFrame'>\n",
        "```\n",
        "\n",
        "### DuckDB to Polars\n",
        "\n",
        "Get query results as Polars DataFrames:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = duckdb.sql(\"\"\"\n",
        "    SELECT category, SUM(amount) as total\n",
        "    FROM 'sales_data.csv'\n",
        "    GROUP BY category\n",
        "\"\"\").pl()\n",
        "\n",
        "print(type(result))\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "<class 'polars.dataframe.frame.DataFrame'>\n",
        "shape: (4, 2)\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ category    â”† total    â”‚\n",
        "â”‚ ---         â”† ---      â”‚\n",
        "â”‚ str         â”† f64      â”‚\n",
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\n",
        "â”‚ Electronics â”† 6.2445e8 â”‚\n",
        "â”‚ Food        â”† 6.2540e8 â”‚\n",
        "â”‚ Clothing    â”† 6.2539e8 â”‚\n",
        "â”‚ Books       â”† 6.2475e8 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### Combined Pipeline Example\n",
        "\n",
        "Each tool has a distinct strength: DuckDB optimizes SQL queries, Polars parallelizes transformations, and pandas integrates with ML libraries. Combine them in a single pipeline to leverage all three:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: DuckDB for initial SQL query\n",
        "aggregated = duckdb.sql(\"\"\"\n",
        "    SELECT category, region,\n",
        "           SUM(amount) as total_amount,\n",
        "           COUNT(*) as order_count\n",
        "    FROM 'sales_data.csv'\n",
        "    GROUP BY category, region\n",
        "\"\"\").pl()\n",
        "\n",
        "# Step 2: Polars for additional transformations\n",
        "enriched = (\n",
        "    aggregated\n",
        "    .with_columns([\n",
        "        (pl.col(\"total_amount\") / pl.col(\"order_count\")).alias(\"avg_order_value\"),\n",
        "        pl.col(\"category\").str.to_uppercase().alias(\"category_upper\")\n",
        "    ])\n",
        "    .filter(pl.col(\"order_count\") > 100000)\n",
        ")\n",
        "\n",
        "# Step 3: Convert to pandas for visualization or ML\n",
        "final_df = enriched.to_pandas()\n",
        "print(final_df.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "category region  total_amount  order_count  avg_order_value category_upper\n",
        "0      Food   East  1.563586e+08       312918       499.679004           FOOD\n",
        "1      Food  North  1.563859e+08       312637       500.215456           FOOD\n",
        "2  Clothing  North  1.560532e+08       311891       500.345286       CLOTHING\n",
        "3  Clothing   East  1.565054e+08       312832       500.285907       CLOTHING\n",
        "4      Food   West  1.560994e+08       312662       499.259318           FOOD\n",
        "```\n",
        "\n",
        "> ğŸ“– **Related**: For writing functions that work across pandas, Polars, and PySpark without conversion, see [Unified DataFrame Functions](https://codecut.ai/unified-dataframe-functions-pandas-polars-pyspark/).\n",
        "\n",
        "## Decision Matrix\n",
        "\n",
        "No single tool wins in every scenario. Use these tables to choose the right tool for your workflow.\n",
        "\n",
        "### Performance Summary\n",
        "\n",
        "Benchmark results from 10 million rows on a single machine:\n",
        "\n",
        "| Operation | pandas | Polars | DuckDB |\n",
        "|-----------|--------|--------|--------|\n",
        "| CSV Read (10M rows) | 1.05s | 137ms | 762ms |\n",
        "| GroupBy | 271ms | 31ms | 29ms |\n",
        "| Join (1M rows) | 60ms | 12ms | 56ms |\n",
        "| Memory Usage | 707 MB | 0.06 KB (streaming) | 0.42 KB (spill-to-disk) |\n",
        "\n",
        "Polars leads in CSV reading (7.7Ã— faster than pandas) and joins (5Ã— faster). DuckDB matches Polars in groupby performance and uses the least memory with automatic spill-to-disk.\n",
        "\n",
        "### Feature Comparison\n",
        "\n",
        "Each tool makes different trade-offs between speed, memory, and ecosystem integration:\n",
        "\n",
        "| Feature | pandas | Polars | DuckDB |\n",
        "|---------|--------|--------|--------|\n",
        "| Multi-threading | No | Yes | Yes |\n",
        "| Lazy evaluation | No | Yes | N/A (SQL) |\n",
        "| Query optimization | No | Yes | Yes |\n",
        "| Larger-than-RAM | No | Streaming | Spill-to-disk |\n",
        "| SQL interface | No | Limited | Native |\n",
        "| ML integration | Excellent | Good | Limited |\n",
        "\n",
        "pandas lacks the performance features that make Polars and DuckDB fast, but remains essential for ML workflows. Choose between Polars and DuckDB based on whether you prefer DataFrame chaining or SQL syntax.\n",
        "\n",
        "### Recommendations\n",
        "\n",
        "Match your use case to the right tool:\n",
        "\n",
        "| Scenario | Recommendation |\n",
        "|----------|----------------|\n",
        "| Small data (<1M rows) | pandas |\n",
        "| Large data (1M-100M rows) | Polars or DuckDB |\n",
        "| SQL-preferred workflow | DuckDB |\n",
        "| DataFrame-preferred workflow | Polars |\n",
        "| Memory-constrained | Polars (streaming) or DuckDB (spill-to-disk) |\n",
        "| ML pipeline integration | pandas (with Polars for preprocessing) |\n",
        "| Production data pipelines | Polars |\n",
        "\n",
        "Data size is the primary decision factor. Under 1M rows, pandas simplicity wins. Above that, the 5-10Ã— speedup from Polars or DuckDB justifies the switch."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}