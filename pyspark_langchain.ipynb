{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Have you ever wanted to ask a question about your PySpark data in plain English instead of writing SQL?\n",
        "\n",
        "[LangChain's Spark SQL Toolkit](https://python.langchain.com/docs/integrations/tools/spark_sql/) enables natural language data querying by:\n",
        "\n",
        "- Translating your requests into SQL\n",
        "- Executing them against your Spark cluster\n",
        "- Returning the results in a readable format\n",
        "\n",
        "This makes it much easier to work with large-scale data while still leveraging Spark's powerful distributed computing capabilities.\n",
        "\n",
        "\n",
        "\n",
        "To demonstrate, we'll create a simple DataFrame and use LangChain's Spark SQL tool to query it.\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession, Row\n",
        "\n",
        "# Create sample data and DataFrame\n",
        "data = [Row(name=\"Alice\", age=30), Row(name=\"Bob\", age=25)]\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "df = spark.createDataFrame(data)\n",
        "df.write.saveAsTable(\"people\")\n",
        "df.show()\n",
        "```\n",
        "\n",
        "This creates a table `people` accessible via SQL.\n",
        "\n",
        "Next, we'll set up the key components that enable natural language querying of our Spark data. Here are the steps:\n",
        "\n",
        "1. Initialize the Spark SQL tool which provides the interface to our Spark database.\n",
        "2. Initialize a language model.\n",
        "3. Initialize the Spark SQL toolkit, which connects the language model with the Spark database.\n",
        "4. Create an agent executor that combines a language model with the Spark SQL toolkit.\n",
        "\n",
        "```python\n",
        "# Initialize Spark SQL tool\n",
        "spark_sql = SparkSQL(schema=\"default\")\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Initialize toolkit\n",
        "toolkit = SparkSQLToolkit(db=spark_sql, llm=llm)\n",
        "\n",
        "# Create agent executor\n",
        "agent_executor = create_spark_sql_agent(llm=llm, toolkit=toolkit, verbose=True)\n",
        "```\n",
        "\n",
        "For a hands-on guide on how to build coordinated AI agents with LangGraph, check out [Building Coordinated AI Agents with LangGraph: A Hands-On Tutorial](https://codecut.ai/building-multi-agent-ai-langgraph-tutorial/).\n",
        "\n",
        "Now we can ask the agent to query the data.\n",
        "\n",
        "```python\n",
        "agent_executor.run(\"What is the average age of people in the table?\")\n",
        "```\n",
        "\n",
        "```\n",
        "> Entering new AgentExecutor chain...\n",
        "Action: list_tables_sql_db\n",
        "Action Input:\n",
        "Observation: people\n",
        "Thought:I can query the \"people\" table for the average age.\n",
        "Action: query_sql_db\n",
        "Action Input: SELECT AVG(age) FROM people\n",
        "Observation: [('27.5',)]\n",
        "Thought:The average age of people in the table is 27.5.\n",
        "Final Answer: 27.5\n",
        "\n",
        "> Finished chain.\n",
        "```\n",
        "\n",
        "The answer for the average age is correct.\n",
        "\n",
        "The output shows that the agent:\n",
        "\n",
        "- Looked up the available tables\n",
        "- Queried the `people` table for the average age\n",
        "- Got the result\n",
        "- Answered the question with the result\n",
        "\n",
        "Let's try another question.\n",
        "\n",
        "```python\n",
        "agent_executor.run(\"Who is the oldest person in the table?\")\n",
        "```\n",
        "\n",
        "```\n",
        "> Entering new AgentExecutor chain...\n",
        "Action: list_tables_sql_db\n",
        "Action Input:\n",
        "Observation: people\n",
        "Thought:I should query the \"people\" table to find the oldest person.\n",
        "Action: schema_sql_db\n",
        "Action Input: people\n",
        "Observation: CREATE TABLE spark_catalog.default.people (\n",
        "  name STRING,\n",
        "  age BIGINT)\n",
        ";\n",
        "\n",
        "/*\n",
        "3 rows from people table:\n",
        "name    age\n",
        "Alice   30\n",
        "Bob     25\n",
        "*/\n",
        "Thought:I should write a query to select the oldest person from the \"people\" table.\n",
        "Action: query_sql_db\n",
        "Action Input: SELECT name, age FROM people ORDER BY age DESC LIMIT 1\n",
        "Observation: [('Alice', '30')]\n",
        "Thought:I now know the final answer\n",
        "Final Answer: Alice\n",
        "\n",
        "> Finished chain.\n",
        "```\n",
        "\n",
        "The answer for the oldest person is also correct."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}