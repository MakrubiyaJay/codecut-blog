{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Coiled: Scale Python Data Pipeline to the Cloud in Minutes\n",
        "\n",
        "## What is Coiled?\n",
        "\n",
        "[Coiled](https://coiled.io) is a lightweight cloud platform that runs Python code on powerful cloud infrastructure without requiring Docker or Kubernetes knowledge. It supports four main capabilities:\n",
        "\n",
        "1. **Batch Jobs**: Submit and run Python scripts asynchronously on cloud infrastructure\n",
        "2. **Serverless Functions**: Execute Python functions (Pandas, Polars, PyTorch) on cloud VMs with decorators\n",
        "3. **Dask Clusters**: Provision multi-worker clusters for distributed computing\n",
        "4. **Jupyter Notebooks**: Launch interactive Jupyter servers directly on cluster schedulers\n",
        "\n",
        "Key features across both:\n",
        "\n",
        "- **Framework-Agnostic**: Works with Pandas, Polars, Dask, or any Python library\n",
        "- **Automatic Package Sync**: Local packages replicate to cloud workers without Docker\n",
        "- **Cost Optimization**: Spot instances, adaptive scaling, and auto-shutdown reduce spending\n",
        "- **Simple APIs**: Decorate functions or create clusters with 2-3 lines of code\n",
        "\n",
        "To install Coiled and Dask, run:\n",
        "\n",
        "```bash\n",
        "pip install coiled dask[complete]\n",
        "```\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, create a free Coiled account by running this command in your terminal:\n",
        "\n",
        "```bash\n",
        "coiled login\n",
        "```\n",
        "\n",
        "This creates a free Coiled Hosted account with 200 CPU-hours per month. Your code runs on Coiled's cloud infrastructure—no AWS/GCP/Azure account needed.\n",
        "\n",
        "> **Note**: Coiled Hosted is perfect for learning and prototyping. For production workloads with your own data, you can later run `coiled setup PROVIDER` to connect your cloud provider. See [Coiled Setup Guide](https://docs.coiled.io/user_guide/setup/) for details.\n",
        "\n",
        "For this tutorial, we'll analyze the NYC Taxi dataset, a public dataset containing taxi trip records. Each monthly file contains millions of trip records and expands significantly in memory when loaded with pandas.\n",
        "\n",
        "## Serverless Functions: Process Data with Any Framework\n",
        "\n",
        "The simplest way to scale Python code to the cloud is with serverless functions. Decorate any function with `@coiled.function`, and Coiled handles provisioning cloud VMs, installing packages, and executing your code.\n",
        "\n",
        "### Scale Beyond Laptop Memory with Cloud VMs\n",
        "\n",
        "Imagine you need to process a 12GB compressed dataset (50GB+ expanded) on a laptop with only 16GB of RAM. Your machine simply doesn't have enough memory to handle this workload.\n",
        "\n",
        "With Coiled, you use the same code you would use on your laptop, but it runs on a cloud VM with 64GB of RAM by simply decorating the function with `@coiled.function`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import coiled\n",
        "import pandas as pd\n",
        "\n",
        "@coiled.function(\n",
        "    memory=\"64 GiB\",\n",
        "    region=\"us-east-1\"\n",
        ")\n",
        "def process_month_with_pandas(month):\n",
        "    # Read 12GB file directly into pandas\n",
        "    df = pd.read_parquet(\n",
        "        f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-{month:02d}.parquet\"\n",
        "    )\n",
        "\n",
        "    # Compute tipping patterns by hour\n",
        "    df[\"hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
        "    result = df.groupby(\"hour\")[\"tip_amount\"].mean()\n",
        "\n",
        "    return result\n",
        "\n",
        "# Run on cloud VM with 64GB RAM\n",
        "january_tips = process_month_with_pandas(1)\n",
        "print(january_tips)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "hour\n",
        "0     3.326543\n",
        "1     2.933899\n",
        "2     2.768246\n",
        "3     2.816333\n",
        "4     3.132973\n",
        "...\n",
        "Name: tip_amount, dtype: float64\n",
        "```\n",
        "\n",
        "This function runs on a cloud VM with 64GB RAM, processes the entire month in memory, and returns just the aggregated result to your laptop.\n",
        "\n",
        "You can view the function's execution progress and resource usage in the Coiled dashboard at https://cloud.coiled.io.\n",
        "\n",
        "![Image showing the Coiled dashboard](https://codecut.ai/wp-content/uploads/2025/11/coiled_function.png)\n",
        "\n",
        "### Parallel Processing with .map()\n",
        "\n",
        "By default Coiled Functions will run sequentially, just like normal Python functions. However, they can also easily run in parallel by using the `.map()` method.\n",
        "\n",
        "Process all 12 months in parallel using `.map()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import coiled\n",
        "import pandas as pd\n",
        "\n",
        "@coiled.function(memory=\"64 GiB\", region=\"us-east-1\")\n",
        "def process_month(month):\n",
        "    df = pd.read_parquet(\n",
        "        f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-{month:02d}.parquet\"\n",
        "    )\n",
        "    return df[\"tip_amount\"].mean()\n",
        "\n",
        "# Process 12 months in parallel on 12 cloud VMs\n",
        "months = range(1, 13)\n",
        "monthly_tips = list(process_month.map(months))\n",
        "\n",
        "print(\"Average tips by month:\", monthly_tips)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Average tips by month: [2.65, 2.58, 2.72, 2.68, 2.75, 2.81, 2.79, 2.73, 2.69, 2.71, 2.66, 2.62]\n",
        "```\n",
        "\n",
        "When you call `.map()` with 12 months, Coiled spins up 12 cloud VMs simultaneously, runs `process_month()` on each VM with a different month, then returns all results.\n",
        "\n",
        "The execution flow:\n",
        "\n",
        "```\n",
        "VM 1: yellow_tripdata_2024-01.parquet → compute mean → 2.65\n",
        "VM 2: yellow_tripdata_2024-02.parquet → compute mean → 2.58\n",
        "VM 3: yellow_tripdata_2024-03.parquet → compute mean → 2.72\n",
        "...   (all running in parallel)\n",
        "VM 12: yellow_tripdata_2024-12.parquet → compute mean → 2.62\n",
        "                                            ↓\n",
        "                        Coiled collects: [2.65, 2.58, 2.72, ..., 2.62]\n",
        "```\n",
        "\n",
        "Each VM works in complete isolation with no data sharing or coordination between them.\n",
        "\n",
        "![Image showing the Coiled dashboard](https://codecut.ai/wp-content/uploads/2025/11/coiled_parallel.png)\n",
        "\n",
        "The dashboard confirms 12 tasks were executed, matching the 12 months we passed to `.map()`.\n",
        "\n",
        "### Framework-Agnostic: Use Any Python Library\n",
        "\n",
        "Coiled Functions aren't limited to pandas. You can use any Python library (Polars, DuckDB, PyTorch, scikit-learn) without any additional configuration. The automatic package synchronization works for all dependencies.\n",
        "\n",
        "**Example with Polars:**\n",
        "\n",
        "Polars is a fast DataFrame library optimized for performance. It works seamlessly with Coiled:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import coiled\n",
        "import polars as pl\n",
        "\n",
        "@coiled.function(memory=\"64 GiB\", region=\"us-east-1\")\n",
        "def process_with_polars(month):\n",
        "    df = pl.read_parquet(\n",
        "        f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-{month:02d}.parquet\"\n",
        "    )\n",
        "    return (\n",
        "        df\n",
        "        .filter(pl.col(\"tip_amount\") > 0)\n",
        "        .group_by(\"PULocationID\")\n",
        "        .agg(pl.col(\"tip_amount\").mean())\n",
        "        .sort(\"tip_amount\", descending=True)\n",
        "        .head(5)\n",
        "    )\n",
        "\n",
        "result = process_with_polars(1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "shape: (5, 2)\n",
        "┌──────────────┬────────────┐\n",
        "│ PULocationID ┆ tip_amount │\n",
        "│ ---          ┆ ---        │\n",
        "│ i64          ┆ f64        │\n",
        "╞══════════════╪════════════╡\n",
        "│ 138          ┆ 4.52       │\n",
        "│ 230          ┆ 4.23       │\n",
        "│ 161          ┆ 4.15       │\n",
        "│ 234          ┆ 3.98       │\n",
        "│ 162          ┆ 3.87       │\n",
        "└──────────────┴────────────┘\n",
        "```\n",
        "\n",
        "**Example with DuckDB:**\n",
        "\n",
        "DuckDB provides fast SQL analytics directly on Parquet files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import coiled\n",
        "import duckdb\n",
        "\n",
        "@coiled.function(memory=\"64 GiB\", region=\"us-east-1\")\n",
        "def query_with_duckdb(month):\n",
        "    con = duckdb.connect()\n",
        "    result = con.execute(f\"\"\"\n",
        "        SELECT\n",
        "            DATE_TRUNC('hour', tpep_pickup_datetime) as pickup_hour,\n",
        "            AVG(tip_amount) as avg_tip,\n",
        "            COUNT(*) as trip_count\n",
        "        FROM 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-{month:02d}.parquet'\n",
        "        WHERE tip_amount > 0\n",
        "        GROUP BY pickup_hour\n",
        "        ORDER BY avg_tip DESC\n",
        "        LIMIT 5\n",
        "    \"\"\").fetchdf()\n",
        "    return result\n",
        "\n",
        "result = query_with_duckdb(1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "         pickup_hour  avg_tip  trip_count\n",
        "0 2024-01-15 14:00:00     4.23       15234\n",
        "1 2024-01-20 18:00:00     4.15       18456\n",
        "2 2024-01-08 12:00:00     3.98       12789\n",
        "3 2024-01-25 16:00:00     3.87       14567\n",
        "4 2024-01-12 20:00:00     3.76       16234\n",
        "```\n",
        "\n",
        "Coiled automatically detects your local Polars and DuckDB installations and replicates them to cloud VMs. No manual configuration needed.\n",
        "\n",
        "## When You Need More: Distributed Clusters with Dask\n",
        "\n",
        "Serverless functions work great for independent file processing. However, when you need to combine and aggregate data across all your files into a single result, you need a Dask cluster.\n",
        "\n",
        "For example, suppose you want to calculate total revenue by pickup location across all 12 months of data. With Coiled Functions, each VM processes one month independently:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@coiled.function(memory=\"64 GiB\", region=\"us-east-1\")\n",
        "def get_monthly_revenue_by_location(month):\n",
        "    df = pd.read_parquet(\n",
        "        f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-{month:02d}.parquet\"\n",
        "    )\n",
        "    return df.groupby(\"PULocationID\")[\"total_amount\"].sum()\n",
        "\n",
        "# This returns 12 separate DataFrames, one per month\n",
        "results = list(get_monthly_revenue_by_location.map(range(1, 13)))\n",
        "print(f'Number of DataFrames: {len(results)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Number of DataFrames: 12\n",
        "```\n",
        "\n",
        "The problem is that you get 12 separate DataFrames that you need to manually combine.\n",
        "\n",
        "Here's what happens: VM 1 processes January and returns a DataFrame like:\n",
        "```\n",
        "PULocationID    total_amount\n",
        "138             15000\n",
        "230             22000\n",
        "```\n",
        "\n",
        "VM 2 processes February and returns:\n",
        "```\n",
        "PULocationID    total_amount\n",
        "138             18000\n",
        "230             19000\n",
        "```\n",
        "\n",
        "Each VM works independently and has no knowledge of the other months' data. To get yearly totals per location, you'd need to write code to merge these 12 DataFrames and sum the revenue for each location.\n",
        "\n",
        "With a Dask cluster, workers coordinate to give you one global result:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import coiled\n",
        "import dask.dataframe as dd\n",
        "\n",
        "# For production workloads, you can scale to 50+ workers\n",
        "cluster = coiled.Cluster(n_workers=3, region=\"us-east-1\")\n",
        "\n",
        "# Read all 12 months of 2024 data\n",
        "files = [\n",
        "    f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-{month:02d}.parquet\"\n",
        "    for month in range(1, 13)\n",
        "]\n",
        "df = dd.read_parquet(files)  # Lazy: builds a plan, doesn't load data yet\n",
        "\n",
        "# This returns ONE DataFrame with total revenue per location across all months\n",
        "total_revenue = (\n",
        "    df.groupby(\"PULocationID\")[\"total_amount\"].sum().compute()\n",
        ")  # Executes the plan\n",
        "\n",
        "total_revenue.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "PULocationID\n",
        "1     563645.70\n",
        "2       3585.80\n",
        "3      67261.41\n",
        "4    1687265.08\n",
        "5        602.98\n",
        "Name: total_amount, dtype: float64\n",
        "```\n",
        "\n",
        "You can see that we got a single DataFrame with the total revenue per location across all months.\n",
        "\n",
        "Here is what happens under the hood:\n",
        "\n",
        "When you call `.compute()`, Dask executes the plan in four steps:\n",
        "\n",
        "```\n",
        "Step 1: Data Distribution\n",
        "├─ Worker 1: [Jan partitions 1-3, Apr partitions 1-2, Jul partitions 1-3]\n",
        "├─ Worker 2: [Feb partitions 1-4, May partitions 1-3, Aug partitions 1-2]\n",
        "└─ Worker 3: [Mar partitions 1-3, Jun partitions 1-2, Sep-Dec partitions]\n",
        "\n",
        "Step 2: Local Aggregation (each worker groups its data)\n",
        "├─ Worker 1: {location_138: $45,000, location_230: $63,000}\n",
        "├─ Worker 2: {location_138: $38,000, location_230: $55,000}\n",
        "└─ Worker 3: {location_138: $50,000, location_230: $62,000}\n",
        "\n",
        "Step 3: Shuffle (redistribute so each location lives on one worker)\n",
        "├─ Worker 1: All location_230 data → $63,000 + $55,000 + $62,000\n",
        "├─ Worker 2: All location_138 data → $45,000 + $38,000 + $50,000\n",
        "└─ Worker 3: All other locations...\n",
        "\n",
        "Step 4: Final Result\n",
        "location_138: $133,000 (yearly total)\n",
        "location_230: $180,000 (yearly total)\n",
        "```\n",
        "\n",
        "This shuffle-and-combine process is what makes Dask different from Coiled Functions. Workers actively coordinate and share data to produce one unified result.\n",
        "\n",
        "## Cost Optimization\n",
        "\n",
        "Cloud costs can spiral quickly. Coiled provides three mechanisms to reduce spending:\n",
        "\n",
        "### 1. Spot Instances\n",
        "\n",
        "You can reduce cloud costs by 60-90% using spot instances. These are discounted servers that cloud providers can reclaim when demand increases. When an interruption occurs, Coiled:\n",
        "\n",
        "- Gracefully shuts down the affected worker\n",
        "- Redistributes its work to healthy workers\n",
        "- Automatically launches a replacement worker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cluster = coiled.Cluster(\n",
        "    n_workers=50,\n",
        "    spot_policy=\"spot_with_fallback\",  # Use spot instances with on-demand backup\n",
        "    region=\"us-east-1\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cost comparison for m5.xlarge instances:\n",
        "\n",
        "- On-demand: $0.192/hour\n",
        "- Spot: $0.05/hour\n",
        "- **Savings: 74%**\n",
        "\n",
        "For a 100-worker cluster:\n",
        "\n",
        "- On-demand: $19.20/hour = $460/day\n",
        "- Spot: $5.00/hour = $120/day\n",
        "\n",
        "### 2. Adaptive Scaling\n",
        "\n",
        "Adaptive scaling automatically adds workers when you have more work and removes them when idle, so you only pay for what you need. Coiled enables this with the `adapt()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cluster = coiled.Cluster(region=\"us-east-1\")\n",
        "cluster.adapt(minimum=10, maximum=50)  # Scale between 10-50 workers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Serverless functions also support auto-scaling by specifying a worker range:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@coiled.function(n_workers=[10, 300])\n",
        "def process_data(files):\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This saves money during light workloads while delivering performance during heavy computation. No manual monitoring required.\n",
        "\n",
        "### 3. Automatic Shutdown\n",
        "\n",
        "To prevent paying for unused resources, Coiled automatically shuts down clusters after 20 minutes of inactivity by default. You can customize this with the `idle_timeout` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cluster = coiled.Cluster(\n",
        "    n_workers=20,\n",
        "    region=\"us-east-1\",\n",
        "    idle_timeout=\"1 hour\"  # Keep cluster alive for longer workloads\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This prevents the common mistake of leaving clusters running overnight.\n",
        "\n",
        "## Environment Synchronization\n",
        "\n",
        "### The \"Works on My Machine\" Problem When Scaling to Cloud\n",
        "\n",
        "Imagine this scenario: your pandas analysis works perfectly on your laptop with pandas 2.1.0, pyarrow 14.0.1, and numpy 1.26.0. You need to process 50GB of data, so you rent a cloud VM with 64GB RAM. When you try to run your code, the analysis fails because the cloud environment lacks the same packages, has different versions installed, or encounters compatibility errors.\n",
        "\n",
        "The traditional solution combines Docker with cloud deployment. Docker packages your environment into a container, but getting it running on cloud infrastructure involves a complex workflow:\n",
        "\n",
        "1. **Write a Dockerfile** listing all dependencies and versions\n",
        "2. **Build the Docker image** (wait 5-10 minutes)\n",
        "3. **Push to cloud container registry** (AWS ECR, Google Container Registry)\n",
        "4. **Configure cloud VMs** (EC2/GCE instances with proper networking and security)\n",
        "5. **Pull and run the image** on cloud machines (3-5 minutes per VM)\n",
        "6. **Rebuild and redeploy** every time you add a package (repeat steps 2-5)\n",
        "\n",
        "This Docker + cloud workflow slows down development and requires expertise in both containerization and cloud infrastructure management.\n",
        "\n",
        "### Coiled's Solution: Automatic Package Synchronization\n",
        "\n",
        "Coiled eliminates Docker entirely through automatic package synchronization. Your local environment replicates to cloud workers—no Dockerfile required.\n",
        "\n",
        "Instead of managing Docker images and cloud infrastructure, you simply add a decorator to your function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import coiled\n",
        "import pandas as pd\n",
        "\n",
        "@coiled.function(memory=\"64 GiB\", region=\"us-east-1\")\n",
        "def process_data():\n",
        "    df = pd.read_parquet(\"s3://my-bucket/data.parquet\")\n",
        "    # Your analysis code here\n",
        "    return df.describe()\n",
        "\n",
        "result = process_data()  # Runs on cloud VM with your exact package versions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What Coiled does automatically:**\n",
        "\n",
        "1. **Scans your local environment** (pip, conda packages with exact versions)\n",
        "2. **Creates a dependency manifest** (a list of all packages and their versions)\n",
        "3. **Installs packages on cloud workers** with matching versions\n",
        "4. **Reuses built environments** when your dependencies haven't changed\n",
        "\n",
        "This is **faster than Docker builds** in most cases thanks to intelligent caching, and requires zero configuration."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}