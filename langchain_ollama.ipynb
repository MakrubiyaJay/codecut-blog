{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why Local AI Matters {#why-local-ai-matters}\n",
        "\n",
        "AI models are changing data science projects by automating feature engineering, summarizing datasets, generating reports, and even writing code to examine or clean data.\n",
        "\n",
        "However, using popular APIs like OpenAI or Anthropic can introduce serious privacy risks, especially when handling regulated data such as medical records, legal documents, or internal company knowledge. These services transmit user inputs to remote servers, making it difficult to guarantee confidentiality or data residency compliance.\n",
        "\n",
        "When data privacy is important, running models locally ensures full control. Nothing leaves your machine, so you manage all inputs, outputs, and processing securely.\n",
        "\n",
        "That's where [LangChain](https://www.langchain.com/) and [Ollama](https://ollama.com/) come in. LangChain provides the framework to build AI applications. Ollama lets you run open-source models locally.\n",
        "\n",
        "## LangChain + Ollama: Integration Tutorial {#langchain-ollama-integration-tutorial}\n",
        "\n",
        "Now that we understand the core technology, let's see how to integrate LangChain with Ollama to run models locally.\n",
        "\n",
        "### Installation and Setup {#installation-and-setup}\n",
        "\n",
        "To run local AI models, let's install both Langchain and Ollama:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pip install langchain langchain-community langchain-ollama"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ollama needs to be installed separately since it's a standalone service that runs locally:\n",
        "\n",
        "- For macOS: Download from [ollama.com](https://ollama.com) - this installs both the CLI tool and service\n",
        "- For Linux: `curl -fsSL https://ollama.com/install.sh | sh` - this script sets up both the binary and system service\n",
        "- For Windows: Download Windows (Preview) from [ollama.com](https://ollama.com) - still in preview mode with some limitations\n",
        "\n",
        "Start the Ollama server:\n",
        "\n",
        "```bash\n",
        "ollama serve\n",
        "```\n",
        "\n",
        "The server will run in the background, handling model loading and inference requests.\n",
        "\n",
        "> ðŸ’» **Get the Code**: The complete source code and Jupyter notebook for this tutorial are available on [GitHub](https://github.com/CodeCutTech/Data-science/blob/master/llm/langchain_ollama.ipynb). Clone it to follow along!\n",
        "\n",
        "### Pulling Models with Ollama {#pulling-models-with-ollama}\n",
        "\n",
        "Before using any model with LangChain, you need to pull it to your local machine with Ollama:\n",
        "\n",
        "```bash\n",
        "ollama pull qwen3:0.6b\n",
        "```\n",
        "\n",
        "Once it is downloaded, you can serve the model with the following command:\n",
        "\n",
        "```bash\n",
        "ollama run qwen3:0.6b\n",
        "```\n",
        "\n",
        "The model size has a large impact on performance and resource requirements:\n",
        "\n",
        "- Smaller models (7B-8B) run well on most modern computers with 16GB+ RAM\n",
        "- Medium models (13B-34B) need more RAM or GPU acceleration\n",
        "- Large models (70B+) typically require a dedicated GPU with 24GB+ VRAM\n",
        "\n",
        "For a full list of models you can serve locally, check out [the Ollama model library](https://ollama.com/search). Before pulling a model and potentially waste your hardware resources, check out [the VRAM calculator](https://apxml.com/tools/vram-calculator) that tells you if you can run a specific model on your machine:\n",
        "\n",
        "![VRAM Calculator showing memory requirements for different LLM models across various quantization levels](https://codecut.ai/wp-content/uploads/2025/05/compute-1.png)\n",
        "\n",
        "### Basic Chat Integration {#basic-chat-integration}\n",
        "\n",
        "Once you have a model downloaded, you need to connect LangChain to Ollama for actual AI interactions. LangChain uses dedicated classes that handle the communication between your Python code and the Ollama service:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Initialize the chat model with specific configurations\n",
        "chat_model = ChatOllama(\n",
        "    model=\"qwen3:0.6b\",\n",
        "    temperature=0.5,\n",
        "    base_url=\"http://localhost:11434\",  # Can be changed for remote Ollama instances\n",
        ")\n",
        "\n",
        "# Create a conversation with system and user messages\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful coding assistant specialized in Python.\"),\n",
        "    HumanMessage(content=\"Write a recursive Fibonacci function with memoization.\")\n",
        "]\n",
        "\n",
        "# Invoke the model\n",
        "response = chat_model.invoke(messages)\n",
        "print(response.content[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "<think>\n",
        "Okay, I need to write a recursive Fibonacci function with memoization. Let me think about how to approach this.\n",
        "\n",
        "First, the recursive approach usually involves a function that calculates the ...\n",
        "```\n",
        "\n",
        "This snippet:\n",
        "\n",
        "* Imports `ChatOllama` from `langchain_ollama` to interface with a local or remote Ollama model\n",
        "* Imports `HumanMessage` and `SystemMessage` for structured message creation\n",
        "* Initializes the `ChatOllama` instance.\n",
        "* Constructs a conversation consisting of a`SystemMessage` and a `HumanMessage`.\n",
        "* Sends the message list to the model using `invoke()`\n",
        "\n",
        "Under the hood, `ChatOllama`:\n",
        "\n",
        "1. Converts LangChain message objects into Ollama API format\n",
        "2. Makes HTTP POST requests to the `/api/chat` endpoint\n",
        "3. Processes streaming responses when activated\n",
        "4. Parses the response back into LangChain message objects\n",
        "\n",
        "The `ChatOllama` class also supports asynchronous operations, allowing data scientists to run multiple model calls in parallelâ€”ideal for building responsive, non-blocking applications like dashboards or chat interfaces:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "async def generate_async():\n",
        "    response = await chat_model.ainvoke(messages)\n",
        "    return response.content\n",
        "\n",
        "# In async context\n",
        "result = await generate_async()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print( result[:200])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "<think>\n",
        "Okay, I need to write a recursive Fibonacci function with memoization. Let me think about how to approach this. \n",
        "\n",
        "First, the Fibonacci sequence is defined such that each number is the sum of ...\n",
        "```\n",
        "\n",
        "### Using Completion Models {#using-completion-models}\n",
        "\n",
        "Chat models are great for conversation, but data science tasks like code generation, doc completion, and creative writing often benefit from text completion instead. \n",
        "\n",
        "The OllamaLLM class supports this mode, letting the model continue from a given input:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLM with specific options\n",
        "llm = OllamaLLM(\n",
        "    model=\"qwen3:0.6b\",\n",
        ")\n",
        "\n",
        "# Generate text from a prompt\n",
        "text = \"\"\"\n",
        "Write a quick sort algorithm in Python with detailed comments:\n",
        "```{python}\n",
        "def quicksort(\n",
        "\"\"\"\n",
        "\n",
        "response = llm.invoke(text)\n",
        "print(response[:500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "<think>\n",
        "Okay, I need to write a quicksort algorithm in Python with detailed comments. Let me start by recalling how quicksort works. The basic idea is to choose a pivot element, partition the array into elements less than the pivot and greater than it, and then recursively sort each partition. The pivot can be chosen in different ways, like the first element, middle element, or random element.\n",
        "\n",
        "First, I should define the function signature. The parameters are the array, and maybe a left and ...\n",
        "```\n",
        "\n",
        "The difference between `ChatOllama` and `OllamaLLM` classes:\n",
        "\n",
        "- `OllamaLLM` uses the `/api/generate` endpoint for text completion\n",
        "- `ChatOllama` uses the `/api/chat` endpoint for chat-style interactions\n",
        "- Completion is better for code continuation, creative writing, and single-turn prompts\n",
        "- Chat is better for multi-turn conversations and when using system prompts\n",
        "\n",
        "For streaming responses (showing tokens as they're generated):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for chunk in llm.stream(\"Explain quantum computing in three sentences:\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "<think>\n",
        "Okay, the user wants me to explain quantum computing in three sentences. Let me start by recalling what I know. Quantum computing uses qubits instead of classical bits. So first sentence should mention qubits and the difference from classical bits. Maybe say \"Quantum computing uses qubits, which can exist in multiple states at once, unlike classical bits that are either 0 or 1.\"\n",
        "\n",
        "...\n",
        "```\n",
        "\n",
        "Use streaming responses to display output in real time, making interactive apps like chatbots feel faster and more responsive.\n",
        "\n",
        "### Customizing Model Parameters {#customizing-model-parameters}\n",
        "\n",
        "Both completion and chat models use default settings that work reasonably well, but data science tasks often need more tailored model behavior. For example:\n",
        "\n",
        "- Scientific analysis needs precise, factual responses\n",
        "- Creative tasks benefit from more randomness and variety \n",
        "\n",
        "Ollama offers fine-grained control over generation parameters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "llm = OllamaLLM(\n",
        "    model=\"qwen3:0.6b\", # Example model, can be any model supported by Ollama\n",
        "    temperature=0.7,      # Controls randomness (0.0 = deterministic, 1.0 = creative)\n",
        "    stop=[\"```\", \"###\"],  # Stop sequences to end generation\n",
        "    repeat_penalty=1.1,   # Penalizes repetition (>1.0 reduces repetition)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Details about these parameters:\n",
        "\n",
        "- `model`: Specifies the language model to use.\n",
        "- `temperature`: Controls randomness; lower = more focused, higher = more creative.\n",
        "- `stop`: Defines stop sequences that terminate generation early. Once one of these sequences is produced, the model stops generating further tokens.\n",
        "- `repeat_penalty`: Penalizes repeated tokens to reduce redundancy. Values greater than 1.0 discourage the model from repeating itself.\n",
        "\n",
        "Parameter recommendations:\n",
        "\n",
        "- For factual or technical responses: Lower `temperature` (0.1-0.3) and higher `repeat_penalty` (1.1-1.2)\n",
        "- For creative writing: Higher `temperature` (0.7-0.9)\n",
        "- For code generation: Medium `temperature` (0.3-0.6) with specific `stop` like \"\\`\\`\\`\"\n",
        "\n",
        "The model behavior changes dramatically with these settings. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scientific writing with precise output\n",
        "scientific_llm = OllamaLLM(model=\"qwen3:0.6b\", temperature=0.1, repeat_penalty=1.2)\n",
        "\n",
        "# Creative storytelling\n",
        "creative_llm = OllamaLLM(model=\"qwen3:0.6b\", temperature=0.9, repeat_penalty=1.0)\n",
        "\n",
        "# Code generation\n",
        "code_llm = OllamaLLM(model=\"codellama\", temperature=0.3, stop=[\"```\", \"def \"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating LangChain Chains {#creating-langchain-chains}\n",
        "\n",
        " AI  workflows often involve multiple steps: data validation, prompt formatting, model inference, and output processing. Running these steps manually for each request becomes repetitive and error-prone. \n",
        "\n",
        "LangChain addresses this by chaining steps into sequences to create end-to-end applications:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import json\n",
        "\n",
        "# Create a structured prompt template\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are an expert educator.\n",
        "Explain the following concept in simple terms that a beginner would understand.\n",
        "Make sure to provide:\n",
        "1. A clear definition\n",
        "2. A real-world analogy\n",
        "3. A practical example\n",
        "\n",
        "Concept: {concept}\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we import the required LangChain components and create a prompt template. The `PromptTemplate.from_template()` method creates a reusable template with placeholder variables (like `{concept}`) that get filled in at runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a parser that extracts structured data\n",
        "class JsonOutputParser:\n",
        "    def parse(self, text):\n",
        "        try:\n",
        "            # Find JSON blocks in the text\n",
        "            if \"```json\" in text and \"```\" in text.split(\"```json\")[1]:\n",
        "                json_str = text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
        "                return json.loads(json_str)\n",
        "            # Try to parse the whole text as JSON\n",
        "            return json.loads(text)\n",
        "        except:\n",
        "            # Fall back to returning the raw text\n",
        "            return {\"raw_output\": text}\n",
        "\n",
        "# Initialize a model instance to be used in the chain\n",
        "llm = OllamaLLM(model=\"qwen3:0.6b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define a custom output parser. This class attempts to extract JSON from the model's response, handling both code-block format and raw JSON. If parsing fails, it returns the original text wrapped in a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build a more complex chain\n",
        "chain = (\n",
        "    {\"concept\": RunnablePassthrough()} \n",
        "    | prompt \n",
        "    | llm \n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Execute the chain with detailed tracking\n",
        "result = chain.invoke(\"Recursive neural networks\")\n",
        "print(result[:500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we build the chain using LangChain's pipe operator (`|`). The `RunnablePassthrough()` passes input directly to the prompt template, which formats it and sends it to the LLM. The `StrOutputParser()` converts the response to a string. Here is the output:\n",
        "\n",
        "```text\n",
        "<think>\n",
        "Okay, so the user is asking for a simple explanation of recursive neural networks. Let me start by breaking down the concept. First, I need to define it clearly. Recursive neural networks... Hmm, I remember they're a type of neural network that can process data in multiple steps. Wait, maybe I should explain it as networks that can be broken down into smaller parts. Like, they can have multiple layers or multiple levels of processing. \n",
        "\n",
        "Now, the user wants a real-world analogy...\n",
        "```\n",
        "\n",
        "The chain architecture allows you to:\n",
        "\n",
        "1. Pre-process inputs before sending to the model\n",
        "2. Change model outputs into structured data\n",
        "3. Chain multiple models together\n",
        "4. Add memory and context management\n",
        "\n",
        "### Working with Embeddings {#working-with-embeddings}\n",
        "\n",
        "Embeddings change text into numerical vectors that capture semantic meaning, allowing computers to understand relationships between words and documents mathematically. Ollama supports specialized embedding models that excel at this conversion.\n",
        "\n",
        "First, let's set up the embedding model and understand what we're working with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_ollama import OllamaEmbeddings\n",
        "import numpy as np\n",
        "\n",
        "# Initialize embeddings model with specific parameters\n",
        "embeddings = OllamaEmbeddings(\n",
        "    model=\"nomic-embed-text\",  # Specialized embedding model that is also supported by Ollama\n",
        "    base_url=\"http://localhost:11434\",\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `nomic-embed-text` model is designed specifically for creating high-quality text embeddings. Unlike general language models that generate text, embedding models focus solely on converting text into meaningful vector representations.\n",
        "\n",
        "Now let's create an embedding for a sample query and examine its properties:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create embeddings for a query\n",
        "query = \"How do neural networks learn?\"\n",
        "query_embedding = embeddings.embed_query(query)\n",
        "print(f\"Embedding dimension: {len(query_embedding)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "Embedding dimension: 768\n",
        "```\n",
        "\n",
        "The 768-dimensional vector represents our query in mathematical space. Each dimension captures different semantic features - some might relate to technical concepts, others to question patterns, and so on. Words with similar meanings will have vectors that point in similar directions.\n",
        "\n",
        "![Diagram showing how text queries are converted to vector embeddings, with similar concepts clustering together in vector space](https://codecut.ai/wp-content/uploads/2025/05/stream_llm-1.gif)\n",
        "\n",
        "Next, we'll create embeddings for multiple documents to demonstrate similarity matching:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create embeddings for multiple documents\n",
        "documents = [\n",
        "    \"Neural networks learn through backpropagation\",\n",
        "    \"Transformers use attention mechanisms\",\n",
        "    \"LLMs are trained on text data\"\n",
        "]\n",
        "\n",
        "doc_embeddings = embeddings.embed_documents(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `embed_documents()` method processes multiple texts at once, which is more efficient than calling `embed_query()` repeatedly. This batch processing saves time when working with large document collections.\n",
        "\n",
        "To find which document best matches our query, we need to measure similarity between vectors. Cosine similarity is the standard approach:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate similarity between vectors\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Find most similar document to query\n",
        "similarities = [cosine_similarity(query_embedding, doc_emb) for doc_emb in doc_embeddings]\n",
        "most_similar_idx = np.argmax(similarities)\n",
        "print(f\"Most similar document: {documents[most_similar_idx]}\")\n",
        "print(f\"Similarity score: {similarities[most_similar_idx]:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "Most similar document: Neural networks learn through backpropagation\n",
        "Similarity score: 0.847\n",
        "```\n",
        "\n",
        "Cosine similarity returns values between -1 and 1, where 1 means identical meaning, 0 means unrelated, and -1 means opposite meanings. Our score of 0.847 indicates strong semantic similarity between the query about neural network learning and the document about backpropagation.\n",
        "\n",
        "These embeddings support several data science applications:\n",
        "\n",
        "1. **Semantic search**: Find documents by meaning rather than exact keyword matches\n",
        "2. **Document clustering**: Group related research papers, reports, or code documentation  \n",
        "3. **Retrieval-Augmented Generation (RAG)**: Retrieve relevant context before generating responses\n",
        "4. **Anomaly detection**: Identify unusual or outlier documents in large collections\n",
        "5. **Content recommendation**: Suggest similar articles, datasets, or code examples\n",
        "\n",
        "When choosing embedding models for your projects, consider these factors:\n",
        "\n",
        "- **Dimension size**: Larger dimensions (1024+) capture more nuance but require more storage and computation\n",
        "- **Domain specialization**: Some models work better for scientific text, others for general content\n",
        "- **Processing speed**: Smaller models like `nomic-embed-text` balance quality with performance\n",
        "- **Language support**: Multilingual models handle multiple languages but may sacrifice quality for any single language\n",
        "\n",
        "The quality of your embeddings directly impacts downstream tasks like search relevance and clustering accuracy. Always test different models with your specific data to find the best fit.\n",
        "\n",
        "### Building a question-answering system for your data {#building-a-question-answering-system-for-your-data}\n",
        "\n",
        "Data scientists work with extensive collections of research papers, project documentation, and dataset descriptions. When stakeholders ask questions like \"What preprocessing steps were used in the customer churn analysis?\" or \"Which machine learning models performed best for fraud detection?\", manual document search becomes time-consuming and error-prone.\n",
        "\n",
        "Standard language models can't answer these domain-specific questions because they lack access to your particular data and documentation. You need a system that searches your documents and generates accurate, source-backed answers.\n",
        "\n",
        "Retrieval-Augmented Generation (RAG) solves this problem by combining the semantic search capabilities we just built with text generation. RAG retrieves relevant information from your documents and uses it to answer questions with proper attribution.\n",
        "\n",
        "Here's how to build a RAG system using the embeddings and chat models we've already configured:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import numpy as np\n",
        "\n",
        "# Initialize components\n",
        "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
        "chat_model = ChatOllama(model=\"qwen3:0.6b\", temperature=0.3)\n",
        "\n",
        "# Sample knowledge base representing project documentation\n",
        "documents = [\n",
        "    Document(page_content=\"Python is a high-level programming language known for its simplicity and readability.\"),\n",
        "    Document(page_content=\"Machine learning algorithms can automatically learn patterns from data without explicit programming.\"),\n",
        "    Document(page_content=\"Data preprocessing involves cleaning, changing, and organizing raw data for analysis.\"),\n",
        "    Document(page_content=\"Neural networks are computational models inspired by biological brain networks.\"),\n",
        "]\n",
        "\n",
        "# Create embeddings for all documents\n",
        "doc_embeddings = embeddings.embed_documents([doc.page_content for doc in documents])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This setup creates a searchable knowledge base from your documents. In production systems, these documents would contain sections from research papers, methodology descriptions, data analysis reports, or code documentation. The embeddings convert each document into vectors that support semantic search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def similarity_search(query, top_k=2):\n",
        "    \"\"\"Find the most relevant documents for a query\"\"\"\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "    \n",
        "    # Calculate cosine similarities\n",
        "    similarities = []\n",
        "    for doc_emb in doc_embeddings:\n",
        "        similarity = np.dot(query_embedding, doc_emb) / (\n",
        "            np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)\n",
        "        )\n",
        "        similarities.append(similarity)\n",
        "    \n",
        "    # Get top-k most similar documents\n",
        "    top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "    return [documents[i] for i in top_indices]\n",
        "\n",
        "# Create RAG prompt template\n",
        "rag_prompt = PromptTemplate.from_template(\"\"\"\n",
        "Use the following context to answer the question. If the answer isn't in the context, say so.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `similarity_search` function finds documents most relevant to a question using the embeddings we created earlier. The prompt template structures how we present retrieved context to the language model, instructing it to base answers on the provided documents rather than general knowledge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def answer_question(question):\n",
        "    \"\"\"Generate an answer using retrieved context\"\"\"\n",
        "    # Retrieve relevant documents\n",
        "    relevant_docs = similarity_search(question, top_k=2)\n",
        "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    \n",
        "    # Generate answer using context\n",
        "    prompt_text = rag_prompt.format(context=context, question=question)\n",
        "    response = chat_model.invoke([{\"role\": \"user\", \"content\": prompt_text}])\n",
        "    \n",
        "    return response.content, relevant_docs\n",
        "\n",
        "# Test the RAG system\n",
        "question = \"What makes Python popular for data science?\"\n",
        "answer, sources = answer_question(question)\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")\n",
        "print(f\"Sources: {[doc.page_content[:50] + '...' for doc in sources]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "<think>\n",
        "...\n",
        "</think>\n",
        "\n",
        "Python is popular for data science because of its simplicity and readability, which make it easy to learn and use for tasks like data preprocessing.\n",
        "Sources: ['Python is a high-level programming language known ...', 'Data preprocessing involves cleaning, changing, an...']\n",
        "```\n",
        "\n",
        "The complete RAG system retrieves relevant documents, presents them as context to the language model, and generates answers based on that specific information. This approach grounds responses in your actual documentation rather than the model's general training data.\n",
        "\n",
        "RAG systems address several common data science workflow challenges:\n",
        "\n",
        "- **Project handoffs**: New team members can query past work to understand methodologies and results\n",
        "- **Literature review**: Researchers can search large paper collections for relevant techniques and findings  \n",
        "- **Data documentation**: Teams can build searchable knowledge bases about datasets, features, and processing steps\n",
        "- **Reproducibility**: Stakeholders can find detailed information about how analyses were conducted\n",
        "\n",
        "The RAG approach combines semantic search precision with natural language generation fluency. Instead of manually searching through documents or receiving generic answers from language models, you get accurate responses backed by specific sources from your knowledge base."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}