{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## From Pandas UDFs to Arrow UDFs: Next-Gen Performance\n",
        "\n",
        "The `pandas_udf` function requires converting Arrow data to Pandas format and back again for each operation. This serialization cost becomes significant when processing large datasets.\n",
        "\n",
        "PySpark 3.5+ introduces Arrow-optimized UDFs via the `useArrow=True` parameter, which operates directly on Arrow data structures, avoiding the Pandas conversion entirely and improving performance.\n",
        "\n",
        "Let's compare the performance with a weighted sum calculation across multiple columns on 100,000 rows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import pyarrow.compute as pc\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import pandas_udf, udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"UDFComparison\").getOrCreate()\n",
        "\n",
        "# Create test data with multiple numeric columns\n",
        "data = [(float(i), float(i*2), float(i*3)) for i in range(100000)]\n",
        "df = spark.createDataFrame(data, [\"val1\", \"val2\", \"val3\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a timing decorator to measure the execution time of the functions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "from functools import wraps\n",
        "\n",
        "\n",
        "# Timing decorator\n",
        "def timer(func):\n",
        "    @wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"{func.__name__}: {elapsed:.2f}s\")\n",
        "        wrapper.elapsed_time = elapsed\n",
        "        return result\n",
        "\n",
        "    return wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the timing decorator to measure the execution time of the `pandas_udf` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@pandas_udf(DoubleType())\n",
        "def weighted_sum_pandas(v1: pd.Series, v2: pd.Series, v3: pd.Series) -> pd.Series:\n",
        "    return v1 * 0.5 + v2 * 0.3 + v3 * 0.2\n",
        "\n",
        "@timer\n",
        "def run_pandas_udf():\n",
        "    result = df.select(\n",
        "        weighted_sum_pandas(df.val1, df.val2, df.val3).alias(\"weighted\")\n",
        "    )\n",
        "    result.count()  # Trigger computation\n",
        "    return result\n",
        "\n",
        "result_pandas = run_pandas_udf()\n",
        "pandas_time = run_pandas_udf.elapsed_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "run_pandas_udf: 1.33s\n",
        "```\n",
        "\n",
        "Use the timing decorator to measure the execution time of the Arrow-optimized UDF using `useArrow`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "@udf(DoubleType(), useArrow=True)\n",
        "def weighted_sum_arrow(v1, v2, v3):\n",
        "    term1 = pc.multiply(v1, 0.5)\n",
        "    term2 = pc.multiply(v2, 0.3)\n",
        "    term3 = pc.multiply(v3, 0.2)\n",
        "    return pc.add(pc.add(term1, term2), term3)\n",
        "\n",
        "@timer\n",
        "def run_arrow_udf():\n",
        "    result = df.select(\n",
        "        weighted_sum_arrow(df.val1, df.val2, df.val3).alias(\"weighted\")\n",
        "    )\n",
        "    result.count()  # Trigger computation\n",
        "    return result\n",
        "\n",
        "result_arrow = run_arrow_udf()\n",
        "arrow_time = run_arrow_udf.elapsed_time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "run_arrow_udf: 0.43s\n",
        "```\n",
        "\n",
        "Measure the speedup:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "speedup = pandas_time / arrow_time\n",
        "print(f\"Speedup: {speedup:.2f}x faster\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "Speedup: 3.06x faster\n",
        "```\n",
        "\n",
        "The output shows that the Arrow-optimized version is 3.06x faster than the `pandas_udf` version!\n",
        "\n",
        "The performance gain comes from avoiding serialization. Arrow-optimized UDFs use PyArrow compute functions like `pc.multiply()` and `pc.add()` directly on Arrow data, while `pandas_udf` must convert each column to Pandas and back.\n",
        "\n",
        "**Trade-off**: The 3.06x performance improvement comes at the cost of using PyArrow's less familiar compute API instead of Pandas operations. However, this becomes increasingly valuable as dataset size and column count grow.\n",
        "\n",
        "## Native Data Visualization (PySpark 4.0+)\n",
        "\n",
        "Visualizing PySpark DataFrames traditionally requires converting to Pandas first, then using external libraries like matplotlib or plotly. This adds memory overhead and extra processing steps.\n",
        "\n",
        "PySpark 4.0 introduces a native plotting API powered by Plotly, enabling direct visualization from PySpark DataFrames without any conversion.\n",
        "\n",
        "Let's visualize sales data across product categories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Visualization\").getOrCreate()\n",
        "\n",
        "# Create sample sales data\n",
        "sales_data = [\n",
        "    (\"Electronics\", 5000, 1200),\n",
        "    (\"Electronics\", 7000, 1800),\n",
        "    (\"Clothing\", 3000, 800),\n",
        "    (\"Clothing\", 4500, 1100),\n",
        "    (\"Furniture\", 6000, 1500),\n",
        "    (\"Furniture\", 8000, 2000),\n",
        "]\n",
        "\n",
        "sales_df = spark.createDataFrame(sales_data, [\"category\", \"sales\", \"profit\"])\n",
        "sales_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "+-----------+-----+------+\n",
        "|   category|sales|profit|\n",
        "+-----------+-----+------+\n",
        "|Electronics| 5000|  1200|\n",
        "|Electronics| 7000|  1800|\n",
        "|   Clothing| 3000|   800|\n",
        "|   Clothing| 4500|  1100|\n",
        "|  Furniture| 6000|  1500|\n",
        "|  Furniture| 8000|  2000|\n",
        "+-----------+-----+------+\n",
        "```\n",
        "\n",
        "Create a scatter plot directly from the PySpark DataFrame using the `.plot()` method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Direct plotting without conversion\n",
        "sales_df.plot(kind=\"scatter\", x=\"sales\", y=\"profit\", color=\"category\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Scatter Plot](https://codecut.ai/wp-content/uploads/2025/11/plotly.png)\n",
        "\n",
        "You can also use shorthand methods such as `plot.scatter()` and `plot.bar()` for specific chart types:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Scatter plot with shorthand\n",
        "sales_df.plot.scatter(x=\"sales\", y=\"profit\", color=\"category\")\n",
        "\n",
        "# Bar chart by category\n",
        "category_totals = sales_df.groupBy(\"category\").agg({\"sales\": \"sum\"}).withColumnRenamed(\"sum(sales)\", \"total_sales\")\n",
        "category_totals.plot.bar(x=\"category\", y=\"total_sales\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The native plotting API supports 8 chart types:\n",
        "- **scatter**: Scatter plots with color grouping\n",
        "- **bar**: Bar charts for categorical comparisons\n",
        "- **line**: Line plots for time series\n",
        "- **area**: Area charts for cumulative values\n",
        "- **pie**: Pie charts for proportions\n",
        "- **box**: Box plots for distributions\n",
        "- **histogram**: Histograms for frequency analysis\n",
        "- **kde/density**: Density plots for probability distributions\n",
        "\n",
        "By default, PySpark visualizes up to 1,000 rows. For larger datasets, configure the limit:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Increase visualization row limit\n",
        "spark.conf.set(\"spark.sql.pyspark.plotting.max_rows\", 5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dynamic Schema Generation with UDTF analyze() (PySpark 4.0+)\n",
        "\n",
        "Python UDTFs (User-Defined Table Functions) generate multiple rows from a single input row, but they come with a critical limitation: you must define the output schema upfront. When your output columns depend on the input data itself (like creating pivot tables or dynamic aggregations where column names come from data values), this rigid schema requirement becomes a problem.\n",
        "\n",
        "For example, a word-counting UDTF requires you to specify all output columns upfront, even though the words themselves are unknown until runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import udtf, lit\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType\n",
        "\n",
        "# Schema must be defined upfront with fixed column names\n",
        "@udtf(returnType=StructType([\n",
        "    StructField(\"hello\", IntegerType()),\n",
        "    StructField(\"world\", IntegerType()),\n",
        "    StructField(\"spark\", IntegerType())\n",
        "]))\n",
        "class StaticWordCountUDTF:\n",
        "    def eval(self, text: str):\n",
        "        words = text.split(\" \")\n",
        "        yield tuple(words.count(word) for word in [\"hello\", \"world\", \"spark\"])\n",
        "\n",
        "# Only works for exactly these three words\n",
        "result = StaticWordCountUDTF(lit(\"hello world hello spark\"))\n",
        "result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "+-----+-----+-----+\n",
        "|hello|world|spark|\n",
        "+-----+-----+-----+\n",
        "|    2|    1|    1|\n",
        "+-----+-----+-----+\n",
        "```\n",
        "\n",
        "If the input text contains a different set of words, the output won't contain the count of the new words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = StaticWordCountUDTF(lit(\"hi world hello spark\"))\n",
        "result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "+-----+-----+-----+\n",
        "|hello|world|spark|\n",
        "+-----+-----+-----+\n",
        "|    1|    1|    1|\n",
        "+-----+-----+-----+\n",
        "```\n",
        "\n",
        "PySpark 4.0 introduces the `analyze()` method for UDTFs, enabling dynamic schema determination based on input data. Instead of hardcoding your output schema, `analyze()` inspects the input and generates the appropriate columns at runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql.functions import udtf, lit\n",
        "from pyspark.sql.types import StructType, IntegerType\n",
        "from pyspark.sql.udtf import AnalyzeArgument, AnalyzeResult\n",
        "\n",
        "@udtf\n",
        "class DynamicWordCountUDTF:\n",
        "    @staticmethod\n",
        "    def analyze(text: AnalyzeArgument) -> AnalyzeResult:\n",
        "        \"\"\"Dynamically create schema based on input text\"\"\"\n",
        "        schema = StructType()\n",
        "        # Create one column per unique word in the input\n",
        "        for word in sorted(set(text.value.split(\" \"))):\n",
        "            schema = schema.add(word, IntegerType())\n",
        "        return AnalyzeResult(schema=schema)\n",
        "\n",
        "    def eval(self, text: str):\n",
        "        \"\"\"Generate counts for each word\"\"\"\n",
        "        words = text.split(\" \")\n",
        "        # Use same logic as analyze() to determine column order\n",
        "        unique_words = sorted(set(words))\n",
        "        yield tuple(words.count(word) for word in unique_words)\n",
        "\n",
        "# Schema adapts to any input text\n",
        "result = DynamicWordCountUDTF(lit(\"hello world hello spark\"))\n",
        "result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "+-----+-----+-----+\n",
        "|hello|spark|world|\n",
        "+-----+-----+-----+\n",
        "|    2|    1|    1|\n",
        "+-----+-----+-----+\n",
        "```\n",
        "\n",
        "Now try with completely different words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Different words - schema adapts automatically\n",
        "result2 = DynamicWordCountUDTF(lit(\"python data science\"))\n",
        "result2.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```text\n",
        "+----+------+-------+\n",
        "|data|python|science|\n",
        "+----+------+-------+\n",
        "|   1|     1|      1|\n",
        "+----+------+-------+\n",
        "```\n",
        "\n",
        "The columns change from `hello`, `spark`, `world` to `data`, `python`, `science` without any code modifications."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}