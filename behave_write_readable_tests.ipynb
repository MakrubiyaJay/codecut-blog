{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Motivation {#motivation}\n",
        "\n",
        "> ðŸ“– Read the full article: [Behave: Write Readable ML Tests with Behavior-Driven Development](https://codecut.ai/behave-write-readable-ml-tests-bdd/)\n",
        "\n",
        "\n",
        "Imagine you create an ML model to predict customer sentiment based on reviews. Upon deploying it, you realize that the model incorrectly labels certain positive reviews as negative when they're rephrased using negative words.\n",
        "\n",
        "![Image showing sentiment analysis example](https://codecut.ai/wp-content/uploads/2025/11/behave_invariance_testing.png)\n",
        "\n",
        "This is just one example of how an extremely accurate ML model can fail without proper testing. Thus, testing your model for accuracy and reliability is crucial before deployment.\n",
        "\n",
        "But how do you test your ML model? One straightforward approach is to use unit-test:\n",
        "\n",
        "```python\n",
        "from textblob import TextBlob\n",
        "\n",
        "def test_sentiment_the_same_after_paraphrasing():\n",
        "    sent = \"The hotel room was great! It was spacious, clean and had a nice view of the city.\"\n",
        "    sent_paraphrased = \"The hotel room wasn't bad. It wasn't cramped, dirty, and had a decent view of the city.\"\n",
        "\n",
        "    sentiment_original = TextBlob(sent).sentiment.polarity\n",
        "    sentiment_paraphrased = TextBlob(sent_paraphrased).sentiment.polarity\n",
        "\n",
        "    both_positive = (sentiment_original > 0) and (sentiment_paraphrased > 0)\n",
        "    both_negative = (sentiment_original < 0) and (sentiment_paraphrased < 0)\n",
        "    assert both_positive or both_negative\n",
        "```\n",
        "\n",
        "This approach works but can be challenging for non-technical or business participants to understand. Wouldn't it be nice if you could incorporate **project objectives and goals** into your tests, expressed in **natural language**?\n",
        "\n",
        "```yaml\n",
        "Feature: Sentiment Analysis\n",
        "  As a data scientist\n",
        "  I want to ensure that my model is invariant to paraphrasing\n",
        "  So that my model can produce consistent results.\n",
        "\n",
        "  Scenario: Paraphrased text\n",
        "    Given a text\n",
        "    When the text is paraphrased\n",
        "    Then both text should have the same sentiment\n",
        "```\n",
        "\n",
        "That is when behave comes in handy.\n",
        "\n",
        "> ðŸ’» **Get the Code**: The complete source code for this tutorial is available on [GitHub](https://github.com/khuyentran1401/Data-science/tree/master/data_science_tools/behave_examples). Clone it to follow along!\n",
        "\n",
        "\n",
        "## What is behave? {#what-is-behave}\n",
        "\n",
        "[behave](https://github.com/behave/behave) is a Python framework for behavior-driven development (BDD). BDD is a software development methodology that:\n",
        "\n",
        "- Emphasizes collaboration between stakeholders (such as business analysts, developers, and testers)\n",
        "- Enables users to define requirements and specifications for a software application\n",
        "\n",
        "Since behave provides a common language and format for expressing requirements and specifications, it can be ideal for defining and validating the behavior of machine learning models.\n",
        "\n",
        "To install behave, type:\n",
        "\n",
        "```bash\n",
        "pip install behave\n",
        "```\n",
        "\n",
        "Let's use behave to perform various tests on machine learning models.\n",
        "\n",
        "> ðŸ“š For comprehensive unit testing strategies and best practices, check out [Production-Ready Data Science](https://codecut.ai/production-ready-data-science/).\n",
        "\n",
        "\n",
        "## Invariance Testing {#invariance-testing}\n",
        "\n",
        "Invariance testing tests whether an ML model produces consistent results under different conditions.\n",
        "\n",
        "An example of invariance testing involves verifying if a model is invariant to paraphrasing. An ideal model should maintain consistent sentiment scores even when a positive review is rephrased using negative words like \"wasn't bad\" instead of \"was good.\"\n",
        "\n",
        "![Image showing paraphrasing test example](https://codecut.ai/wp-content/uploads/2025/11/behave_invariance_testing-2.png)\n",
        "\n",
        "### Feature File {#feature-file}\n",
        "\n",
        "To use behave for invariance testing, create a directory called `features`. Under that directory, create a file called `invariant_test_sentiment.feature`.\n",
        "\n",
        "```bash\n",
        "â””â”€â”€  features/\n",
        "  â””â”€â”€â”€  invariant_test_sentiment.feature\n",
        "```\n",
        "\n",
        "Within the `invariant_test_sentiment.feature` file, we will specify the project requirements:\n",
        "\n",
        "```yaml\n",
        "Feature: Sentiment Analysis\n",
        "  As a data scientist\n",
        "  I want to ensure that my model is invariant to paraphrasing\n",
        "  So that my model can produce consistent results.\n",
        "\n",
        "  Scenario: Paraphrased text\n",
        "    Given a text\n",
        "    When the text is paraphrased\n",
        "    Then both text should have the same sentiment\n",
        "```\n",
        "\n",
        "The \"Given,\" \"When,\" and \"Then\" parts of this file present the actual steps that will be executed by behave during the test.\n",
        "\n",
        "The Feature section serves as living documentation to provide context but does not trigger test execution.\n",
        "\n",
        "### Python Step Implementation {#python-step-implementation}\n",
        "\n",
        "To implement the steps used in the scenarios with Python, start with creating the `features/steps` directory and a file called `invariant_test_sentiment.py` within it:\n",
        "\n",
        "```bash\n",
        "â””â”€â”€  features/\n",
        "    â”œâ”€â”€â”€â”€  invariant_test_sentiment.feature\n",
        "    â””â”€â”€â”€â”€  steps/\n",
        "       â””â”€â”€â”€â”€  invariant_test_sentiment.py\n",
        "```\n",
        "\n",
        "The `invariant_test_sentiment.py` file contains the following code, which tests whether the sentiment produced by the [TextBlob](https://textblob.readthedocs.io/en/dev/) model is consistent between the original text and its paraphrased version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from behave import given, then, when\n",
        "from textblob import TextBlob\n",
        "\n",
        "@given(\"a text\")\n",
        "def step_given_positive_sentiment(context):\n",
        "    context.sent = \"The hotel room was great! It was spacious, clean and had a nice view of the city.\"\n",
        "\n",
        "@when(\"the text is paraphrased\")\n",
        "def step_when_paraphrased(context):\n",
        "    context.sent_paraphrased = \"The hotel room wasn't bad. It wasn't cramped, dirty, and had a decent view of the city.\"\n",
        "\n",
        "@then(\"both text should have the same sentiment\")\n",
        "def step_then_sentiment_analysis(context):\n",
        "    # Get sentiment of each sentence\n",
        "    sentiment_original = TextBlob(context.sent).sentiment.polarity\n",
        "    sentiment_paraphrased = TextBlob(context.sent_paraphrased).sentiment.polarity\n",
        "\n",
        "    # Print sentiment\n",
        "    print(f\"Sentiment of the original text: {sentiment_original:.2f}\")\n",
        "    print(f\"Sentiment of the paraphrased sentence: {sentiment_paraphrased:.2f}\")\n",
        "\n",
        "    # Assert that both sentences have the same sentiment\n",
        "    both_positive = (sentiment_original > 0) and (sentiment_paraphrased > 0)\n",
        "    both_negative = (sentiment_original < 0) and (sentiment_paraphrased < 0)\n",
        "    assert both_positive or both_negative"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explanation of the code above:\n",
        "\n",
        "- The steps are identified using decorators matching the feature's predicate: `given`, `when`, and `then`.\n",
        "- The decorator accepts a string containing the rest of the phrase in the matching scenario step.\n",
        "- The `context` variable allows you to share values between steps.\n",
        "\n",
        "### Run the Test {#run-the-test}\n",
        "\n",
        "To run the `invariant_test_sentiment.feature` test, type the following command:\n",
        "\n",
        "```bash\n",
        "behave features/invariant_test_sentiment.feature\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```python\n",
        "Feature: Sentiment Analysis # features/invariant_test_sentiment.feature:1\n",
        "  As a data scientist\n",
        "  I want to ensure that my model is invariant to paraphrasing\n",
        "  So that my model can produce consistent results in real-world scenarios.\n",
        "  Scenario: Paraphrased text\n",
        "    Given a text\n",
        "    When the text is paraphrased\n",
        "    Then both text should have the same sentiment\n",
        "      Traceback (most recent call last):\n",
        "          assert both_positive or both_negative\n",
        "      AssertionError\n",
        "\n",
        "      Captured stdout:\n",
        "      Sentiment of the original text: 0.66\n",
        "      Sentiment of the paraphrased sentence: -0.38\n",
        "\n",
        "Failing scenarios:\n",
        "  features/invariant_test_sentiment.feature:6  Paraphrased text\n",
        "\n",
        "0 features passed, 1 failed, 0 skipped\n",
        "0 scenarios passed, 1 failed, 0 skipped\n",
        "2 steps passed, 1 failed, 0 skipped, 0 undefined\n",
        "```\n",
        "\n",
        "The output shows that the first two steps passed and the last step failed, indicating that the model is affected by paraphrasing.\n",
        "\n",
        "## Directional Testing {#directional-testing}\n",
        "\n",
        "Directional testing is a statistical method used to assess whether the impact of an independent variable on a dependent variable is in a particular direction, either positive or negative.\n",
        "\n",
        "An example of directional testing is to check whether the presence of a specific word has a positive or negative effect on the sentiment score of a given text.\n",
        "\n",
        "![Image showing directional testing example](https://codecut.ai/wp-content/uploads/2025/11/behave_directional_testing.png)\n",
        "\n",
        "To use behave for directional testing, we will create two files `directional_test_sentiment.feature` and `directional_test_sentiment.py`.\n",
        "\n",
        "```bash\n",
        "â””â”€â”€  features/\n",
        "    â”œâ”€â”€â”€â”€  directional_test_sentiment.feature\n",
        "    â””â”€â”€â”€â”€  steps/\n",
        "       â””â”€â”€â”€â”€  directional_test_sentiment.py\n",
        "```\n",
        "\n",
        "### Feature File {#feature-file-1}\n",
        "\n",
        "The code in `directional_test_sentiment.feature` specifies the requirements of the project as follows:\n",
        "\n",
        "```yaml\n",
        "Feature: Sentiment Analysis with Specific Word\n",
        "  As a data scientist\n",
        "  I want to ensure that the presence of a specific word\n",
        "  has a positive or negative effect on the sentiment score of a text\n",
        "\n",
        "  Scenario: Sentiment analysis with specific word\n",
        "    Given a sentence\n",
        "    And the same sentence with the addition of the word 'awesome'\n",
        "    When I input the new sentence into the model\n",
        "    Then the sentiment score should increase\n",
        "```\n",
        "\n",
        "Notice that \"And\" is added to the prose. Since the preceding step starts with \"Given,\" behave will rename \"And\" to \"Given.\"\n",
        "\n",
        "### Python Step Implementation {#python-step-implementation-1}\n",
        "\n",
        "The code in `directional_test_sentiment.py` implements a test scenario, which checks whether the presence of the word \"awesome \" positively affects the sentiment score generated by the TextBlob model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from behave import given, then, when\n",
        "from textblob import TextBlob\n",
        "\n",
        "@given(\"a sentence\")\n",
        "def step_given_positive_word(context):\n",
        "    context.sent = \"I love this product\"\n",
        "\n",
        "@given(\"the same sentence with the addition of the word '{word}'\")\n",
        "def step_given_a_positive_word(context, word):\n",
        "    context.new_sent = f\"I love this {word} product\"\n",
        "\n",
        "@when(\"I input the new sentence into the model\")\n",
        "def step_when_use_model(context):\n",
        "    context.sentiment_score = TextBlob(context.sent).sentiment.polarity\n",
        "    context.adjusted_score = TextBlob(context.new_sent).sentiment.polarity\n",
        "\n",
        "@then(\"the sentiment score should increase\")\n",
        "def step_then_positive(context):\n",
        "    assert context.adjusted_score > context.sentiment_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The second step uses the parameter syntax `{word}`. When the `.feature` file is run, the value specified for `{word}` in the scenario is automatically passed to the corresponding step function.\n",
        "\n",
        "This means that if the scenario states that the same sentence should include the word \"awesome,\" behave will automatically replace `{word}` with \"awesome.\"\n",
        "\n",
        "> This conversion is useful when you want to use different values for the `{word}` parameter without changing both the `.feature` file and the `.py` file.\n",
        "\n",
        "### Run the Test {#run-the-test-1}\n",
        "\n",
        "```bash\n",
        "behave features/directional_test_sentiment.feature\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```python\n",
        "Feature: Sentiment Analysis with Specific Word\n",
        "  As a data scientist\n",
        "  I want to ensure that the presence of a specific word has a positive or negative effect on the sentiment score of a text\n",
        "  Scenario: Sentiment analysis with specific word\n",
        "    Given a sentence\n",
        "    And the same sentence with the addition of the word 'awesome'\n",
        "    When I input the new sentence into the model\n",
        "    Then the sentiment score should increase\n",
        "\n",
        "1 feature passed, 0 failed, 0 skipped\n",
        "1 scenario passed, 0 failed, 0 skipped\n",
        "4 steps passed, 0 failed, 0 skipped, 0 undefined\n",
        "```\n",
        "\n",
        "Since all the steps passed, we can infer that the sentiment score increases due to the new word's presence.\n",
        "\n",
        "## Minimum Functionality Testing {#minimum-functionality-testing}\n",
        "\n",
        "Minimum functionality testing is a type of testing that verifies if the system or product meets the minimum requirements and is functional for its intended use.\n",
        "\n",
        "One example of minimum functionality testing is to check whether the model can handle different types of inputs, such as numerical, categorical, or textual data. To test with diverse inputs, generate test data using [Faker](https://codecut.ai/faker-python-generate-test-data/) for more comprehensive validation.\n",
        "\n",
        "![Image showing minimum functionality testing example](https://codecut.ai/wp-content/uploads/2025/11/minimum_functionality_testing.png)\n",
        "\n",
        "To use minimum functionality testing for input validation, create two files `minimum_func_test_input.feature` and `minimum_func_test_input.py`.\n",
        "\n",
        "```bash\n",
        "â””â”€â”€  features/\n",
        "    â”œâ”€â”€â”€â”€  minimum_func_test_input.feature\n",
        "    â””â”€â”€â”€â”€  steps/\n",
        "       â””â”€â”€â”€â”€  minimum_func_test_input.py\n",
        "```\n",
        "\n",
        "### Feature File {#feature-file-2}\n",
        "\n",
        "The code in `minimum_func_test_input.feature` specifies the project requirements as follows:\n",
        "\n",
        "```yaml\n",
        "Feature: Test my_ml_model\n",
        "\n",
        "  Scenario: Test integer input\n",
        "    Given I have an integer input of 42\n",
        "    When I run the model\n",
        "    Then the output should be an array of one number\n",
        "\n",
        "  Scenario: Test float input\n",
        "    Given I have a float input of 3.14\n",
        "    When I run the model\n",
        "    Then the output should be an array of one number\n",
        "\n",
        "  Scenario: Test list input\n",
        "    Given I have a list input of [1, 2, 3]\n",
        "    When I run the model\n",
        "    Then the output should be an array of three numbers\n",
        "```\n",
        "\n",
        "\n",
        "### Python Step Implementation {#python-step-implementation-2}\n",
        "\n",
        "The code in `minimum_func_test_input.py` implements the requirements, checking if the output generated by `predict` for a specific input type meets the expectations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from behave import given, then, when\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from typing import Union\n",
        "\n",
        "def predict(input_data: Union[int, float, str, list]):\n",
        "    \"\"\"Create a model to predict input data\"\"\"\n",
        "\n",
        "    # Reshape the input data\n",
        "    if isinstance(input_data, (int, float, list)):\n",
        "        input_array = np.array(input_data).reshape(-1, 1)\n",
        "    else:\n",
        "        raise ValueError(\"Input type not supported\")\n",
        "\n",
        "    # Create a linear regression model\n",
        "    model = LinearRegression()\n",
        "\n",
        "    # Train the model on a sample dataset\n",
        "    X = np.array([[1], [2], [3], [4], [5]])\n",
        "    y = np.array([2, 4, 6, 8, 10])\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Predict the output using the input array\n",
        "    return model.predict(input_array)\n",
        "\n",
        "@given(\"I have an integer input of {input_value}\")\n",
        "def step_given_integer_input(context, input_value):\n",
        "    context.input_value = int(input_value)\n",
        "\n",
        "@given(\"I have a float input of {input_value}\")\n",
        "def step_given_float_input(context, input_value):\n",
        "    context.input_value = float(input_value)\n",
        "\n",
        "@given(\"I have a list input of {input_value}\")\n",
        "def step_given_list_input(context, input_value):\n",
        "    context.input_value = eval(input_value)\n",
        "\n",
        "@when(\"I run the model\")\n",
        "def step_when_run_model(context):\n",
        "    context.output = predict(context.input_value)\n",
        "\n",
        "@then(\"the output should be an array of one number\")\n",
        "def step_then_check_output(context):\n",
        "    assert isinstance(context.output, np.ndarray)\n",
        "    assert all(isinstance(x, (int, float)) for x in context.output)\n",
        "    assert len(context.output) == 1\n",
        "\n",
        "@then(\"the output should be an array of three numbers\")\n",
        "def step_then_check_output(context):\n",
        "    assert isinstance(context.output, np.ndarray)\n",
        "    assert all(isinstance(x, (int, float)) for x in context.output)\n",
        "    assert len(context.output) == 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the Test {#run-the-test-2}\n",
        "\n",
        "```bash\n",
        "behave features/minimum_func_test_input.feature\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```python\n",
        "Feature: Test my_ml_model\n",
        "\n",
        "  Scenario: Test integer input\n",
        "    Given I have an integer input of 42\n",
        "    When I run the model\n",
        "    Then the output should be an array of one number\n",
        "\n",
        "  Scenario: Test float input\n",
        "    Given I have a float input of 3.14\n",
        "    When I run the model\n",
        "    Then the output should be an array of one number\n",
        "\n",
        "  Scenario: Test list input\n",
        "    Given I have a list input of [1, 2, 3]\n",
        "    When I run the model\n",
        "    Then the output should be an array of three numbers\n",
        "\n",
        "1 feature passed, 0 failed, 0 skipped\n",
        "3 scenarios passed, 0 failed, 0 skipped\n",
        "9 steps passed, 0 failed, 0 skipped, 0 undefined\n",
        "```\n",
        "\n",
        "Since all the steps passed, we can conclude that the model outputs match our expectations.\n",
        "\n",
        "## Behave's Trade-offs {#behaves-trade-offs}\n",
        "\n",
        "This section will outline some drawbacks of using behave compared to [pytest](https://codecut.ai/pytest-for-data-scientists/), and explain why it may still be worth considering the tool.\n",
        "\n",
        "### Learning Curve {#learning-curve}\n",
        "\n",
        "Using Behavior-Driven Development (BDD) in behavior may result in a steeper learning curve than the more traditional testing approach used by pytest.\n",
        "\n",
        "> **Counter argument:** The focus on collaboration in BDD can lead to better alignment between business requirements and software development, resulting in a more efficient development process overall.\n",
        "\n",
        "\n",
        "### Slower performance {#slower-performance}\n",
        "\n",
        "behave tests can be slower than pytest tests because behave must parse the feature files and map them to step definitions before running the tests.\n",
        "\n",
        "> **Counter argument:** behave's focus on well-defined steps can lead to tests that are easier to understand and modify, reducing the overall effort required for test maintenance.\n",
        "\n",
        "### Less flexibility {#less-flexibility}\n",
        "\n",
        "behave is more rigid in its syntax, while pytest allows more flexibility in defining tests and fixtures.\n",
        "\n",
        "> **Counter argument:** behave's rigid structure can help ensure consistency and readability across tests, making them easier to understand and maintain over time.\n",
        "\n",
        "\n",
        "## Related Tutorials\n",
        "\n",
        "- **Configuration Management**: [Hydra for Python Configuration](https://codecut.ai/stop-hard-coding-in-a-data-science-project-use-configuration-files-instead/) for managing test specifications with YAML-like syntax"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}