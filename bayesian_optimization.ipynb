{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Installing Libraries and Simulated Function {#setup-installing-libraries-and-simulated-function}\n",
        "\n",
        "Let's begin by installing and importing the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel as C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll also define a synthetic **black-box function** to optimize. In a data science project, the black-box function typically represents the entire training and evaluation pipeline of a machine learning model. Given a set of hyperparameters as input, it returns the corresponding error or loss value.\n",
        "\n",
        "![](https://codecut.ai/wp-content/uploads/2025/06/black-box-flowchart.png)\n",
        "\n",
        "We call it a \"black-box function\" because, in real life, we don't have an explicit mathematical formula for how hyperparameters affect the final performance. Instead, we can only train and observe the output error, which is expensive and time-consuming.\n",
        "\n",
        "Here, we're using a simplified synthetic function to emulate this process, but the principles we demonstrate apply equally to higher-dimensional hyperparameter spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def black_box_function(x):\n",
        "    return - (np.sin(3*x) + 0.5 * x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate 1000 evenly spaced values between 0 and 5.5\n",
        "X = np.linspace(0, 5.5, 1000).reshape(-1, 1)\n",
        "\n",
        "# Evaluate the black-box function on the generated values\n",
        "y = black_box_function(X)\n",
        "\n",
        "# Plot\n",
        "plt.plot(X, y, \"--\", color=\"white\")\n",
        "plt.title(\"Black-box function\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://codecut.ai/wp-content/uploads/2025/06/image.png)\n",
        "\n",
        "In the plot above:\n",
        "\n",
        "- The **x-axis** represents a hyperparameter (e.g., learning rate, number of epochs).\n",
        "- The **y-axis** shows the model's performance (e.g,. error or loss).\n",
        "- The function's **wavy shape** reflects the unpredictable behavior of real-world performance across hyperparameters.\n",
        "- The **lowest point** on the curve is the **optimal hyperparameter,** where the model performs best.\n",
        "\n",
        "Key insights:\n",
        "\n",
        "- **Challenge**: The true performance curve is unknown and costly to evaluate.\n",
        "- **Goal**: Efficiently identify the best hyperparameter setting (i.e., the global minimum).\n",
        "- **Approach**: Start with brute-force as a baseline, then introduce Gaussian Process-based Bayesian Optimization to search smarter and faster.\n",
        "\n",
        "## Brute force hyperparameter search {#brute-force-hyperparameter-search}\n",
        "\n",
        "Imagine that x is your hyperparameter. You could try all the hyperparameters (multiple x values) and test which one is better:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a grid of 100 evenly spaced values between 0 and 2\n",
        "X_grid = np.linspace(0, 2, 100).reshape(-1, 1)\n",
        "\n",
        "# Evaluate the black-box function at each point in the grid\n",
        "y_grid = black_box_function(X_grid)\n",
        "\n",
        "# Identify the input value that gives the maximum function output\n",
        "x_best = X_grid[np.argmax(y_grid)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot this approach:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(X_grid, y_grid, '--', color='white', label=\"True function\")\n",
        "plt.scatter(X_grid, y_grid, c=\"#E583B6\", label=\"Evaluated Points\")\n",
        "plt.scatter(x_best, black_box_function(x_best), c=\"#72BEFA\", s=80, edgecolors=\"black\", label=\"Best Point\")\n",
        "\n",
        "plt.title(\"Brute Force Search Over Full Range\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://codecut.ai/wp-content/uploads/2025/06/image-6.png)\n",
        "\n",
        "While this approach works, it is extremely inefficient and computationally expensive for real-world hyperparameter tuning.\n",
        "\n",
        "For example, if `x` represents the number of epochs, evaluating 1000 different values means running 1000 separate training sessions, which could take days for large models.\n",
        "\n",
        "Here's a simple simulation of how costly that can be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def train_model(epochs):\n",
        "    time.sleep(0.1)  # Simulate a slow training step\n",
        "    return - (np.sin(3 * epochs) + 0.5 * epochs)\n",
        "\n",
        "search_space = np.linspace(0, 5, 1000)\n",
        "results = []\n",
        "\n",
        "start = time.time()\n",
        "for x in search_space:\n",
        "    loss = train_model(x)\n",
        "    results.append((x, loss))\n",
        "end = time.time()\n",
        "\n",
        "print(\"Best x:\", search_space[np.argmin([r[1] for r in results])])\n",
        "print(\"Time taken:\", round(end - start, 2), \"seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```\n",
        "Best x: 4.76976976976977\n",
        "Time taken: 103.8 seconds\n",
        "```\n",
        "\n",
        "Even in this small example, evaluating just 1000 points with a minimal 0.1-second delay already takes nearly two minutes, showing how quickly brute-force methods can become inefficient as the parameter space grows.\n",
        "\n",
        "## Introducing Gaussian Process Regression {#introducing-gaussian-process-regression}\n",
        "\n",
        "A Gaussian Process is a probabilistic model that predicts not just the output of a function but also the uncertainty of that prediction by modeling a distribution over possible functions that fit the observed data.\n",
        "\n",
        "To use a Gaussian Process, we need:\n",
        "\n",
        "- A set of input-output pairs (initial evaluations of the function)\n",
        "- A kernel function to define the similarity between input points\n",
        "- A regression framework to fit the model and make predictions with uncertainty estimates\n",
        "\n",
        "We begin by testing the model at a few chosen hyperparameter values and recording their results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initial sample points (simulate prior evaluations)\n",
        "X_sample = np.array([[1.0], [3.0], [5.5]])\n",
        "y_sample = black_box_function(X_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a real-life scenario, if `x` is our hyperparameter, this would be like training a model using `x = 30` epochs, `x = 50` epochs, and `x = 100` epochs to see how performance changes with that hyperparameter.\n",
        "\n",
        "Next, define a **kernel function** to define the similarity between input points. We'll use a flexible kernel that combines Matern and WhiteKernel to capture noise and complexity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define the kernel\n",
        "kernel = C(1.0) * Matern(length_scale=1.0, nu=2.5) + WhiteKernel(noise_level=1e-5)\n",
        "\n",
        "# Create and fit the Gaussian Process model\n",
        "gpr = GaussianProcessRegressor(kernel=kernel, alpha=0.0)\n",
        "gpr.fit(X_sample, y_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, let's predict the function values across our domain and visualize the model's mean prediction and uncertainty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Predict across the domain\n",
        "mu, std = gpr.predict(X, return_std=True)\n",
        "\n",
        "# Plot the result\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(X, y, 'k--', label=\"True function\")\n",
        "plt.plot(X, mu, 'b-', label=\"GPR mean\")\n",
        "plt.fill_between(X.ravel(), mu - std, mu + std, alpha=0.3, label=\"Uncertainty\")\n",
        "plt.scatter(X_sample, y_sample, c='red', label=\"Samples\")\n",
        "plt.legend()\n",
        "plt.title(\"Gaussian Process Fit\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://codecut.ai/wp-content/uploads/2025/06/image-1.png)\n",
        "\n",
        "In the plot above:\n",
        "\n",
        "- The **black dashed line** shows the true (unknown) function we aim to approximate.\n",
        "- The **red dots** are the sample points we've already evaluated—our training data.\n",
        "- The **blue line** is the Gaussian Process's predicted mean of the function.\n",
        "- The **shaded blue area** represents the model's uncertainty—one standard deviation above and below the mean.\n",
        "\n",
        "Notice how the uncertainty is low near the red dots, where we have data, and higher in regions where we haven't sampled yet. This is one of the key strengths of GPR: it doesn't just give us a prediction, but also tells us how **confident** it is in that prediction.\n",
        "\n",
        "In the next step, we'll use this uncertainty to guide where to sample next.\n",
        "\n",
        "## Bayesian Optimization Step {#bayesian-optimization-step}\n",
        "\n",
        "To pick the next hyperparameter to evaluate, we'll define a quantity called **Expected Improvement (EI)**. This quantity is our polar star, which will guide us to the next point to sample.\n",
        "\n",
        "More specifically, EI is a statistical value that combines two things: the **mean** prediction of our Gaussian Process model (how good we think the function might be at a certain point) and the **standard deviation** (how uncertain we are about that prediction).\n",
        "\n",
        "EI helps us decide where to sample next based on both potential performance and uncertainty:\n",
        "\n",
        "- If the model predicts a low mean and has low uncertainty (confident that the point is good), EI will be high. It's worth sampling.\n",
        "- If uncertainty is high but the potential gain is large, EI will also be high. We should explore that point.\n",
        "- If the model predicts a high mean and is confident (point is likely bad), EI will be low. It's not worth evaluating.\n",
        "\n",
        "As we will show below, the point with the **largest EI** will be selected as the next one to sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "def expected_improvement(X, X_sample, y_sample, model, xi=0.01):\n",
        "    mu, std = model.predict(X, return_std=True)\n",
        "    mu_sample_opt = np.min(y_sample)\n",
        "\n",
        "    with np.errstate(divide='warn'):\n",
        "        imp = mu_sample_opt - mu - xi  # because we are minimizing\n",
        "        Z = imp / std\n",
        "        ei = imp * norm.cdf(Z) + std * norm.pdf(Z)\n",
        "        ei[std == 0.0] = 0.0\n",
        "\n",
        "    return ei"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's compute and plot the EI over the search space:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ei = expected_improvement(X, X_sample, y_sample, gpr)\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(X, ei, label=\"Expected Improvement\")\n",
        "plt.axvline(X[np.argmax(ei)], color='r', linestyle='--', label=\"Next sample point\")\n",
        "plt.title(\"Acquisition Function (Expected Improvement)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"EI(x)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://codecut.ai/wp-content/uploads/2025/06/image-2.png)\n",
        "\n",
        "As we can see, we are correctly identifying the area where to sample, which is in the x in [3,5], as the area with the largest expected improvement. The maximum of EI will be the next point to sample. Once we sample that point, we can repeat the process in a loop, as shown in the next section.\n",
        "\n",
        "## Hyperparameter Search Loop {#hyperparameter-search-loop}\n",
        "\n",
        "Let's put everything in a loop that will search for the minimum of our function:\n",
        "\n",
        "1. **Fit** Gaussian Process Regression (GPR) on a limited number of samples (input points and their loss values)\n",
        "2. **Compute** the Expected Improvement (EI) on a larger number of sample points across the search space\n",
        "3. **Choose** the next best point (the one with maximum EI)\n",
        "4. **Evaluate** the function at the new point (compute loss at chosen point)\n",
        "5. **Retrain** GPR with the new point and its loss value added to the training data\n",
        "6. **Repeat** for a specified number of iterations\n",
        "\n",
        "This is the code that implements the loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def bayesian_optimization(n_iter=10):\n",
        "    # Initial data\n",
        "    X_sample = np.array([[1.0], [2.5], [4.0]])\n",
        "    y_sample = black_box_function(X_sample)\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        gpr.fit(X_sample, y_sample)\n",
        "        ei = expected_improvement(X, X_sample, y_sample, gpr)\n",
        "        x_next = X[np.argmax(ei)].reshape(-1, 1)\n",
        "\n",
        "        # Evaluate the function at the new point\n",
        "        y_next = black_box_function(x_next)\n",
        "\n",
        "        # Add the new sample to our dataset\n",
        "        X_sample = np.vstack((X_sample, x_next))\n",
        "        y_sample = np.append(y_sample, y_next)\n",
        "    return X_sample, y_sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And you can run it with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_opt, y_opt = bayesian_optimization(n_iter=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Visualization {#final-visualization}\n",
        "\n",
        "We can see that with just 10 points (+3 initial samples), we have found the minimum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot final sampled points\n",
        "plt.plot(X, black_box_function(X), 'k--', label=\"True function\")\n",
        "plt.scatter(X_opt, y_opt, c='red', label=\"Sampled Points\")\n",
        "plt.title(\"Bayesian Optimization with Gaussian Process\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](https://codecut.ai/wp-content/uploads/2025/06/image-3.png)\n",
        "\n",
        "Et voilà. The minimum of your hyperparameter tuning process is found in only 10+3 steps.\n",
        "\n",
        "This saves you from having to run hundreds of training sessions across different values of `x`, which is exactly what brute-force methods like grid search would require. In contrast, by using GPR, you only sample in the areas where you expect to find the minimum of your black-box function."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}