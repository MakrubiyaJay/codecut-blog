{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What You Will Learn\n",
        "\n",
        "- Convert scanned receipts to structured data with [LlamaParse](https://github.com/run-llama/llama_parse) and [Pydantic](https://github.com/pydantic/pydantic) models\n",
        "- Validate extraction accuracy by comparing results against ground truth annotations\n",
        "- Fix parsing errors by preprocessing low-quality images\n",
        "- Export clean receipt data to spreadsheet format\n",
        "\n",
        "## Basic Image Processing with LlamaParse\n",
        "\n",
        "In this tutorial, we will use the [SROIE Dataset v2](https://www.kaggle.com/datasets/urbikn/sroie-datasetv2) from Kaggle. This dataset contains real-world receipt scans from the ICDAR 2019 competition.\n",
        "\n",
        "You can download the dataset directly from Kaggle's website or use the Kaggle CLI:\n",
        "\n",
        "```bash\n",
        "# Install the Kaggle CLI once\n",
        "uv pip install kaggle\n",
        "\n",
        "# Configure Kaggle credentials (run once per environment)\n",
        "export KAGGLE_USERNAME=your_username\n",
        "export KAGGLE_KEY=your_api_key\n",
        "\n",
        "# Create a workspace folder and download the full archive (~1 GB)\n",
        "mkdir -p data\n",
        "kaggle datasets download urbikn/sroie-datasetv2 -p data\n",
        "\n",
        "# Extract everything and inspect a few image files\n",
        "unzip -q -o data/sroie-datasetv2.zip -d data\n",
        "```\n",
        "\n",
        "This tutorial uses data from the `data/SROIE2019/train/` directory, which contains:\n",
        "\n",
        "- `img`: Original receipt images\n",
        "- `entities`: Ground truth annotations for validation\n",
        "\n",
        "Load the first 10 receipts into a list of paths:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "\n",
        "receipt_dir = Path(\"data/SROIE2019/train/img\")\n",
        "num_receipts = 10\n",
        "receipt_paths = sorted(receipt_dir.glob(\"*.jpg\"))[:num_receipts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Take a look at the first receipt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import Image\n",
        "\n",
        "first_receipt_path = receipt_paths[0]\n",
        "Image(filename=first_receipt_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![First receipt](https://codecut.ai/wp-content/uploads/2025/10/X00016469612.jpg)\n",
        "\n",
        "\n",
        "Next, use `LlamaParse` to convert the first receipt into markdown."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from llama_parse import LlamaParse\n",
        "\n",
        "\n",
        "# Parse receipts with LlamaParse\n",
        "parser = LlamaParse(\n",
        "    api_key=os.environ[\"LLAMA_CLOUD_API_KEY\"],\n",
        "    result_type=\"markdown\",  # Output format\n",
        "    num_workers=4,  # Number of parallel workers for faster processing\n",
        "    language=\"en\",  # Language hint for OCR accuracy\n",
        "    skip_diagonal_text=True,  # Ignore rotated or diagonal text\n",
        ")\n",
        "first_receipt = parser.load_data(first_receipt_path)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preview the markdown for the first receipt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preview the first receipt\n",
        "preview = \"\\n\".join(first_receipt.text.splitlines()[:10])\n",
        "print(preview)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "tan woon yann\n",
        "BOOK TA K (TAMAN DAYA) SDN BHD\n",
        "789417-W\n",
        "NO.5: 55,57 & 59, JALAN SAGU 18,\n",
        "TAMAN DaYA,\n",
        "81100 JOHOR BAHRU,\n",
        "JOHOR.\n",
        "```\n",
        "\n",
        "LlamaParse successfully converts receipt images to text, but there is no structure: vendor names, dates, and totals are all mixed together in plain text. This format is not ideal for exporting to spreadsheets or analytics tools for further analysis.\n",
        "\n",
        "The next section uses Pydantic models to extract structured fields like `company`, `total`, and `purchase_date` automatically.\n",
        "\n",
        "## Structured Data Extraction with Pydantic\n",
        "\n",
        "[Pydantic](https://docs.pydantic.dev/) is a Python library that uses type hints for data validation and automatic type conversion. By defining a receipt schema once, you can extract consistent structured data from receipts regardless of their format or layout.\n",
        "\n",
        "Start by defining two Pydantic models that represent receipt structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datetime import date\n",
        "from typing import List, Optional\n",
        "from pydantic import BaseModel, Field, ValidationInfo, model_validator\n",
        "\n",
        "\n",
        "class ReceiptItem(BaseModel):\n",
        "    \"\"\"Represents a single line item extracted from a receipt.\"\"\"\n",
        "\n",
        "    description: str = Field(description=\"Item name exactly as shown on the receipt\")\n",
        "    quantity: int = Field(default=1, ge=1, description=\"Integer quantity of the item\")\n",
        "    unit_price: Optional[float] = Field(\n",
        "        default=None, ge=0, description=\"Price per unit in the receipt currency\"\n",
        "    )\n",
        "    discount_amount: float = Field(\n",
        "        default=0.0, ge=0, description=\"Discount applied to this line item\"\n",
        "    )\n",
        "\n",
        "\n",
        "class Receipt(BaseModel):\n",
        "    \"\"\"Structured fields extracted from a retail receipt.\"\"\"\n",
        "\n",
        "    company: str = Field(description=\"Business or merchant name\")\n",
        "    purchase_date: Optional[date] = Field(\n",
        "        default=None, description=\"Date in YYYY-MM-DD format\"\n",
        "    )\n",
        "    address: Optional[str] = Field(default=None, description=\"Address of the business\")\n",
        "    total: float = Field(description=\"Final charged amount\")\n",
        "    items: List[ReceiptItem] = Field(default_factory=list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create an `OpenAIPydanticProgram` that instructs the LLM to extract data according to our `Receipt` model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from llama_index.program.openai import OpenAIPydanticProgram\n",
        "\n",
        "prompt = \"\"\"\n",
        "You are extracting structured data from a receipt.\n",
        "Use the provided text to populate the Receipt model.\n",
        "Interpret every receipt date as day-first.\n",
        "If a field is missing, return null.\n",
        "\n",
        "{context_str}\n",
        "\"\"\"\n",
        "\n",
        "receipt_program = OpenAIPydanticProgram.from_defaults(\n",
        "    output_cls=Receipt,\n",
        "    llm=Settings.llm,\n",
        "    prompt_template_str=prompt,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Process the first parsed document to make sure everything works before scaling to the full batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Process the first receipt\n",
        "structured_first_receipt = receipt_program(context_str=first_receipt.text)\n",
        "\n",
        "# Print the receipt as a JSON string for better readability\n",
        "print(structured_first_receipt.model_dump_json(indent=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"company\": \"tan woon yann BOOK TA K (TAMAN DAYA) SDN BHD\",\n",
        "  \"purchase_date\": \"2018-12-25\",\n",
        "  \"address\": \"NO.5: 55,57 & 59, JALAN SAGU 18, TAMAN DaYA, 81100 JOHOR BAHRU, JOHOR.\",\n",
        "  \"total\": 9.0,\n",
        "  \"items\": [\n",
        "    {\n",
        "      \"description\": \"KF MODELLING CLAY KIDDY FISH\",\n",
        "      \"quantity\": 1,\n",
        "      \"unit_price\": 9.0,\n",
        "      \"discount_amount\": 0.0\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "LlamaIndex populates the Pydantic schema with extracted values:\n",
        "\n",
        "- `company`: Vendor name from the receipt header\n",
        "- `purchase_date`: Parsed date (2018-12-25)\n",
        "- `total`: Final amount (9.0)\n",
        "- `items`: Line items with description, quantity, and price\n",
        "\n",
        "Now that the extraction works, let's scale it to process all receipts in a batch. The function uses each receipt's filename as a unique identifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def extract_documents(paths: List[str], prompt: str, id_column: str = \"receipt_id\") -> List[dict]:\n",
        "    \"\"\"Extract structured data from documents using LlamaParse and LLM.\"\"\"\n",
        "    results: List[dict] = []\n",
        "\n",
        "    # Initialize parser with OCR settings\n",
        "    parser = LlamaParse(\n",
        "        api_key=os.environ[\"LLAMA_CLOUD_API_KEY\"],\n",
        "        result_type=\"markdown\",\n",
        "        num_workers=4,\n",
        "        language=\"en\",\n",
        "        skip_diagonal_text=True,\n",
        "    )\n",
        "\n",
        "    # Convert images to markdown text\n",
        "    documents = parser.load_data(paths)\n",
        "\n",
        "    # Create structured extraction program\n",
        "    program = OpenAIPydanticProgram.from_defaults(\n",
        "        output_cls=Receipt,\n",
        "        llm=Settings.llm,\n",
        "        prompt_template_str=prompt,\n",
        "    )\n",
        "\n",
        "    # Extract structured data from each document\n",
        "    for path, doc in zip(paths, documents):\n",
        "        document_id = Path(path).stem\n",
        "        parsed_document = program(context_str=doc.text)\n",
        "        results.append(\n",
        "            {\n",
        "                id_column: document_id,\n",
        "                \"data\": parsed_document,\n",
        "            }\n",
        "        )\n",
        "    return results\n",
        "\n",
        "# Extract structured data from all receipts\n",
        "structured_receipts = extract_documents(receipt_paths, prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert the extracted receipts into a DataFrame for easier inspection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def transform_receipt_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Apply standard transformations to receipt DataFrame columns.\"\"\"\n",
        "    df = df.copy()\n",
        "    df[\"company\"] = df[\"company\"].str.upper()\n",
        "    df[\"total\"] = pd.to_numeric(df[\"total\"], errors=\"coerce\")\n",
        "    df[\"purchase_date\"] = pd.to_datetime(\n",
        "        df[\"purchase_date\"], errors=\"coerce\", dayfirst=True\n",
        "    ).dt.date\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_extracted_df(records: List[dict], id_column: str = \"receipt_id\") -> pd.DataFrame:\n",
        "    df = pd.DataFrame(\n",
        "        [\n",
        "            {\n",
        "                id_column: record[id_column],\n",
        "                \"company\": record[\"data\"].company,\n",
        "                \"total\": record[\"data\"].total,\n",
        "                \"purchase_date\": record[\"data\"].purchase_date,\n",
        "            }\n",
        "            for record in records\n",
        "        ]\n",
        "    )\n",
        "    return transform_receipt_columns(df)\n",
        "\n",
        "\n",
        "extracted_df = create_extracted_df(structured_receipts)\n",
        "extracted_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|    | receipt_id   | company                                         |   total | purchase_date   |\n",
        "|---:|:-------------|:------------------------------------------------|--------:|:----------------|\n",
        "|  0 | X00016469612 | TAN WOON YANN BOOK TA K (TAMAN DAYA) SDN BHD    |     9   | 2018-12-25      |\n",
        "|  1 | X00016469619 | INDAH GIFT & HOME DECO                          |    60.3 | 2018-10-19      |\n",
        "|  2 | X00016469620 | MR D.I.Y. (JOHOR) SDN BHD                       |    33.9 | 2019-01-12      |\n",
        "|  3 | X00016469622 | YONGFATT ENTERPRISE                             |   80.9 | 2018-12-25      |\n",
        "|  4 | X00016469623 | MR D.I.Y. (M) SDN BHD                           |    30.9 | 2018-11-18      |\n",
        "|  5 | X00016469669 | ABC HO TRADING                                  |    31   | 2019-01-09      |\n",
        "|  6 | X00016469672 | SOON HUAT MACHINERY ENTERPRISE                  |   327   | 2019-01-11      |\n",
        "|  7 | X00016469676 | S.H.H. MOTOR (SUNGAI RENGIT SN. BHD. (801580-T) |    20   | 2019-01-23      |\n",
        "|  8 | X51005200938 | TH MNAN                                         |     0   | 2023-10-11      |\n",
        "|  9 | X51005230617 | GERBANG ALAF RESTAURANTS SDN BHD                |    26.6 | 2018-01-18      |\n",
        "\n",
        "Most receipts are extracted correctly, but receipt X51005200938 shows issues:\n",
        "\n",
        "- The company name is incomplete (\"TH MNAN\")\n",
        "- Total is 0 instead of the actual amount\n",
        "- Date (2023-10-11) appears incorrect\n",
        "\n",
        "## Compare Extraction with Ground Truth\n",
        "\n",
        "To verify the extraction accuracy, load the ground-truth annotations from `data/SROIE2019/train/entities`:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def normalize_date(value: str) -> str:\n",
        "    \"\"\"Normalize date strings to consistent format.\"\"\"\n",
        "    value = (value or \"\").strip()\n",
        "    if not value:\n",
        "        return value\n",
        "    # Convert hyphens to slashes\n",
        "    value = value.replace(\"-\", \"/\")\n",
        "    parts = value.split(\"/\")\n",
        "    # Convert 2-digit years to 4-digit (e.g., 18 -> 2018)\n",
        "    if len(parts[-1]) == 2:\n",
        "        parts[-1] = f\"20{parts[-1]}\"\n",
        "    return \"/\".join(parts)\n",
        "\n",
        "\n",
        "def create_ground_truth_df(\n",
        "    label_paths: List[str], id_column: str = \"receipt_id\"\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Create ground truth DataFrame from label JSON files.\"\"\"\n",
        "    records = []\n",
        "    # Load each JSON file and extract key fields\n",
        "    for path in label_paths:\n",
        "        payload = pd.read_json(Path(path), typ=\"series\").to_dict()\n",
        "        records.append(\n",
        "            {\n",
        "                id_column: Path(path).stem,\n",
        "                \"company\": payload.get(\"company\"),\n",
        "                \"total\": payload.get(\"total\"),\n",
        "                \"purchase_date\": normalize_date(payload.get(\"date\")),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "    # Apply same transformations as extracted data\n",
        "    return transform_receipt_columns(df)\n",
        "\n",
        "\n",
        "# Load ground truth annotations\n",
        "label_dir = Path(\"data/SROIE2019/train/entities\")\n",
        "label_paths = sorted(label_dir.glob(\"*.txt\"))[:num_receipts]\n",
        "\n",
        "ground_truth_df = create_ground_truth_df(label_paths)\n",
        "ground_truth_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|    | receipt_id   | company                                |   total | purchase_date   |\n",
        "|---:|:-------------|:---------------------------------------|--------:|:----------------|\n",
        "|  0 | X00016469612 | BOOK TA .K (TAMAN DAYA) SDN BHD        |    9    | 2018-12-25      |\n",
        "|  1 | X00016469619 | INDAH GIFT & HOME DECO                 |   60.3  | 2018-10-19      |\n",
        "|  2 | X00016469620 | MR D.I.Y. (JOHOR) SDN BHD              |   33.9  | 2019-01-12      |\n",
        "|  3 | X00016469622 | YONGFATT ENTERPRISE                    |   80.9  | 2018-12-25      |\n",
        "|  4 | X00016469623 | MR D.I.Y. (M) SDN BHD                  |   30.9  | 2018-11-18      |\n",
        "|  5 | X00016469669 | ABC HO TRADING                         |   31    | 2019-01-09      |\n",
        "|  6 | X00016469672 | SOON HUAT MACHINERY ENTERPRISE         |  327    | 2019-01-11      |\n",
        "|  7 | X00016469676 | S.H.H. MOTOR (SUNGAI RENGIT) SDN. BHD. |   20    | 2019-01-23      |\n",
        "|  8 | X51005200938 | PERNIAGAAN ZHENG HUI                   |  112.45 | 2018-02-12      |\n",
        "|  9 | X51005230617 | GERBANG ALAF RESTAURANTS SDN BHD       |   26.6  | 2018-01-18      |\n",
        "\n",
        "Let's validate extraction accuracy by comparing results against ground truth.\n",
        "\n",
        "Company names often have minor variations (spacing, punctuation, extra characters), so we'll use [fuzzy matching](https://codecut.ai/text-similarity-fuzzy-matching-guide/) to tolerate these formatting differences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from rapidfuzz import fuzz\n",
        "\n",
        "\n",
        "def fuzzy_match_score(text1: str, text2: str) -> int:\n",
        "    \"\"\"Calculate fuzzy match score between two strings.\"\"\"\n",
        "    return fuzz.token_set_ratio(str(text1), str(text2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test the fuzzy matching with sample company names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Nearly identical strings score high\n",
        "print(f\"Score: {fuzzy_match_score('BOOK TA K SDN BHD', 'BOOK TA .K SDN BHD'):.2f}\")\n",
        "\n",
        "# Different punctuation still matches well\n",
        "print(f\"Score: {fuzzy_match_score('MR D.I.Y. JOHOR', 'MR DIY JOHOR'):.2f}\")\n",
        "\n",
        "# Completely different strings score low\n",
        "print(f\"Score: {fuzzy_match_score('ABC TRADING', 'XYZ COMPANY'):.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Score: 97.14\n",
        "Score: 55.17\n",
        "Score: 27.27\n",
        "```\n",
        "\n",
        "Now build a comparison function that merges extracted and ground truth data, then applies fuzzy matching for company names and exact matching for numeric fields:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compare_receipts(\n",
        "    extracted_df: pd.DataFrame,\n",
        "    ground_truth_df: pd.DataFrame,\n",
        "    id_column: str,\n",
        "    fuzzy_match_cols: List[str],\n",
        "    exact_match_cols: List[str],\n",
        "    fuzzy_threshold: int = 80,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Compare extracted and ground truth data with explicit column specifications.\"\"\"\n",
        "    comparison_df = extracted_df.merge(\n",
        "        ground_truth_df,\n",
        "        on=id_column,\n",
        "        how=\"inner\",\n",
        "        suffixes=(\"_extracted\", \"_truth\"),\n",
        "    )\n",
        "\n",
        "    # Fuzzy matching\n",
        "    for col in fuzzy_match_cols:\n",
        "        extracted_col = f\"{col}_extracted\"\n",
        "        truth_col = f\"{col}_truth\"\n",
        "        comparison_df[f\"{col}_score\"] = comparison_df.apply(\n",
        "            lambda row: fuzzy_match_score(row[extracted_col], row[truth_col]),\n",
        "            axis=1,\n",
        "        )\n",
        "        comparison_df[f\"{col}_match\"] = comparison_df[f\"{col}_score\"] >= fuzzy_threshold\n",
        "\n",
        "    # Exact matching\n",
        "    for col in exact_match_cols:\n",
        "        extracted_col = f\"{col}_extracted\"\n",
        "        truth_col = f\"{col}_truth\"\n",
        "        comparison_df[f\"{col}_match\"] = (\n",
        "            comparison_df[extracted_col] == comparison_df[truth_col]\n",
        "        )\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "\n",
        "comparison_df = compare_receipts(\n",
        "    extracted_df,\n",
        "    ground_truth_df,\n",
        "    id_column=\"receipt_id\",\n",
        "    fuzzy_match_cols=[\"company\"],\n",
        "    exact_match_cols=[\"total\", \"purchase_date\"],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect any rows where the company, total, or purchase-date checks fail:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_mismatch_rows(comparison_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Get mismatched rows, excluding match indicator columns.\"\"\"\n",
        "    # Extract match columns and data columns\n",
        "    match_columns = [col for col in comparison_df.columns if col.endswith(\"_match\")]\n",
        "    data_columns = sorted([col for col in comparison_df.columns if col.endswith(\"_extracted\") or col.endswith(\"_truth\")])\n",
        "\n",
        "    # Check for rows where not all matches are True\n",
        "    has_mismatch = comparison_df[match_columns].all(axis=1).eq(False)\n",
        "\n",
        "    return comparison_df[has_mismatch][data_columns]\n",
        "\n",
        "\n",
        "mismatch_df = get_mismatch_rows(comparison_df)\n",
        "\n",
        "\n",
        "mismatch_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|    | company_extracted   | company_truth        | purchase_date_extracted   | purchase_date_truth   |   total_extracted |   total_truth |\n",
        "|---:|:--------------------|:---------------------|:--------------------------|:----------------------|------------------:|--------------:|\n",
        "|  8 | TH MNAN             | PERNIAGAAN ZHENG HUI | 2023-10-11                | 2018-02-12            |                 0 |        112.45 |\n",
        "\n",
        "\n",
        "This confirms what we saw earlier. All receipts match the ground truth annotations except for receipt ID X51005200938 for the following fields:\n",
        "\n",
        "- Company name\n",
        "- Total\n",
        "- Purchase date\n",
        "\n",
        "Let's take a closer look at this receipt to see if we can identify the issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import IPython.display as display\n",
        "\n",
        "file_to_inspect = receipt_dir / \"X51005200938.jpg\"\n",
        "\n",
        "display.Image(filename=file_to_inspect)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Problematic receipt](https://codecut.ai/wp-content/uploads/2025/10/X51005200938.jpg)\n",
        "\n",
        "This receipt appears smaller than the others in the dataset, which may affect OCR readability. In the next section, we will scale up the receipt to improve the extraction.\n",
        "\n",
        "\n",
        "## Process the Images for Better Extraction\n",
        "\n",
        "Create a function to scale up the receipt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "def scale_image(image_path: Path, output_dir: Path, scale_factor: int = 3) -> Path:\n",
        "    \"\"\"Scale up an image using high-quality resampling.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the original image\n",
        "        output_dir: Directory to save the scaled image\n",
        "        scale_factor: Factor to scale up the image (default: 3x)\n",
        "\n",
        "    Returns:\n",
        "        Path to the scaled image\n",
        "    \"\"\"\n",
        "    # Load the image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # Scale up the image using high-quality resampling\n",
        "    new_size = (img.width * scale_factor, img.height * scale_factor)\n",
        "    img_resized = img.resize(new_size, Image.Resampling.LANCZOS)\n",
        "\n",
        "    # Save to output directory with same filename\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    output_path = output_dir / image_path.name\n",
        "    img_resized.save(output_path, quality=95)\n",
        "\n",
        "    return output_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply the function to the problematic receipt:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "problematic_receipt_path = receipt_dir / \"X51005200938.jpg\"\n",
        "adjusted_receipt_dir = Path(\"data/SROIE2019/train/img_adjusted\")\n",
        "\n",
        "scaled_image_path = scale_image(problematic_receipt_path, adjusted_receipt_dir, scale_factor=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's extract the structured data from the scaled image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "problematic_structured_receipts = extract_documents([scaled_image_path], prompt)\n",
        "problematic_extracted_df = create_extracted_df(problematic_structured_receipts)\n",
        "\n",
        "problematic_extracted_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|    | receipt_id   | company              |   total | purchase_date   |\n",
        "|---:|:-------------|:---------------------|--------:|:----------------|\n",
        "|  0 | X51005200938 | PERNIAGAAN ZHENG HUI |  112.46 | 2018-02-12      |\n",
        "\n",
        "Nice! Scaling fixes the extraction. Company name and purchase date are now accurate. The total is 112.46 vs 112.45, acceptable since 112.45 actually looks like 112.46 when printed on the receipt.\n",
        "\n",
        "## Export Clean Data to CSV or Excel\n",
        "\n",
        "Apply the scaling fix to all receipts. Copy the remaining images to the processed directory, excluding the already-scaled receipt:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil\n",
        "\n",
        "clean_receipt_paths = [scaled_image_path]\n",
        "# Copy all receipts except the already processed one\n",
        "for receipt_path in receipt_paths:\n",
        "    if receipt_path != problematic_receipt_path:  # Skip the already scaled image\n",
        "        output_path = adjusted_receipt_dir / receipt_path.name\n",
        "        shutil.copy2(receipt_path, output_path)\n",
        "        clean_receipt_paths.append(output_path)\n",
        "        print(f\"Copied {receipt_path.name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run the pipeline again with the processed images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clean_structured_receipts = extract_documents(clean_receipt_paths, prompt)\n",
        "clean_extracted_df = create_extracted_df(clean_structured_receipts)\n",
        "clean_extracted_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|    | receipt_id   | company                                         |   total | purchase_date   |\n",
        "|---:|:-------------|:------------------------------------------------|--------:|:----------------|\n",
        "|  0 | X51005200938 | PERNIAGAAN ZHENG HUI                            |  112.46 | 2018-02-12      |\n",
        "|  1 | X00016469612 | TAN WOON YANN                                   |    9    | 2018-12-25      |\n",
        "|  2 | X00016469619 | INDAH GIFT & HOME DECO                          |   60.3  | 2018-10-19      |\n",
        "|  3 | X00016469620 | MR D.I.Y. (JOHOR) SDN BHD                       |   33.9  | 2019-01-12      |\n",
        "|  4 | X00016469622 | YONGFATT ENTERPRISE                             |   80.9  | 2018-12-25      |\n",
        "|  5 | X00016469623 | MR D.I.Y. (M) SDN BHD                           |   30.9  | 2018-11-18      |\n",
        "|  6 | X00016469669 | ABC HO TRADING                                  |   31    | 2019-01-09      |\n",
        "|  7 | X00016469672 | SOON HUAT MACHINERY ENTERPRISE                  |  327    | 2019-01-11      |\n",
        "|  8 | X00016469676 | S.H.H. MOTOR (SUNGAI RENGIT SN. BHD. (801580-T) |   20    | 2019-01-23      |\n",
        "|  9 | X51005230617 | GERBANG ALAF RESTAURANTS SDN BHD                |   26.6  | 2018-01-18      |\n",
        "\n",
        "\n",
        "Awesome! All receipts now match the ground truth annotations.\n",
        "\n",
        "Now we can export the dataset to a spreadsheet with just a few lines of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Export to CSV\n",
        "output_path = Path(\"reports/receipts.csv\")\n",
        "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "clean_extracted_df.to_csv(output_path, index=False)\n",
        "print(f\"Exported {len(clean_extracted_df)} receipts to {output_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```text\n",
        "Exported 10 receipts to reports/receipts.csv\n",
        "```\n",
        "\n",
        "The exported data can now be imported into spreadsheet applications, analytics tools, or business intelligence platforms.\n",
        "\n",
        "## Speed Up Processing with Async Parallel Execution\n",
        "\n",
        "LlamaIndex supports asynchronous processing to handle multiple receipts concurrently. By using `async`/`await` with the `aget_nodes_from_documents()` method, you can process receipts in parallel instead of sequentially, significantly reducing total processing time.\n",
        "\n",
        "Here's how to modify the extraction function to use async processing. Setting `num_workers=10` means the parser will process up to 10 receipts concurrently:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import asyncio\n",
        "\n",
        "\n",
        "async def extract_documents_async(\n",
        "    paths: List[str], prompt: str, id_column: str = \"receipt_id\"\n",
        ") -> List[dict]:\n",
        "    \"\"\"Extract structured data from documents using async LlamaParse.\"\"\"\n",
        "    results: List[dict] = []\n",
        "\n",
        "    parser = LlamaParse(\n",
        "        api_key=os.environ[\"LLAMA_CLOUD_API_KEY\"],\n",
        "        result_type=\"markdown\",\n",
        "        num_workers=10,  # Process 10 receipts concurrently\n",
        "        language=\"en\",\n",
        "        skip_diagonal_text=True,\n",
        "    )\n",
        "\n",
        "    # Use async method for parallel processing\n",
        "    documents = await parser.aload_data(paths)\n",
        "\n",
        "    program = OpenAIPydanticProgram.from_defaults(\n",
        "        output_cls=Receipt,\n",
        "        llm=Settings.llm,\n",
        "        prompt_template_str=prompt,\n",
        "    )\n",
        "\n",
        "    for path, doc in zip(paths, documents):\n",
        "        document_id = Path(path).stem\n",
        "        parsed_document = program(context_str=doc.text)\n",
        "        results.append({id_column: document_id, \"data\": parsed_document})\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# Run with asyncio\n",
        "structured_receipts = await extract_documents_async(receipt_paths, prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See the [LlamaIndex async documentation](https://docs.llamaindex.ai/en/stable/getting_started/async_python/) for more details.\n",
        "\n",
        "## Try It Yourself\n",
        "\n",
        "The concepts from this tutorial are available as a reusable pipeline in [this GitHub repository](https://github.com/CodeCutTech/Data-science/tree/master/llm/smart_data_extraction_llamaindex). The code includes both synchronous and asynchronous versions:\n",
        "\n",
        "**Synchronous pipelines** (simple, sequential processing):\n",
        "\n",
        "- **Generic pipeline** ([`document_extraction_pipeline.py`](https://github.com/CodeCutTech/Data-science/blob/master/llm/smart_data_extraction_llamaindex/document_extraction_pipeline.py)): Reusable extraction function that works with any Pydantic schema\n",
        "- **Receipt pipeline** ([`extract_receipts_pipeline.py`](https://github.com/CodeCutTech/Data-science/blob/master/llm/smart_data_extraction_llamaindex/extract_receipts_pipeline.py)): Complete example with Receipt schema, image scaling, and data transformations\n",
        "\n",
        "**Asynchronous pipelines** (parallel processing with 3-10x speedup):\n",
        "\n",
        "- **Async generic pipeline** ([`async_document_extraction_pipeline.py`](https://github.com/CodeCutTech/Data-science/blob/master/llm/smart_data_extraction_llamaindex/async_document_extraction_pipeline.py)): Concurrent document processing\n",
        "- **Async receipt pipeline** ([`async_extract_receipts_pipeline.py`](https://github.com/CodeCutTech/Data-science/blob/master/llm/smart_data_extraction_llamaindex/async_extract_receipts_pipeline.py)): Batch receipt processing with progress tracking\n",
        "\n",
        "Run the receipt extraction example:\n",
        "\n",
        "```bash\n",
        "# Synchronous version (simple, sequential)\n",
        "uv run extract_receipts_pipeline.py\n",
        "\n",
        "# Asynchronous version (parallel processing, 3-10x faster)\n",
        "uv run async_extract_receipts_pipeline.py\n",
        "```\n",
        "\n",
        "Or create your own extractor by importing `extract_structured_data()` and providing your custom Pydantic schema, extraction prompt, and optional preprocessing functions.\n",
        "\n",
        "> Learn production-ready practices for data science and AI projects in [Production-Ready Data Science](https://codecut.ai/production-ready-data-science/)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}