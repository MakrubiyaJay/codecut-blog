{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Motivation {#motivation}\n",
        "\n",
        "> ðŸ“– Read the full article: [Pytest for Data Scientists](https://codecut.ai/pytest-for-data-scientists/)\n",
        "\n",
        "\n",
        "As a data scientist, one way to test your Python code is by using an interactive notebook to verify the accuracy of the outputs.\n",
        "\n",
        "However, this approach does not guarantee that your code works as intended in all cases.\n",
        "\n",
        "A better approach is to identify the expected behavior of the code in various scenarios, and then verify if the code executes accordingly.\n",
        "\n",
        "For example, testing a function used to extract the sentiment of a text might include checking whether:\n",
        "\n",
        "- The function returns a value that is greater than 0 if the test is positive.\n",
        "- The function returns a value that is less than 0 if the text is negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#sentiment.py\n",
        "\n",
        "def test_extract_sentiment_positive():\n",
        "\n",
        "    text = \"I think today will be a great day\"\n",
        "\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment > 0\n",
        "\n",
        "def test_extract_sentiment_negative():\n",
        "\n",
        "    text = \"I do not think this will turn out well\"\n",
        "\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment < 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides ensuring that your code works as intended, incorporating testing in a data science project also provides the following benefits:\n",
        "\n",
        "- Identifies edge cases.\n",
        "- Enables safe replacement of existing code with enhanced versions, without risking disruption of the entire process.\n",
        "- Makes it easier for your teammates to understand the behaviors of your functions.\n",
        "\n",
        "While Python offers various testing tools, Pytest is the most user-friendly option.\n",
        "\n",
        "## Get Started with Pytest {#get-started-with-pytest}\n",
        "\n",
        "[Pytest](https://docs.pytest.org/en/stable/) is the framework that makes it easy to write small tests in Python. I like pytest because it helps me to write tests with minimal code. If you are not familiar with testing, pytest is a great tool to get started.\n",
        "\n",
        "To install pytest, run\n",
        "\n",
        "```{bash}\n",
        "pip install -U pytest\n",
        "```\n",
        "\n",
        "To test the `extract_sentiment` function, create a function that starts with `test_` followed by the name of the tested function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#sentiment.py\n",
        "def extract_sentiment(text: str):\n",
        "        '''Extract sentiment using textblob.\n",
        "        Polarity is within range [-1, 1]'''\n",
        "\n",
        "        text = TextBlob(text)\n",
        "\n",
        "        return text.sentiment.polarity\n",
        "\n",
        "def test_extract_sentiment():\n",
        "\n",
        "    text = \"I think today will be a great day\"\n",
        "\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment > 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That's it! Now we are ready to run the test.\n",
        "\n",
        "To test the `sentiment.py` file, run:\n",
        "\n",
        "```{bash}\n",
        "pytest sentiment.py\n",
        "```\n",
        "\n",
        "Pytest will run all functions that start with `test` in the current working directory. The output of the test above will look like this:\n",
        "\n",
        "```bash\n",
        "========================================= test session starts ==========================================\n",
        "process.py .                                                                                     [100%]\n",
        "========================================= 1 passed in 0.68s ===========================================\n",
        "```\n",
        "\n",
        "If the test fails, pytest will produce the following outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#sentiment.py\n",
        "\n",
        "def test_extract_sentiment():\n",
        "\n",
        "    text = \"I think today will be a great day\"\n",
        "\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment < 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```bash\n",
        "$ pytest sentiment.py\n",
        "========================================= test session starts ==========================================\n",
        "process.py F                                                                                     [100%]\n",
        "=============================================== FAILURES ===============================================\n",
        "________________________________________ test_extract_sentiment ________________________________________\n",
        "def test_extract_sentiment():\n",
        "\n",
        "        text = \"I think today will be a great day\"\n",
        "\n",
        "        sentiment = extract_sentiment(text)\n",
        "\n",
        ">       assert sentiment < 0\n",
        "E       assert 0.8 < 0\n",
        "process.py:17: AssertionError\n",
        "======================================= short test summary info ========================================\n",
        "FAILED process.py::test_extract_sentiment - assert 0.8 < 0\n",
        "========================================== 1 failed in 0.84s ===========================================\n",
        "```\n",
        "\n",
        "The test failed because the sentiment of the function is 0.8, which is not less than 0. Knowing why the function doesn't work gives us directions on how to fix it.\n",
        "\n",
        "\n",
        "\n",
        "## Multiple Tests for the Same Function {#multiple-tests-for-the-same-function}\n",
        "\n",
        "With pytest, we can also create multiple tests for the same function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#sentiment.py\n",
        "\n",
        "def test_extract_sentiment_positive():\n",
        "\n",
        "    text = \"I think today will be a great day\"\n",
        "\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment > 0\n",
        "\n",
        "def test_extract_sentiment_negative():\n",
        "\n",
        "    text = \"I do not think this will turn out well\"\n",
        "\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment < 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```bash\n",
        "$ pytest sentiment.py\n",
        "========================================= test session starts ==========================================\n",
        "process.py .F                                                                                    [100%]\n",
        "=============================================== FAILURES ===============================================\n",
        "___________________________________ test_extract_sentiment_negative ____________________________________\n",
        "def test_extract_sentiment_negative():\n",
        "\n",
        "        text = \"I do not think this will turn out well\"\n",
        "\n",
        "        sentiment = extract_sentiment(text)\n",
        "\n",
        ">       assert sentiment < 0\n",
        "E       assert 0.0 < 0\n",
        "process.py:25: AssertionError\n",
        "======================================= short test summary info ========================================\n",
        "FAILED process.py::test_extract_sentiment_negative - assert 0.0 < 0\n",
        "===================================== 1 failed, 1 passed in 0.80s ======================================\n",
        "```\n",
        "\n",
        "## Parametrization: Combining Tests {#parametrization-combining-tests}\n",
        "\n",
        "Since the two test functions mentioned earlier test the same function, we can combine them into one test function with parameterization.\n",
        "\n",
        "### Parametrize with a List of Samples {#parametrize-with-a-list-of-samples}\n",
        "\n",
        "`pytest.mark.parametrize()` allows us to execute a test with different examples by providing a list of examples in the argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# sentiment.py\n",
        "\n",
        "import pytest\n",
        "from textblob import TextBlob\n",
        "\n",
        "def extract_sentiment(text: str):\n",
        "        '''Extract sentiment using textblob.\n",
        "        Polarity is within range [-1, 1]'''\n",
        "\n",
        "        text = TextBlob(text)\n",
        "\n",
        "        return text.sentiment.polarity\n",
        "\n",
        "testdata = [\"I think today will be a great day\", \"I do not think this will turn out well\"]\n",
        "\n",
        "@pytest.mark.parametrize(\"sample\", testdata)\n",
        "def test_extract_sentiment(sample):\n",
        "\n",
        "    sentiment = extract_sentiment(sample)\n",
        "\n",
        "    assert sentiment > 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```bash\n",
        "========================== test session starts ===========================\n",
        "platform linux -- Python 3.8.3, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\n",
        "collected 2 items\n",
        "sentiment.py .F                                                    [100%]\n",
        "================================ FAILURES ================================\n",
        "_____ test_extract_sentiment[I do not think this will turn out well] _____\n",
        "sample = \"I do not think this will turn out well\"\n",
        "@pytest.mark.parametrize(\"sample\", testdata)\n",
        "    def test_extract_sentiment(sample):\n",
        "\n",
        "        sentiment = extract_sentiment(sample)\n",
        "\n",
        ">       assert sentiment > 0\n",
        "E       assert 0.0 > 0\n",
        "sentiment.py:19: AssertionError\n",
        "======================== short test summary info =========================\n",
        "FAILED sentiment.py::test_extract_sentiment[I do not think this will turn out well]\n",
        "====================== 1 failed, 1 passed in 0.80s ===================\n",
        "```\n",
        "\n",
        "### Parametrize with a List of Examples and Expected Outputs {#parametrize-with-a-list-of-examples-and-expected-outputs}\n",
        "\n",
        "What if we expect **different examples** to have **different outputs**?\n",
        "\n",
        "For example, we might want to check if the function `text_contain_word`:\n",
        "\n",
        "- Returns `True` if `word=\"duck\"` and `text=\"There is a duck in this text\"`\n",
        "- Returns `False` if `word=\"duck\"` and `text=\"There is nothing here\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def text_contain_word(word: str, text: str):\n",
        "    '''Find whether the text contains a particular word'''\n",
        "\n",
        "    return word in text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To create a test for multiple examples with different expected outputs, we can use `parametrize(\"sample, expected_out\", testdata)` with `testdata=[(<sample1>, <output1>), (<sample2>, <output2>)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# process.py\n",
        "import pytest\n",
        "\n",
        "def text_contain_word(word: str, text: str):\n",
        "    '''Find whether the text contains a particular word'''\n",
        "\n",
        "    return word in text\n",
        "\n",
        "testdata = [\n",
        "    (\"There is a duck in this text\", True),\n",
        "    (\"There is nothing here\", False)\n",
        "    ]\n",
        "\n",
        "@pytest.mark.parametrize(\"sample, expected_output\", testdata)\n",
        "def test_text_contain_word(sample, expected_output):\n",
        "\n",
        "    word = \"duck\"\n",
        "\n",
        "    assert text_contain_word(word, sample) == expected_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```bash\n",
        "$ pytest process.py\n",
        "========================================= test session starts ==========================================\n",
        "platform linux -- Python 3.8.3, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\n",
        "plugins: hydra-core-1.0.0, Faker-4.1.1\n",
        "collected 2 items\n",
        "process.py ..                                                                                    [100%]\n",
        "========================================== 2 passed in 0.04s ===========================================\n",
        "```\n",
        "\n",
        "Awesome! Both tests passed!\n",
        "\n",
        "## Test One Function at a Time {#test-one-function-at-a-time}\n",
        "\n",
        "To test a specific function, run `pytest file.py::function_name`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "testdata = [\"I think today will be a great day\", \"I do not think this will turn out well\"]\n",
        "\n",
        "@pytest.mark.parametrize(\"sample\", testdata)\n",
        "def test_extract_sentiment(sample):\n",
        "\n",
        "    sentiment = extract_sentiment(sample)\n",
        "\n",
        "    assert sentiment > 0\n",
        "\n",
        "testdata = [\n",
        "    (\"There is a duck in this text\", True),\n",
        "    (\"There is nothing here\", False)\n",
        "    ]\n",
        "\n",
        "@pytest.mark.parametrize(\"sample, expected_output\", testdata)\n",
        "def test_text_contain_word(sample, expected_output):\n",
        "\n",
        "    word = \"duck\"\n",
        "\n",
        "    assert text_contain_word(word, sample) == expected_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example, to run only `test_text_contain_word`, type:\n",
        "\n",
        "```{bash}\n",
        "pytest process.py::test_text_contain_word\n",
        "```\n",
        "\n",
        "## Fixtures: Use the Same Data to Test Different Functions {#fixtures-use-the-same-data-to-test-different-functions}\n",
        "\n",
        "We can also use the same data to test different functions with pytest fixture.\n",
        "\n",
        "In the code below, we use pytest fixture to convert the sentence \"Today I found a duck and I am happy\" into a reusable fixture and use it in multiple tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@pytest.fixture\n",
        "def example_data():\n",
        "    return \"Today I found a duck and I am happy\"\n",
        "\n",
        "def test_extract_sentiment(example_data):\n",
        "\n",
        "    sentiment = extract_sentiment(example_data)\n",
        "\n",
        "    assert sentiment > 0\n",
        "\n",
        "def test_text_contain_word(example_data):\n",
        "\n",
        "    word = \"duck\"\n",
        "\n",
        "    assert text_contain_word(word, example_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Fixtures: Optimize Your Test Setup {#advanced-fixtures-optimize-your-test-setup}\n",
        "\n",
        "When working with data science projects, you often need to load expensive datasets or set up consistent environments across multiple tests. Basic fixtures work well for small examples, but advanced fixtures can optimize your test performance and ensure reproducibility.\n",
        "\n",
        "### Session-Scoped Fixtures\n",
        "\n",
        "Basic pytest fixtures reload data for every test function, which becomes inefficient with large datasets. Session-scoped fixtures solve this by loading the data once and reusing it across all tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytest\n",
        "\n",
        "# This fixture runs once per test session\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def large_dataset():\n",
        "    # Simulate loading an expensive dataset\n",
        "    print(\"Loading large dataset...\")\n",
        "    return pd.DataFrame({\n",
        "        \"feature1\": np.random.randn(10000),\n",
        "        \"feature2\": np.random.randn(10000),\n",
        "        \"target\": np.random.randint(0, 2, 10000)\n",
        "    })\n",
        "\n",
        "def test_data_shape(large_dataset):\n",
        "    assert large_dataset.shape == (10000, 3)\n",
        "\n",
        "def test_feature_types(large_dataset):\n",
        "    assert large_dataset[\"target\"].dtype == int\n",
        "    assert large_dataset[\"feature1\"].dtype == float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```\n",
        "Loading large dataset...\n",
        "test_session_fixture.py::test_data_shape PASSED\n",
        "test_session_fixture.py::test_feature_types PASSED\n",
        "```\n",
        "\n",
        "The dataset is loaded only once, even if you have multiple tests using it.\n",
        "\n",
        "### Autouse Fixtures\n",
        "\n",
        "Regular fixtures require explicit inclusion in each test function, which becomes repetitive for universal setup like random seeds. Autouse fixtures solve this by running automatically before every test, ensuring consistent setup across your entire test suite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pytest\n",
        "\n",
        "@pytest.fixture(autouse=True)\n",
        "def setup_random_seeds():\n",
        "    print(\"Setting up random seeds...\")\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "def test_model_prediction():\n",
        "    # This test will have reproducible random results\n",
        "    X = np.random.randn(100, 5)\n",
        "    # Your model training and prediction code here\n",
        "    assert len(X) == 100\n",
        "\n",
        "def test_data_sampling():\n",
        "    # This test also gets reproducible randomness\n",
        "    sample = np.random.choice([1, 2, 3, 4, 5], size=10)\n",
        "    assert len(sample) == 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```\n",
        "Setting up random seeds...\n",
        "test_autouse_fixture.py::test_model_prediction PASSED\n",
        "Setting up random seeds...\n",
        "test_autouse_fixture.py::test_data_sampling PASSED\n",
        "```\n",
        "\n",
        "You can see the fixture runs twice automatically - once before each test - even though neither test function explicitly requests the fixture.\n",
        "\n",
        "## Test with Temporary Files Safely {#test-with-temporary-files-safely}\n",
        "\n",
        "Testing file operations with real files can corrupt your actual data or leave behind test artifacts. Safe file testing requires temporary files that don't interfere with your actual data and are automatically cleaned up after tests.\n",
        "\n",
        "Pytest provides the `tmp_path` fixture that creates a temporary directory for each test. This is perfect for testing data processing pipelines, model serialization, or any file I/O operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def save_model_predictions(predictions, filepath):\n",
        "    \"\"\"Save model predictions to a CSV file\"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    pd.DataFrame({\"predictions\": predictions}).to_csv(filepath, index=False)\n",
        "\n",
        "\n",
        "def load_model_predictions(filepath):\n",
        "    \"\"\"Load model predictions from a CSV file\"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    return pd.read_csv(filepath)[\"predictions\"].tolist()\n",
        "\n",
        "\n",
        "def test_save_and_load_predictions(tmp_path):\n",
        "    # tmp_path is automatically created and cleaned up\n",
        "    predictions = [0.1, 0.9, 0.3, 0.7]\n",
        "\n",
        "    # Create a temporary file path\n",
        "    file_path = tmp_path / \"predictions.csv\"\n",
        "\n",
        "    # Test saving\n",
        "    save_model_predictions(predictions, file_path)\n",
        "    assert file_path.exists()\n",
        "\n",
        "    # Test loading\n",
        "    loaded_predictions = load_model_predictions(file_path)\n",
        "    assert loaded_predictions == predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also test entire data processing pipelines:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def test_data_processing_pipeline(tmp_path):\n",
        "    # Create temporary input file\n",
        "    input_file = tmp_path / \"input.csv\"\n",
        "    input_data = pd.DataFrame({\"value\": [1, 2, 3, 4, 5]})\n",
        "    input_data.to_csv(input_file, index=False)\n",
        "\n",
        "    # Create temporary output file path\n",
        "    output_file = tmp_path / \"processed.csv\"\n",
        "\n",
        "    # Test your processing function\n",
        "    process_data(input_file, output_file)\n",
        "\n",
        "    # Verify the output\n",
        "    result = pd.read_csv(output_file)\n",
        "    assert len(result) == 5\n",
        "    # Add more specific assertions about your processing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test NumPy Arrays and DataFrames Properly {#test-numpy-arrays-and-dataframes-properly}\n",
        "\n",
        "In data science, you frequently work with floating-point numbers, NumPy arrays, and pandas DataFrames. Regular equality assertions often fail due to floating-point precision issues. Python provides specialized testing utilities for numerical comparisons.\n",
        "\n",
        "### Testing NumPy Arrays\n",
        "\n",
        "Use NumPy's testing utilities for comparing arrays with appropriate tolerance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from numpy.testing import assert_array_almost_equal, assert_array_equal\n",
        "\n",
        "\n",
        "def normalize_features(data):\n",
        "    \"\"\"Normalize features to 0-1 range\"\"\"\n",
        "    return (data - data.min()) / (data.max() - data.min())\n",
        "\n",
        "\n",
        "def test_normalization():\n",
        "    data = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
        "    normalized = normalize_features(data)\n",
        "\n",
        "    expected = np.array([0.0, 0.25, 0.5, 0.75, 1.0])\n",
        "\n",
        "    # Better than: assert normalized == expected (this would fail!)\n",
        "    assert_array_almost_equal(normalized, expected, decimal=2)\n",
        "\n",
        "\n",
        "def test_model_predictions():\n",
        "    # Simulate model predictions with floating point results\n",
        "    predictions = np.array([0.123456, 0.789012, 0.345678])\n",
        "    expected = np.array([0.12, 0.79, 0.35])\n",
        "\n",
        "    # Compare with 2 decimal places\n",
        "    assert_array_almost_equal(predictions, expected, decimal=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Pandas DataFrames\n",
        "\n",
        "Use pandas testing utilities for DataFrame comparisons:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def clean_dataframe(df):\n",
        "    \"\"\"Remove duplicates and fill missing values\"\"\"\n",
        "    return df.drop_duplicates().fillna(0)\n",
        "\n",
        "\n",
        "def test_dataframe_cleaning():\n",
        "    # Create test data with duplicates and NaN\n",
        "    dirty_data = pd.DataFrame({\"A\": [1, 2, 2, np.nan], \"B\": [4, 5, 5, 6]})\n",
        "\n",
        "    cleaned = clean_dataframe(dirty_data)\n",
        "\n",
        "    expected = pd.DataFrame({\"A\": [1.0, 2.0, 0.0], \"B\": [4, 5, 6]})\n",
        "\n",
        "    # Use pandas testing utility\n",
        "    pd.testing.assert_frame_equal(cleaned.reset_index(drop=True), expected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mock External Dependencies {#mock-external-dependencies}\n",
        "\n",
        "Data science projects often depend on external services like APIs, databases, or cloud storage. Testing these dependencies can be slow, expensive, or unreliable. Mocking allows you to replace real external calls with fake responses.\n",
        "\n",
        "### Mocking API Calls\n",
        "\n",
        "The `@patch` decorator replaces a real function with a mock during the test. When your code tries to call the original function, it gets the mock instead.\n",
        "\n",
        "Let's start with a simple example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from unittest.mock import Mock, patch\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "def fetch_stock_data(symbol):\n",
        "    \"\"\"Fetch stock price data from an API\"\"\"\n",
        "    response = requests.get(f\"https://api.example.com/stock/{symbol}\")\n",
        "    return response.json()[\"price\"]\n",
        "\n",
        "\n",
        "@patch(\"requests.get\")\n",
        "def test_fetch_stock_data_simple(mock_get):\n",
        "    # Create a fake response object\n",
        "    mock_response = Mock()\n",
        "    mock_response.json.return_value = {\"price\": 150.0}\n",
        "\n",
        "    # Make the mock return our fake response\n",
        "    mock_get.return_value = mock_response\n",
        "\n",
        "    # Use the mock instead of real requests.get\n",
        "    price = fetch_stock_data(\"AAPL\")\n",
        "\n",
        "    # Verify we got the fake data\n",
        "    assert price == 150.0\n",
        "\n",
        "    # Verify the mock was called with the right URL\n",
        "    mock_get.assert_called_once_with(\"https://api.example.com/stock/AAPL\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Breaking down the mock syntax:\n",
        "\n",
        "- `@patch(\"requests.get\")` - Decorates the test function to replace `requests.get` with a mock object for this test only\n",
        "- `mock_get` - The mock object that replaces `requests.get`, automatically passed as a parameter to your test function\n",
        "- `Mock()` - Creates a fake object that can simulate any behavior you need\n",
        "- `mock_response.json.return_value = {\"price\": 150.0}` - Tells the fake response: \"when someone calls `.json()` on you, return this dictionary\"\n",
        "- `mock_get.return_value = mock_response` - Tells the fake `requests.get`: \"when someone calls you, return this fake response object\"\n",
        "- `mock_get.assert_called_once_with(...)` - Verifies that `requests.get` was called exactly once with the expected URL\n",
        "\n",
        "Output:\n",
        "```\n",
        "test_mock_api.py::test_fetch_stock_data_simple PASSED\n",
        "```\n",
        "\n",
        "The test passes successfully without making any real network requests. That is pretty cool!\n",
        "\n",
        "### Mocking Database Queries\n",
        "\n",
        "Database mocking applies the same `@patch` principles to pandas database operations. Instead of mocking `requests.get`, we mock `pandas.read_sql` to simulate database query results without needing an actual database connection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from unittest.mock import patch\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "connection = None  # Simulated database connection\n",
        "\n",
        "\n",
        "def get_sales_data(start_date, end_date):\n",
        "    \"\"\"Fetch sales data from database\"\"\"\n",
        "    query = f\"SELECT * FROM sales WHERE date BETWEEN '{start_date}' AND '{end_date}'\"\n",
        "    return pd.read_sql(query, connection)\n",
        "\n",
        "\n",
        "def analyze_sales_trends(start_date, end_date):\n",
        "    \"\"\"Analyze sales trends over a period\"\"\"\n",
        "    data = get_sales_data(start_date, end_date)\n",
        "    return data.groupby(\"product\")[\"amount\"].sum().to_dict()\n",
        "\n",
        "\n",
        "@patch(\"pandas.read_sql\")\n",
        "def test_sales_analysis(mock_read_sql):\n",
        "    # Mock the database query result\n",
        "    mock_data = pd.DataFrame(\n",
        "        {\n",
        "            \"product\": [\"A\", \"B\", \"A\", \"B\"],\n",
        "            \"amount\": [100, 150, 200, 250],\n",
        "            \"date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\", \"2023-01-04\"],\n",
        "        }\n",
        "    )\n",
        "    mock_read_sql.return_value = mock_data\n",
        "\n",
        "    result = analyze_sales_trends(\"2023-01-01\", \"2023-01-04\")\n",
        "\n",
        "    expected = {\"A\": 300, \"B\": 400}\n",
        "    assert result == expected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "```\n",
        "test_database_mock.py::test_sales_analysis PASSED\n",
        "```\n",
        "\n",
        "The mock DataFrame lets you test complex pandas operations without database setup.\n",
        "\n",
        "## Organize Tests with Custom Markers {#organize-tests-with-custom-markers}\n",
        "\n",
        "As your data science project grows, you'll have different types of tests: fast unit tests, slow integration tests, tests that require special hardware (like GPUs), and tests for different stages of your ML pipeline. Custom markers help you organize and run specific test categories.\n",
        "\n",
        "First, configure your markers in a `pytest.ini` file:\n",
        "\n",
        "```ini\n",
        "[tool:pytest]\n",
        "markers =\n",
        "    slow: marks tests as slow (deselect with \"-m \\\"not slow\\\"\")\n",
        "    fast: marks tests as fast unit tests\n",
        "    gpu: marks tests that require GPU acceleration\n",
        "    integration: marks tests as integration tests\n",
        "    model_training: marks tests that train ML models\n",
        "    data_processing: marks tests for data processing functions\n",
        "```\n",
        "\n",
        "Then use these markers in your test files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pytest\n",
        "\n",
        "@pytest.mark.fast\n",
        "def test_data_validation():\n",
        "    \"\"\"Quick validation test\"\"\"\n",
        "    data = [1, 2, 3, 4, 5]\n",
        "    assert all(x > 0 for x in data)\n",
        "\n",
        "@pytest.mark.slow\n",
        "@pytest.mark.model_training\n",
        "def test_train_complex_model():\n",
        "    \"\"\"This test takes several minutes\"\"\"\n",
        "    # Simulate training a complex model\n",
        "    import time\n",
        "    time.sleep(1)  # Simulate long training\n",
        "    assert True\n",
        "\n",
        "@pytest.mark.gpu\n",
        "def test_gpu_acceleration():\n",
        "    \"\"\"Test that requires CUDA/GPU\"\"\"\n",
        "    # Test GPU-accelerated computations\n",
        "    pytest.importorskip(\"cupy\")  # Skip if GPU library not available\n",
        "    import cupy as cp\n",
        "    data = cp.array([1, 2, 3, 4, 5])\n",
        "    assert len(data) == 5\n",
        "\n",
        "@pytest.mark.integration\n",
        "@pytest.mark.data_processing\n",
        "def test_full_data_pipeline():\n",
        "    \"\"\"Test the complete data processing pipeline\"\"\"\n",
        "    # Test end-to-end data processing\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can run specific test categories:\n",
        "\n",
        "```{bash}\n",
        "# Run only fast tests\n",
        "pytest -m fast\n",
        "\n",
        "# Run everything except slow tests\n",
        "pytest -m \"not slow\"\n",
        "\n",
        "# Run only GPU tests\n",
        "pytest -m gpu\n",
        "\n",
        "# Run model training and data processing tests\n",
        "pytest -m \"model_training or data_processing\"\n",
        "\n",
        "# Run integration tests that are not slow\n",
        "pytest -m \"integration and not slow\"\n",
        "```\n",
        "\n",
        "## Configure Pytest for Your Project {#configure-pytest-for-your-project}\n",
        "\n",
        "Large data science projects benefit from centralized test configuration and shared fixtures. Two files help organize this: `pytest.ini` for configuration and `conftest.py` for shared test utilities.\n",
        "\n",
        "\n",
        "### Project Configuration with pytest.ini\n",
        "\n",
        "Create a `pytest.ini` file in your project root to configure pytest behavior:\n",
        "\n",
        "```ini\n",
        "[tool:pytest]\n",
        "# Configure test discovery\n",
        "testpaths = tests\n",
        "python_files = test_*.py *_test.py\n",
        "python_classes = Test*\n",
        "python_functions = test_*\n",
        "\n",
        "# Configure markers\n",
        "markers =\n",
        "    slow: marks tests as slow running\n",
        "    fast: marks tests as fast unit tests\n",
        "    gpu: marks tests requiring GPU\n",
        "    integration: marks integration tests\n",
        "    unit: marks unit tests\n",
        "\n",
        "# Configure output\n",
        "addopts = -v --tb=short --strict-markers\n",
        "\n",
        "# Configure warnings\n",
        "filterwarnings =\n",
        "    ignore::UserWarning\n",
        "    ignore::DeprecationWarning:sklearn.*\n",
        "```\n",
        "\n",
        "Key configuration sections:\n",
        "\n",
        "- **Test Discovery**: Where pytest finds tests (`testpaths = tests`) and naming patterns\n",
        "- **Custom Markers**: Categories for your tests (slow, fast, gpu, integration, unit)\n",
        "- **Output Options**: Verbose output (`-v`) and concise error traces (`--tb=short`)\n",
        "- **Warning Filters**: Hide library warnings that clutter output\n",
        "\n",
        "### Shared Fixtures with conftest.py\n",
        "\n",
        "Create a `conftest.py` file to define fixtures available to all your tests:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# conftest.py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytest\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def sample_dataset():\n",
        "    \"\"\"Create a sample dataset for testing\"\"\"\n",
        "    np.random.seed(42)\n",
        "    X = np.random.randn(1000, 5)\n",
        "    y = np.random.randint(0, 2, 1000)\n",
        "\n",
        "    return pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(5)]).assign(target=y)\n",
        "\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def trained_model(sample_dataset):\n",
        "    \"\"\"Provide a pre-trained model for testing\"\"\"\n",
        "    X = sample_dataset.drop(\"target\", axis=1)\n",
        "    y = sample_dataset[\"target\"]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    model = LogisticRegression(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    return {\n",
        "        \"model\": model,\n",
        "        \"X_train\": X_train,\n",
        "        \"X_test\": X_test,\n",
        "        \"y_train\": y_train,\n",
        "        \"y_test\": y_test,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now any test file can use these fixtures without importing them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# test_models.py\n",
        "def test_model_accuracy(trained_model):\n",
        "    model_info = trained_model\n",
        "    model = model_info[\"model\"]\n",
        "    X_test = model_info[\"X_test\"]\n",
        "    y_test = model_info[\"y_test\"]\n",
        "\n",
        "    accuracy = model.score(X_test, y_test)\n",
        "    assert accuracy > 0.5\n",
        "\n",
        "\n",
        "def test_dataset_shape(sample_dataset):\n",
        "    assert sample_dataset.shape == (1000, 6)  # 5 features + 1 target\n",
        "    assert \"target\" in sample_dataset.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Structure your Projects {#structure-your-projects}\n",
        "\n",
        "Last but not least, when our code grows bigger, we should organize the code by storing functions and their tests in two different folders. Conventionally, source code is kept in the \"src\" folder, while tests are stored in the \"tests\" folder.\n",
        "\n",
        "To automate test executions, name your test functions as either \"test_<name>.py\" or \"<name>_test.py\". Pytest will then identify and run all files ending or beginning with \"test\".\n",
        "\n",
        "This is how these two files will look like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def extract_sentiment(text: str):\n",
        "        '''Extract sentiment using textblob.\n",
        "        Polarity is within range [-1, 1]'''\n",
        "\n",
        "        text = TextBlob(text)\n",
        "\n",
        "        return text.sentiment.polarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pytest\n",
        "from src.process import extract_sentiment\n",
        "\n",
        "def test_extract_sentiment():\n",
        "\n",
        "    text = \"Today I found a duck and I am happy\"\n",
        "\n",
        "    sentiment = extract_sentiment(text)\n",
        "\n",
        "    assert sentiment > 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run all tests, type `pytest tests` in the root directory:\n",
        "\n",
        "```bash\n",
        "========================== test session starts ===========================\n",
        "platform linux -- Python 3.8.3, pytest-5.4.2, py-1.8.1, pluggy-0.13.1\n",
        "collected 1 item\n",
        "tests/test_process.py .                                            [100%]\n",
        "=========================== 1 passed in 0.69s ============================\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}