{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Create a Spark Session and Input Data {#setup-create-a-spark-session-and-input-data}\n",
        "\n",
        "> ðŸ“– Read the full article: [Writing Safer PySpark Queries with Parameters](https://codecut.ai/pyspark-sql-enhancing-reusability-with-parameterized-queries/)\n",
        "\n",
        "\n",
        "We'll begin by creating a Spark session and generating a sample DataFrame using the Pandas-to-Spark conversion method. For other common ways to build DataFrames in PySpark, see [this guide on creating PySpark DataFrames](https://codecut.ai/3-powerful-ways-to-create-pyspark-dataframes/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datetime import date\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Create a Spark DataFrame\n",
        "item_price_pandas = pd.DataFrame(\n",
        "    {\n",
        "        \"item_id\": [1, 2, 3, 4],\n",
        "        \"price\": [4, 2, 5, 1],\n",
        "        \"transaction_date\": [\n",
        "            date(2025, 1, 15),\n",
        "            date(2025, 2, 1),\n",
        "            date(2025, 3, 10),\n",
        "            date(2025, 4, 22),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "\n",
        "item_price = spark.createDataFrame(item_price_pandas)\n",
        "item_price.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**\n",
        "\n",
        "```python\n",
        "+-------+-----+----------------+\n",
        "|item_id|price|transaction_date|\n",
        "+-------+-----+----------------+\n",
        "|      1|    4|      2025-01-15|\n",
        "|      2|    2|      2025-02-01|\n",
        "|      3|    5|      2025-03-10|\n",
        "|      4|    1|      2025-04-22|\n",
        "+-------+-----+----------------+\n",
        "```\n",
        "\n",
        "## Traditional PySpark Query Approach {#traditional-pyspark-query-approach}\n",
        "\n",
        "The traditional approach uses f-strings to build SQL, which is not ideal because:\n",
        "\n",
        "- **Security Risk**: Interpolated strings can expose your query to SQL injection.\n",
        "- **Limited Flexibility**: F-strings can't handle Python objects like DataFrames directly, so you have to create temporary views and manually quote values like dates to match SQL syntax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "item_price.createOrReplaceTempView(\"item_price_view\")\n",
        "transaction_date_str = \"2025-02-15\"\n",
        "\n",
        "query_with_fstring = f\"\"\"SELECT *\n",
        "FROM item_price_view\n",
        "WHERE transaction_date > '{transaction_date_str}'\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(query_with_fstring).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output**\n",
        "\n",
        "```python\n",
        "+-------+-----+----------------+\n",
        "|item_id|price|transaction_date|\n",
        "+-------+-----+----------------+\n",
        "|      3|    5|      2025-03-10|\n",
        "|      4|    1|      2025-04-22|\n",
        "+-------+-----+----------------+\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## Parameterized Queries with PySpark Custom String Formatting {#parameterized-queries-with-pyspark-custom-string-formatting}\n",
        "\n",
        "PySpark supports parameterized SQL with custom string formatting, separating SQL logic from parameter values. During parsing, it safely handles each value as a typed literal and inserts it into the SQL parse tree, preventing injection attacks and ensuring correct data types.\n",
        "\n",
        "```python\n",
        "Query\n",
        "â”œâ”€â”€ SELECT\n",
        "â”‚   â””â”€â”€ *\n",
        "â”œâ”€â”€ FROM\n",
        "â”‚   â””â”€â”€ {item_price}\n",
        "â””â”€â”€ WHERE\n",
        "    â””â”€â”€ Condition\n",
        "        â”œâ”€â”€ Left: transaction_date\n",
        "        â”œâ”€â”€ Operator: >\n",
        "        â””â”€â”€ Right: {transaction_date}\n",
        "```\n",
        "\n",
        "Because it handles each value as a typed literal, it treats the value according to its actual data type, not as raw text, when inserting it into a SQL query, meaning:\n",
        "\n",
        "- `item_price` can be passed directly without creating a temporary view\n",
        "- `transaction_date` does not need to be manually wrapped in single quotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parametrized_query = \"\"\"SELECT *\n",
        "FROM {item_price}\n",
        "WHERE transaction_date > {transaction_date}\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(\n",
        "    parametrized_query, item_price=item_price, transaction_date=transaction_date_str\n",
        ").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```python\n",
        "+-------+-----+----------------+\n",
        "|item_id|price|transaction_date|\n",
        "+-------+-----+----------------+\n",
        "|      3|    5|      2025-03-10|\n",
        "|      4|    1|      2025-04-22|\n",
        "+-------+-----+----------------+\n",
        "```\n",
        "\n",
        "## Parameterized Queries with Parameter Markers {#parameterized-queries-with-parameter-markers}\n",
        "\n",
        "Custom string formatting would treat `date(2023, 2, 15)` as a mathematical expression rather than a date, which would cause a type mismatch error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "parametrized_query = \"\"\"SELECT *\n",
        "FROM {item_price}\n",
        "WHERE transaction_date > {transaction_date}\n",
        "\"\"\"\n",
        "\n",
        "spark.sql(parametrized_query, item_price=item_price, transaction_date=transaction_date).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```python\n",
        "[DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(transaction_date > ((2023 - 2) - 15))\" due to data type mismatch\n",
        "```\n",
        "\n",
        "Parameter markers preserve type information, so `date` objects are passed as proper SQL DATE literals. This allows you to safely use Python dates without formatting or quoting them manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "query_with_markers = \"\"\"SELECT *\n",
        "FROM {item_price}\n",
        "WHERE transaction_date > :transaction_date\n",
        "\"\"\"\n",
        "\n",
        "transaction_date = date(2025, 2, 15)\n",
        "\n",
        "spark.sql(\n",
        "    query_with_markers,\n",
        "    item_price=item_price,\n",
        "    args={\"transaction_date\": transaction_date},\n",
        ").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make PySpark SQL Easier to Reuse {#make-pyspark-sql-easier-to-reuse}\n",
        "\n",
        "Parameterized SQL templates are easier to reuse across your codebase. Instead of copying and pasting full SQL strings with values hardcoded inside, you can define flexible query templates that accept different input variables.\n",
        "\n",
        "Here's a reusable query to filter using different transaction dates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transaction_date_1 = date(2025, 3, 9)\n",
        "\n",
        "spark.sql(\n",
        "    query_with_markers,\n",
        "    item_price=item_price,\n",
        "    args={\"transaction_date\": transaction_date_1},\n",
        ").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```python\n",
        "+-------+-----+----------------+\n",
        "|item_id|price|transaction_date|\n",
        "+-------+-----+----------------+\n",
        "|      3|    5|      2025-03-10|\n",
        "|      4|    1|      2025-04-22|\n",
        "+-------+-----+----------------+\n",
        "```\n",
        "\n",
        "You can easily change the filter with a different date:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "transaction_date_2 = date(2025, 3, 15)\n",
        "\n",
        "spark.sql(\n",
        "    query_with_markers,\n",
        "    item_price=item_price,\n",
        "    args={\"transaction_date\": transaction_date_2},\n",
        ").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```python\n",
        "+-------+-----+----------------+\n",
        "|item_id|price|transaction_date|\n",
        "+-------+-----+----------------+\n",
        "|      4|    1|      2025-04-22|\n",
        "+-------+-----+----------------+\n",
        "```\n",
        "\n",
        "## Easier Unit Testing with PySpark Parameterized Queries {#easier-unit-testing-with-pyspark-parameterized-queries}\n",
        "\n",
        "Parameterization also simplifies testing by letting you pass different inputs into a reusable query string.\n",
        "\n",
        "For example, in the code below, we define a function that takes a DataFrame and a threshold value, then filters rows using a parameterized query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "def filter_by_price_threshold(df, amount):\n",
        "    return spark.sql(\n",
        "        \"SELECT * from {df} where price > :amount\", df=df, args={\"amount\": amount}\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the values are passed separately from the SQL logic, we can easily reuse and test this function with different parameters without rewriting the query itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def test_query_return_correct_number_of_rows():\n",
        "    # Create test input DataFrame\n",
        "    df = spark.createDataFrame(\n",
        "        [\n",
        "            (\"Product 1\", 10.0, 5),\n",
        "            (\"Product 2\", 15.0, 3),\n",
        "            (\"Product 3\", 8.0, 2),\n",
        "        ],\n",
        "        [\"name\", \"price\", \"quantity\"],\n",
        "    )\n",
        "\n",
        "    # Execute query with parameters\n",
        "    assert filter_by_price_threshold(df, 10).count() == 1\n",
        "    assert filter_by_price_threshold(df, 8).count() == 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more tips on validating DataFrame outputs effectively, see [best practices for PySpark DataFrame comparison and testing](https://codecut.ai/best-practices-for-pyspark-dataframe-comparison-testing/)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}