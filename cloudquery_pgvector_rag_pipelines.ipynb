{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install CloudQuery and Prepare PostgreSQL {#install-cloudquery-and-prepare-postgresql}\n",
        "\n",
        "> ðŸ“– Read the full article: [Hacker News Semantic Search: Production RAG with CloudQuery and Postgres](https://codecut.ai/cloudquery-pgvector-rag-pipelines/)\n",
        "\n",
        "\n",
        "To install CloudQuery, run the following command:\n",
        "\n",
        "```bash\n",
        "# macOS\n",
        "brew install cloudquery/tap/cloudquery\n",
        "\n",
        "# or download a release (Linux example)\n",
        "curl -L https://github.com/cloudquery/cloudquery/releases/download/cli-v6.29.2/cloudquery_linux_amd64 -o cloudquery\n",
        "chmod +x cloudquery\n",
        "```\n",
        "\n",
        "For other installation methods (Windows, Docker, or package managers), visit the [CloudQuery installation guide](https://docs.cloudquery.io/docs/quickstart).\n",
        "\n",
        "Create a database that will be used for this tutorial:\n",
        "\n",
        "```bash\n",
        "createdb cloudquery\n",
        "```\n",
        "\n",
        "Export the credentials that will be used for the later steps:\n",
        "\n",
        "```bash\n",
        "export POSTGRESQL_CONNECTION_STRING=\"postgresql://user:pass@localhost:5432/database\"\n",
        "export OPENAI_API_KEY=sk-...\n",
        "```\n",
        "\n",
        "Here are how to get the credentials:\n",
        "\n",
        "- **PostgreSQL**: Replace `user:pass@localhost:5432/database` with your actual database connection details\n",
        "- **OpenAI API Key**: Get your API key from the [OpenAI platform](https://platform.openai.com/api-keys)\n",
        "\n",
        "## Sync Hacker News to PostgreSQL {#sync-hacker-news-to-postgresql}\n",
        "\n",
        "To sync Hacker News to PostgreSQL, start by authenticating with CloudQuery Hub:\n",
        "\n",
        "```bash\n",
        "cloudquery login\n",
        "```\n",
        "\n",
        "Then run the following command to create a YAML file that syncs Hacker News to PostgreSQL:\n",
        "\n",
        "```bash\n",
        "cloudquery init --source hackernews --destination postgresql\n",
        "```\n",
        "\n",
        "This command will create a YAML file called `hackernews_to_postgresql.yaml`:\n",
        "\n",
        "```yaml\n",
        "kind: source\n",
        "spec:\n",
        "  name: \"hackernews\"\n",
        "  path: \"cloudquery/hackernews\"\n",
        "  registry: \"cloudquery\"\n",
        "  version: \"v3.8.2\"\n",
        "  tables: [\"*\"]\n",
        "  backend_options:\n",
        "    table_name: \"cq_state_hackernews\"\n",
        "    connection: \"@@plugins.postgresql.connection\"\n",
        "  destinations:\n",
        "    - \"postgresql\"\n",
        "  spec:\n",
        "    item_concurrency: 100\n",
        "    start_time: 3 hours ago\n",
        "---\n",
        "kind: destination\n",
        "spec:\n",
        "  name: \"postgresql\"\n",
        "  path: \"cloudquery/postgresql\"\n",
        "  registry: \"cloudquery\"\n",
        "  version: \"v8.12.1\"\n",
        "  write_mode: \"overwrite-delete-stale\"\n",
        "  spec:\n",
        "    connection_string: \"${POSTGRESQL_CONNECTION_STRING}\"\n",
        "```\n",
        "\n",
        "\n",
        "We can now sync the Hacker News data to PostgreSQL by running the following command:\n",
        "\n",
        "```bash\n",
        "cloudquery sync hackernews_to_postgresql.yaml\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "Loading spec(s) from hackernews_to_postgresql.yaml\n",
        "Starting sync for: hackernews (cloudquery/hackernews@v3.8.2) -> [postgresql (cloudquery/postgresql@v8.12.1)]\n",
        "Sync completed successfully. Resources: 9168, Errors: 0, Warnings: 0, Time: 26s\n",
        "```\n",
        "\n",
        "Let's check if the data was ingested successfully by connecting to the CloudQuery database:\n",
        "\n",
        "```bash\n",
        "psql -U postgres -d cloudquery\n",
        "```\n",
        "\n",
        "Then inspect the available tables:\n",
        "\n",
        "```sql\n",
        "\\dt\n",
        "```\n",
        "\n",
        "```\n",
        " Schema |            Name             | Type  |   Owner\n",
        "--------+-----------------------------+-------+------------\n",
        " public | cq_state_hackernews         | table | khuyentran\n",
        " public | hackernews_items            | table | khuyentran\n",
        "```\n",
        "\n",
        "CloudQuery automatically creates two tables:\n",
        "\n",
        "- `cq_state_hackernews`: tracks sync state for incremental updates\n",
        "- `hackernews_items`: contains the actual Hacker News data\n",
        "\n",
        "View the schema of the `hackernews_items` table:\n",
        "\n",
        "```sql\n",
        "\\d hackernews_items\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```text\n",
        "                        Table \"public.hackernews_items\"\n",
        "     Column      |            Type             | Collation | Nullable | Default\n",
        "-----------------+-----------------------------+-----------+----------+---------\n",
        " _cq_sync_time   | timestamp without time zone |           |          |\n",
        " _cq_source_name | text                        |           |          |\n",
        " _cq_id          | uuid                        |           | not null |\n",
        " _cq_parent_id   | uuid                        |           |          |\n",
        " id              | bigint                      |           | not null |\n",
        " deleted         | boolean                     |           |          |\n",
        " type            | text                        |           |          |\n",
        " by              | text                        |           |          |\n",
        " time            | timestamp without time zone |           |          |\n",
        " text            | text                        |           |          |\n",
        " dead            | boolean                     |           |          |\n",
        " parent          | bigint                      |           |          |\n",
        " kids            | bigint[]                    |           |          |\n",
        " url             | text                        |           |          |\n",
        " score           | bigint                      |           |          |\n",
        " title           | text                        |           |          |\n",
        " parts           | bigint[]                    |           |          |\n",
        " descendants     | bigint                      |           |          |\n",
        "```\n",
        "\n",
        "Check the first 5 rows with type `story`:\n",
        "\n",
        "```bash\n",
        "SELECT id, type, score, title FROM hackernews_items WHERE type='story' LIMIT 5;\n",
        "```\n",
        "\n",
        "```\n",
        "    id    | type  | score |                                 title\n",
        "----------+-------+-------+-----------------------------------------------------------------------\n",
        " 45316982 | story |     3 | Ask HN: Why don't Americans hire human assistants for everyday tasks?\n",
        " 45317015 | story |     2 | Streaming Live: San Francisco Low Riders Festival\n",
        " 45316989 | story |     1 | The Collapse of Coliving Operators, and Why the Solution Is Upstream\n",
        " 45317092 | story |     0 |\n",
        " 45317108 | story |     1 | Patrick McGovern was the maven of ancient tipples\n",
        "```\n",
        "\n",
        "## Add pgvector Embeddings {#add-pgvector-embeddings}\n",
        "\n",
        "[pgvector](https://github.com/pgvector/pgvector) is a PostgreSQL extension that adds vector similarity search capabilities, perfect for RAG applications. For a complete guide on implementing RAG with pgvector, see our [semantic search tutorial](https://codecut.ai/semantic-search-postgres-pgvector-ollama/).\n",
        "\n",
        "CloudQuery provides built-in pgvector support, automatically generating embeddings alongside your data sync. First, enable the pgvector extension in PostgreSQL:\n",
        "\n",
        "```bash\n",
        "psql -d cloudquery -c \"CREATE EXTENSION IF NOT EXISTS vector;\"\n",
        "```\n",
        "\n",
        "Now add the pgvector configuration to the `hackernews_to_postgresql.yaml`:\n",
        "\n",
        "```yaml\n",
        "kind: source\n",
        "spec:\n",
        "  name: \"hackernews\"\n",
        "  path: \"cloudquery/hackernews\"\n",
        "  registry: \"cloudquery\"\n",
        "  version: \"v3.8.2\"\n",
        "  tables: [\"*\"]\n",
        "  backend_options:\n",
        "    table_name: \"cq_state_hackernews\"\n",
        "    connection: \"@@plugins.postgresql.connection\"\n",
        "  destinations:\n",
        "    - \"postgresql\"\n",
        "  spec:\n",
        "    item_concurrency: 100\n",
        "    start_time: 3 hours ago\n",
        "---\n",
        "kind: destination\n",
        "spec:\n",
        "  name: \"postgresql\"\n",
        "  path: \"cloudquery/postgresql\"\n",
        "  registry: \"cloudquery\"\n",
        "  version: \"v8.12.1\"\n",
        "  write_mode: \"overwrite-delete-stale\"\n",
        "  spec:\n",
        "    connection_string: \"${POSTGRESQL_CONNECTION_STRING}\"\n",
        "\n",
        "    pgvector_config:\n",
        "      tables:\n",
        "        - source_table_name: \"hackernews_items\"\n",
        "          target_table_name: \"hackernews_items_embeddings\"\n",
        "          embed_columns: [\"title\"]\n",
        "          metadata_columns: [\"id\", \"type\", \"url\", \"by\"]\n",
        "          filter_condition: \"type = 'story' AND title IS NOT NULL AND title != ''\"\n",
        "      text_splitter:\n",
        "        recursive_text:\n",
        "          chunk_size: 200\n",
        "          chunk_overlap: 0\n",
        "      openai_embedding:\n",
        "        api_key: \"${OPENAI_API_KEY}\"\n",
        "        model_name: \"text-embedding-3-small\"\n",
        "        dimensions: 1536\n",
        "```\n",
        "\n",
        "Explanation of pgvector configuration:\n",
        "\n",
        "- **`source_table_name`**: The original CloudQuery table to read data from\n",
        "- **`target_table_name`**: New table where embeddings and metadata will be stored\n",
        "- **`embed_columns`**: Which columns to convert into vector embeddings (only non-empty text is processed)\n",
        "- **`metadata_columns`**: Additional columns to preserve alongside embeddings for filtering and context\n",
        "- **`filter_condition`**: SQL WHERE clause to only embed specific rows (stories with non-empty titles)\n",
        "- **`chunk_size`**: Maximum characters per text chunk (short titles become single chunks)\n",
        "- **`chunk_overlap`**: Overlapping characters between chunks to preserve context across boundaries\n",
        "- **`model_name`**: OpenAI embedding model (text-embedding-3-small offers 5x cost savings vs ada-002)\n",
        "\n",
        "Since we've already synced the data, let's clean up the existing tables before running the sync again:\n",
        "\n",
        "```bash\n",
        "psql -U postgres -d cloudquery -c 'DROP TABLE IF EXISTS cq_state_hackernews'\n",
        "```\n",
        "\n",
        "Run the enhanced sync:\n",
        "\n",
        "```bash\n",
        "cloudquery sync hackernews_to_postgresql.yaml\n",
        "```\n",
        "\n",
        "CloudQuery now produces an embeddings table alongside the source data:\n",
        "\n",
        "```bash\n",
        "psql -U postgres -d cloudquery\n",
        "```\n",
        "\n",
        "List the available tables:\n",
        "\n",
        "```sql\n",
        "\\dt\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        " Schema |            Name             | Type  |   Owner\n",
        "--------+-----------------------------+-------+------------\n",
        " public | cq_state_hackernews         | table | khuyentran\n",
        " public | hackernews_items            | table | khuyentran\n",
        " public | hackernews_items_embeddings | table | khuyentran\n",
        "```\n",
        "\n",
        "Inspect the embeddings table structure:\n",
        "\n",
        "```sql\n",
        "-- Check table structure and vector dimensions\n",
        "\\d hackernews_items_embeddings;\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "cloudquery=# \\d hackernews_items_embeddings;\n",
        "                  Table \"public.hackernews_items_embeddings\"\n",
        "    Column     |            Type             | Collation | Nullable | Default\n",
        "---------------+-----------------------------+-----------+----------+---------\n",
        " _cq_id        | uuid                        |           |          |\n",
        " id            | bigint                      |           | not null |\n",
        " type          | text                        |           |          |\n",
        " url           | text                        |           |          |\n",
        " _cq_sync_time | timestamp without time zone |           |          |\n",
        " chunk         | text                        |           |          |\n",
        " embedding     | vector(1536)                |           |          |\n",
        "```\n",
        "\n",
        "Sample a few records to see the data:\n",
        "\n",
        "```sql\n",
        "-- Sample a few records to see the data\n",
        "SELECT id, type, url, by, chunk\n",
        "FROM hackernews_items_embeddings\n",
        "LIMIT 5;\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        " type  |       by        |                         chunk\n",
        "-------+-----------------+--------------------------------------------------------\n",
        " story | Kaibeezy        | Autonomous Airport Ground Support Equipment\n",
        " story | drankl          | Dining across the divide: 'We disagreed on...\n",
        " story | wjSgoWPm5bWAhXB | World's First AI-designed viruses a step towards...\n",
        " story | Brajeshwar      | Banned in the U.S. and Europe, Huawei aims for...\n",
        " story | danielfalbo     | Zig Z-ant: run ML models on microcontrollers\n",
        "```\n",
        "\n",
        "Check for NULL embeddings:\n",
        "\n",
        "```sql\n",
        "SELECT COUNT(*) as rows_without_embeddings\n",
        "FROM hackernews_items_embeddings\n",
        "WHERE embedding IS NULL;\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        " rows_without_embeddings\n",
        "----------------------\n",
        "                    0\n",
        "```\n",
        "\n",
        "Great! There is no NULL embeddings.\n",
        "\n",
        "Check the chunk sizes and content distribution:\n",
        "\n",
        "```sql\n",
        "-- Check chunk sizes and content distribution\n",
        "SELECT\n",
        "    type,\n",
        "    COUNT(*) as count,\n",
        "    AVG(LENGTH(chunk)) as avg_chunk_length,\n",
        "    MIN(LENGTH(chunk)) as min_chunk_length,\n",
        "    MAX(LENGTH(chunk)) as max_chunk_length\n",
        "FROM hackernews_items_embeddings\n",
        "GROUP BY type;\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        " type  | count |  avg_chunk_length   | min_chunk_length | max_chunk_length\n",
        "-------+-------+---------------------+------------------+------------------\n",
        " story |   695 | 52.1366906474820144 |                4 |               86\n",
        "```\n",
        "\n",
        "The 52-character average chunk length confirms that most Hacker News titles fit comfortably within the configured 200-character chunk_size limit, validating the text splitter settings.\n",
        "\n",
        "\n",
        "\n",
        "## Semantic Search with LangChain Postgres {#semantic-search-with-langchain-postgres}\n",
        "\n",
        "Now that we have embeddings stored in PostgreSQL, let's use LangChain Postgres to handle vector operations.\n",
        "\n",
        "Start with installing the necessary packages:\n",
        "\n",
        "```bash\n",
        "pip install langchain-postgres langchain-openai psycopg[binary] greenlet\n",
        "```\n",
        "\n",
        "Next, set up the embedding service that will convert text queries into vector representations for similarity matching. In this example, we'll use the OpenAI embedding model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "from langchain_postgres import PGEngine, PGVectorStore\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Initialize embeddings (requires OPENAI_API_KEY environment variable)\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Connect to the `hackernews_items_embeddings` table generated by CloudQuery containing the pre-computed embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialize connection engine\n",
        "CONNECTION_STRING = \"postgresql+asyncpg://khuyentran@localhost:5432/cloudquery\"\n",
        "engine = PGEngine.from_connection_string(url=CONNECTION_STRING)\n",
        "\n",
        "# Connect to existing CloudQuery vector store table\n",
        "vectorstore = PGVectorStore.create_sync(\n",
        "    engine=engine,\n",
        "    table_name=\"hackernews_items_embeddings\",\n",
        "    embedding_service=embeddings,\n",
        "    content_column=\"chunk\",\n",
        "    embedding_column=\"embedding\",\n",
        "    id_column=\"id\",  # Map to CloudQuery's id column\n",
        "    metadata_columns=[\"type\", \"url\", \"by\"],  # Include story metadata\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can query the vector store to find semantically similar content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Semantic search for Apple-related stories\n",
        "docs = vectorstore.similarity_search(\"Apple technology news\", k=4)\n",
        "for doc in docs:\n",
        "    print(f\"Title: {doc.page_content}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "```\n",
        "Title: Apple takes control of all core chips in iPhoneAir with new arch prioritizing AI\n",
        "Title: Apple used AI to uncover new blood pressure notification\n",
        "Title: Apple Losing Talent to OpenAI\n",
        "Title: Standard iPhone 17 Outperforms Expectations as Apple Ramps Up Manufacturing\n",
        "```\n",
        "\n",
        "The vector search successfully identifies relevant Apple content beyond exact keyword matches, capturing stories about AI development, talent acquisition, and product development that share semantic meaning with the query.\n",
        "\n",
        "\n",
        "## Other CloudQuery Features {#other-cloudquery-features}\n",
        "\n",
        "Let's explore some of the other features of CloudQuery to enhance your pipeline.\n",
        "\n",
        "### Multi-Destination Support\n",
        "\n",
        "CloudQuery can route the same source data to multiple destinations in a single sync operation:\n",
        "\n",
        "```yaml\n",
        "destinations: [\"postgresql\", \"bigquery\", \"s3\"]\n",
        "```\n",
        "\n",
        "This capability means you can simultaneously:\n",
        "\n",
        "- Store operational data in PostgreSQL for real-time queries\n",
        "- Load analytical data into BigQuery for data warehousing\n",
        "- Archive raw data to S3 for compliance and backup\n",
        "\n",
        "### Write Modes\n",
        "\n",
        "CloudQuery provides different write modes to control how data updates are handled:\n",
        "\n",
        "**Smart Incremental Updates:**\n",
        "\n",
        "Smart incremental updates is the default write mode and is recommended for most use cases. It updates existing records and removes any data that's no longer present in the source. This is perfect for maintaining accurate, up-to-date datasets where items can be deleted or modified.\n",
        "\n",
        "```yaml\n",
        "write_mode: \"overwrite-delete-stale\"\n",
        "```\n",
        "\n",
        "**Append-Only Mode:**\n",
        "\n",
        "Append-only mode only adds new data without modifying existing records. Ideal for time-series data, logs, or when you want to preserve historical versions of records.\n",
        "\n",
        "```yaml\n",
        "write_mode: \"append\"\n",
        "```\n",
        "\n",
        "**Selective Overwrite:**\n",
        "\n",
        "Selective overwrite mode replaces existing records with matching primary keys but doesn't remove stale data. Useful when you know the source data is complete but want to keep orphaned records.\n",
        "\n",
        "```yaml\n",
        "write_mode: \"overwrite\"\n",
        "```\n",
        "\n",
        "### Performance Batching\n",
        "\n",
        "You can optimize memory usage and database performance by configuring the batch size and timeout:\n",
        "\n",
        "```yaml\n",
        "spec:\n",
        "  batch_size: 1000              # Records per batch\n",
        "  batch_size_bytes: 4194304     # 4MB memory limit\n",
        "  batch_timeout: \"20s\"          # Max wait between writes\n",
        "```\n",
        "\n",
        "Details of the parameters:\n",
        "\n",
        "- **`batch_size`**: Number of records grouped together before writing.\n",
        "- **`batch_size_bytes`**: Maximum memory size per batch in bytes.\n",
        "- **`batch_timeout`**: Time limit before writing partial batches.\n",
        "\n",
        "### Retry Handling\n",
        "\n",
        "The `max_retries` parameter ensures reliable data delivery by automatically retrying failed write operations a specified number of times before marking them as permanently failed.\n",
        "\n",
        "```yaml\n",
        "spec:\n",
        "  max_retries: 5                # Number of retry attempts\n",
        "```\n",
        "\n",
        "### Time-Based Incremental Syncing\n",
        "\n",
        "CloudQuery's `start_time` configuration prevents unnecessary data reprocessing by only syncing records created after a specified timestamp, dramatically reducing sync time and resource usage:\n",
        "\n",
        "```yaml\n",
        "spec:\n",
        "  start_time: \"7 days ago\"       # Only sync recent data\n",
        "  # Alternative formats:\n",
        "  # start_time: \"2024-01-15T10:00:00Z\"  # Specific timestamp\n",
        "  # start_time: \"1 hour ago\"            # Relative time\n",
        "```\n",
        "\n",
        "\n",
        "See the [source plugin configuration](https://docs.cloudquery.io/docs/reference/source-spec) and [destination plugin configuration](https://docs.cloudquery.io/docs/reference/destination-spec) documentation for all available options.\n",
        "\n",
        "## What You've Built {#what-youve-built}\n",
        "\n",
        "In this tutorial, you've created a production-ready RAG pipeline that:\n",
        "\n",
        "- Syncs live data from Hacker News with automatic retry and state persistence\n",
        "- Generates vector embeddings using OpenAI's latest models\n",
        "- Enables semantic search across thousands of posts and comments\n",
        "- Scales to handle any CloudQuery-supported data source (100+ connectors)\n",
        "\n",
        "All with zero custom ETL code and enterprise-grade reliability.\n",
        "\n",
        "**Next steps:**\n",
        "\n",
        "- Explore the [Hacker News source documentation](https://docs.cloudquery.io/docs/plugins/sources/hackernews) for advanced filtering options (top, best, ask HN, etc.)\n",
        "- Add additional text sources (GitHub issues, Typeform surveys, Airtable notes) to the same pipeline\n",
        "- Schedule CloudQuery with cron, Airflow, or Kubernetes for continuous refresh\n",
        "- Integrate with [LangChain PGVector](https://python.langchain.com/docs/integrations/vectorstores/pgvector) or [LlamaIndex PGVector](https://docs.llamaindex.ai/en/stable/examples/vector_stores/postgres/) in production RAG systems\n",
        "\n",
        "## Related Tutorials\n",
        "\n",
        "- **Foundational Concepts**: [Implement Semantic Search in Postgres Using pgvector and Ollama](https://codecut.ai/semantic-search-postgres-pgvector-ollama/) for comprehensive pgvector fundamentals\n",
        "- **Production Quality**: [Build Production-Ready RAG Systems with MLflow Quality Metrics](https://codecut.ai/rag-evaluation-mlflow-quality-metrics/) for RAG evaluation and monitoring\n",
        "- **Scaling Beyond PostgreSQL**: [Natural-Language Queries for Spark: Using LangChain to Run SQL on DataFrames](https://codecut.ai/natural-language-queries-spark-langchain/) for distributed processing with LangChain"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}